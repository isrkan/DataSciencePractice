{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fnmz_uC7xhG"
   },
   "source": [
    "# Function calling with Gemini\n",
    "\n",
    "This notebook provides a guide to using Google's Gemini API for function calling. We will explore how to enable AI models to interact with external functions, APIs, and tools.\n",
    "\n",
    "Function calling (also known as tool calling) allows language models to interact with external functions, APIs, and tools. Instead of just generating text, the model can call predefined functions with appropriate parameters and make decisions about which functions to call based on context. It can receive function results and incorporate them into responses and continue the conversation with context from function outputs.\n",
    "\n",
    "### How function calling works\n",
    "1. **Function definition**: Define functions with clear schemas.\n",
    "2. **Model decision**: The model decides when and which function to call.\n",
    "3. **Parameter extraction**: The model extracts appropriate parameters from context.\n",
    "4. **Function execution**: The function is called with extracted.parameters.\n",
    "5. **Result integration**: Function results are integrated back into the conversation\n",
    "\n",
    "Function calling is useful because it allows access to real-time data like weather or stocks, enables integration with APIs or databases, can trigger actions such as sending emails or booking appointments, and returns structured data instead of plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qbqT-6Ic7wKE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google import generativeai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API key\n",
    "generativeai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJpKGaRNBRXx"
   },
   "source": [
    "- `typing` imports provide type hints for better code clarity and IDE support.\n",
    "- `google.generativeai` is the official Google SDK for Gemini\n",
    "- `genai.configure()` sets up the API key for all subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-aBWPZOCp4BX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "- models/gemini-1.0-pro-vision-latest\n",
      "- models/gemini-pro-vision\n",
      "- models/gemini-1.5-pro-latest\n",
      "- models/gemini-1.5-pro-002\n",
      "- models/gemini-1.5-pro\n",
      "- models/gemini-1.5-flash-latest\n",
      "- models/gemini-1.5-flash\n",
      "- models/gemini-1.5-flash-002\n",
      "- models/gemini-1.5-flash-8b\n",
      "- models/gemini-1.5-flash-8b-001\n",
      "- models/gemini-1.5-flash-8b-latest\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash-preview-04-17\n",
      "- models/gemini-2.5-flash-preview-05-20\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-flash-preview-04-17-thinking\n",
      "- models/gemini-2.5-flash-lite-preview-06-17\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "print(\"Available Gemini models:\")\n",
    "for model in generativeai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnOWfNV9p4kO"
   },
   "source": [
    "- `genai.list_models()` returns all available models\n",
    "\n",
    "We filter for models that support `generateContent` (text generation). This helps identify which models support function calling\n",
    "\n",
    "### Basic function calling\n",
    "\n",
    "#### Define a mock function\n",
    "\n",
    "Let‚Äôs simulate a weather API function. This mock implementation returns fixed values and serves as a placeholder during development or testing. Mock implementation means that it doesn't connect to a real weather API. It simulates behavior using hardcoded values (Mock functions are useful during prototyping, testing, or tutorials where external API setup is unnecessary or unavailable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kUD-tAvCBRKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è  Getting weather for London, UK...\n",
      "Test result: {'location': 'London, UK', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n"
     ]
    }
   ],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get current weather information for a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state/country (e.g., \"New York, NY\")\n",
    "        unit: Temperature unit (\"celsius\" or \"fahrenheit\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing weather information\n",
    "    \"\"\"\n",
    "    # Simulate weather API call (in real implementation, we will use actual weather API)\n",
    "    weather_data = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": 22 if unit == \"celsius\" else 72,\n",
    "        \"unit\": unit,\n",
    "        \"description\": \"Partly cloudy\",\n",
    "        \"humidity\": 65,\n",
    "        \"wind_speed\": 10\n",
    "    }\n",
    "\n",
    "    print(f\"üå§Ô∏è  Getting weather for {location}...\")\n",
    "    return weather_data\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_result = get_current_weather(\"London, UK\")\n",
    "print(f\"Test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq6EZby-BQ4q"
   },
   "source": [
    "This function acts as a standalone unit of business logic that Gemini will eventually be able to call.\n",
    "- The function includes proper type hints for parameters and return values.\n",
    "  - Input: `location` as `str`, `unit` as optional `str` (default \"celsius\").\n",
    "  - Output: `Dict[str, Any]` for flexible, JSON-like structured return data.\n",
    "- The triple-quoted string immediately under the function is called a docstring. Docstring provides clear description of purpose, parameters, and return format.\n",
    "- Instead of making an actual API call, it uses hardcoded values: temperature varies depending on unit, Weather data like humidity, description, and wind speed are fixed.\n",
    "\n",
    "The function simulates an API call (in production, we would call a real weather service). This lays the groundwork for enabling the model to interact with external logic, such as APIs or backend services.\n",
    "\n",
    "#### Creating function schema\n",
    "To enable Gemini to understand and call a function, we need to formally describe that function in a way the model can interpret. This is done using a JSON schema, which defines what the function is called, what it does, and what inputs it expects ‚Äî including their types, required fields, and constraints.\n",
    "\n",
    "We can think of the schema like a contract between our function and the model. Gemini uses this schema to decide whether the function is relevant to a given user query, what arguments should be passed into the function, and whether it has enough information to perform a call.\n",
    "\n",
    "The schema does not include the actual implementation of the function ‚Äî only its signature and input specification. This is what we register with Gemini's tool interface so that the model can intelligently match user prompts to external tool logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gOlXkAKB4XD"
   },
   "outputs": [],
   "source": [
    "# Define the function declaration using proper JSON Schema\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Fetches the full current weather report including temperature, humidity, wind speed, and conditions.\",  # Helps the model understand when this tool should be used\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",  # The input to the function is expected to be a single object (i.e., a dictionary of named parameters)\n",
    "        \"properties\": {  # This defines the structure and validation for each expected parameter\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country (e.g., 'New York, NY')\"  # Human-readable guidance for the model\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],  # Only allow two possible values for temperature unit\n",
    "                \"description\": \"Temperature unit\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # The 'location' field must be provided by the model for the function to be called\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7b11Zt5B9zy"
   },
   "source": [
    "* The `\"name\"` field must match the actual function name in our Python code (`get_current_weather`).\n",
    "* The `\"description\"` is used by the model during inference. It helps Gemini know when to use this tool and explain it in internal decision-making.\n",
    "* The `\"parameters\"` key defines the input structure the model needs to provide when calling the function. It uses the JSON Schema standard ‚Äî a widely-used convention for describing and validating structured data.\n",
    "  * Each entry under `\"properties\"` represents one expected input to the function.\n",
    "  * `\"location\"` is a required string input ‚Äî this will be something like `\"London, UK\"`.\n",
    "  * `\"unit\"` is an optional string that can only be `\"celsius\"` or `\"fahrenheit\"` (thanks to the `\"enum\"` constraint).\n",
    "  * The `enum` ensures the model won't make up invalid units like `\"kelvin\"` or `\"degrees\"`.\n",
    "* The `\"required\"` list enforces that the function must be called with a valid `\"location\"` field. If the user prompt lacks this, Gemini will either skip the function or ask a follow-up to gather that missing piece.\n",
    "\n",
    "This schema does not provide default values. If we wanted the model to assume defaults automatically, we could explicitly include `\"default\"` fields in the schema ‚Äî but here it is kept minimal and clean.\n",
    "\n",
    "\n",
    "- **Schema Format**: Uses JSON Schema specification for parameter validation\n",
    "- **name**: Must match the actual function name exactly\n",
    "- **description**: Helps the model understand when to use this function\n",
    "- **parameters**: Defines the expected input structure\n",
    "- **properties**: Specifies each parameter's type and description\n",
    "- **required**: Lists mandatory parameters\n",
    "- **enum**: Restricts values to specific options\n",
    "- **default**: Provides fallback values for optional parameters\n",
    "\n",
    "#### Basic function call example\n",
    "Now that we have defined a schema for our function and implemented the logic, it's time to bring the whole pipeline together: letting Gemini recognize when to call a tool and extract arguments from natural language input.\n",
    "\n",
    "Function calling adds reasoning and interactivity to static prompts. Instead of hardcoding logic into prompts, we allow Gemini to choose when to use tools ‚Äî making your application smarter, modular, and easier to maintain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n7fa-IbeCIu6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tokyo, Japan'}\n",
      "üå§Ô∏è  Getting weather for Tokyo, Japan...\n",
      "‚úÖ Function result: {'location': 'Tokyo, Japan', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tokyo, Japan is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and wind speed of 10 km/h.\n"
     ]
    }
   ],
   "source": [
    "  # Register the function schema as a Gemini tool\n",
    "  client = genai.Client()  # Initialize the Gemini API client\n",
    "  # Wrap the schema in a Tool object ‚Äì this tells Gemini what functions it can potentially call\n",
    "  weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "  # Configuration for content generation ‚Äì includes registered tools\n",
    "  config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "  # Prompt\n",
    "  prompt = \"What's the weather like in Tokyo, Japan?\"\n",
    "\n",
    "  # Step 1: Send the request to Gemini, including the prompt and tool configuration\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=prompt,\n",
    "      config=config,  # This allows Gemini to consider available tools\n",
    "  )\n",
    "\n",
    "  # Step 2: Extract the model's decision (whether it wants to call a function/tool)\n",
    "  function_call = response.candidates[0].content.parts[0].function_call  # This is where Gemini tells us if it wants to call a tool\n",
    "\n",
    "  # Step 3: If the model decided to call a tool\n",
    "  if function_call:\n",
    "      print(f\"üîß Function to call: {function_call.name}\")\n",
    "      print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "      # Step 4: Execute the function locally with extracted arguments\n",
    "      if function_call.name == \"get_current_weather\":\n",
    "          result = get_current_weather(**dict(function_call.args))\n",
    "          print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "          # Send the function result back to the model to get natural language response\n",
    "          # Step 5: Create a new conversation with the function result\n",
    "          conversation = [\n",
    "              {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
    "              {\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]},\n",
    "              {\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                  \"name\": function_call.name,\n",
    "                  \"response\": result\n",
    "              }}]}\n",
    "          ]\n",
    "\n",
    "          # Step 6: Get the final natural language response\n",
    "          final_response = client.models.generate_content(\n",
    "              model=\"gemini-2.5-flash\",\n",
    "              contents=conversation,\n",
    "              config=config,\n",
    "          )\n",
    "\n",
    "          print(\"üí¨ Final natural language response:\")\n",
    "          for part in final_response.candidates[0].content.parts:\n",
    "              if hasattr(part, 'text') and part.text:\n",
    "                  print(f\"   Text: {part.text}\")\n",
    "              elif hasattr(part, 'thought_signature'):\n",
    "                  print(f\"   Thought: {part.thought_signature}\")\n",
    "              else:\n",
    "                  print(f\"   Other part type: {type(part)}\")\n",
    "  # If the model responded with plain text instead of a function call\n",
    "  else:\n",
    "      print(\"üí¨ No function call. Model responded:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ1IYFAgCSy_"
   },
   "source": [
    "- The function schema (`weather_function`) is registered as a tool using the `Tool` wrapper. This acts as a contract the Gemini model uses to recognize when and how to call this function. The model won‚Äôt use any external logic unless we explicitly register it this way.\n",
    "- The `GenerateContentConfig` binds the tools to the generation context. This ensures the model is aware of available tools at the time of content generation.\n",
    "- The prompt is a regular natural language question about the weather. The model processes this and decides whether to respond with text or invoke a function. Behind the scenes, Gemini analyzes the tools available and tries to match them to the user intent.\n",
    "- If the function name matches our implementation, we call it locally using Python‚Äôs unpacking (`**dict(...)`). This simulates what would happen in a real system, where backend logic would be triggered based on Gemini‚Äôs output.\n",
    "- After executing the function, the result (a dictionary) is formatted into a `function_response` block. This is sent back to Gemini as part of a new `conversation` payload, mimicking how it would see the result if calling an external system.\n",
    "\n",
    "This pattern ‚Äî prompt ‚Üí tool call ‚Üí local execution ‚Üí structured result ‚Üí natural language summary ‚Äî is the essence of tool-augmented generation in Gemini. It allows the model to go beyond static language generation and become an intelligent agent connected to real-world logic and data.\n",
    "\n",
    "### Function calling integration with ongoing conversations\n",
    "In real-world applications, users interact with AI assistants through ongoing conversations rather than isolated prompts. Function calling in Gemini is designed to work within these multi-turn exchanges, preserving context and memory as the chat evolves.\n",
    "\n",
    "Instead of rebuilding the conversation manually each time a function is triggered, a more scalable approach is to maintain a single, persistent conversation history. Every new user message and model response‚Äîwhether plain text, function call, or function response‚Äîis appended to this history. This allows Gemini to reason over the entire interaction timeline and make decisions based on full conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Z3pFswAlj1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Model response (no function call):\n",
      "   Text: Hello! How can I help you today?\n",
      "\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tel Aviv, Israel'}\n",
      "üå§Ô∏è  Getting weather for Tel Aviv, Israel...\n",
      "‚úÖ Function result: {'location': 'Tel Aviv, Israel', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tel Aviv, Israel is partly cloudy with a temperature of 22 degrees Celsius. The humidity is 65% and the wind speed is 10 km/h.\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Mumbai, India'}\n",
      "üå§Ô∏è  Getting weather for Mumbai, India...\n",
      "‚úÖ Function result: {'location': 'Mumbai, India', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Mumbai, India is partly cloudy with a temperature of 22 degrees Celsius. The humidity is 65% and the wind speed is 10 km/h.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini client and tool configuration\n",
    "client = genai.Client()\n",
    "\n",
    "# Register the function schema once\n",
    "weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "# Persistent conversation history\n",
    "conversation_history = []  # Keeps track of all chat turns\n",
    "\n",
    "def send_and_handle_message(message_text: str):\n",
    "    # Append the latest user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "    # Request model output with full chat history\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=conversation_history,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    candidate = response.candidates[0].content\n",
    "    function_call = candidate.parts[0].function_call if candidate.parts and hasattr(candidate.parts[0], 'function_call') else None\n",
    "\n",
    "    if function_call:\n",
    "        print(f\"üîß Function to call: {function_call.name}\")\n",
    "        print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "        if function_call.name == \"get_current_weather\":\n",
    "            result = get_current_weather(**dict(function_call.args))\n",
    "            print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "            # Add model's function call and user's function response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "            conversation_history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                \"name\": function_call.name,\n",
    "                \"response\": result\n",
    "            }}]})\n",
    "\n",
    "            # Ask Gemini to generate final reply considering everything so far\n",
    "            final_response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=conversation_history,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            print(\"üí¨ Final natural language response:\")\n",
    "            for part in final_response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   Text: {part.text}\")\n",
    "\n",
    "            # Add final model response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": final_response.candidates[0].content.parts})\n",
    "\n",
    "    else:\n",
    "        # If no function call, handle as a normal response\n",
    "        print(\"üí¨ Model response (no function call):\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   Text: {part.text}\")\n",
    "\n",
    "        conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage\n",
    "send_and_handle_message(\"Hi!\")\n",
    "send_and_handle_message(\"What's the weather like in Tel Aviv, Israel?\")\n",
    "send_and_handle_message(\"Thanks, and do you know what's the weather like in Mumbai, India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr64WZWpl0T-"
   },
   "source": [
    "- One persistent `conversation_history` list is used throughout multiple function calls and prompts.\n",
    "- All roles (`user`, `model`) and parts (`text`, `function_call`, `function_response`) are appended to this list as the conversation evolves."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
