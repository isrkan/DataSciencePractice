{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fnmz_uC7xhG"
   },
   "source": [
    "# Function calling with Gemini\n",
    "\n",
    "This notebook provides a guide to using Google's Gemini API for function calling. We will explore how to enable AI models to interact with external functions, APIs, and tools.\n",
    "\n",
    "Function calling (also known as tool calling) allows language models to interact with external functions, APIs, and tools. Instead of just generating text, the model can call predefined functions with appropriate parameters and make decisions about which functions to call based on context. It can receive function results and incorporate them into responses and continue the conversation with context from function outputs.\n",
    "\n",
    "### How function calling works\n",
    "1. **Function definition**: Define functions with clear schemas.\n",
    "2. **Model decision**: The model decides when and which function to call.\n",
    "3. **Parameter extraction**: The model extracts appropriate parameters from context.\n",
    "4. **Function execution**: The function is called with extracted.parameters.\n",
    "5. **Result integration**: Function results are integrated back into the conversation\n",
    "\n",
    "Function calling is useful because it allows access to real-time data like weather or stocks, enables integration with APIs or databases, can trigger actions such as sending emails or booking appointments, and returns structured data instead of plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qbqT-6Ic7wKE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google import generativeai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API key\n",
    "generativeai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJpKGaRNBRXx"
   },
   "source": [
    "- `typing` imports provide type hints for better code clarity and IDE support.\n",
    "- `google.generativeai` is the official Google SDK for Gemini\n",
    "- `genai.configure()` sets up the API key for all subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-aBWPZOCp4BX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "- models/gemini-1.0-pro-vision-latest\n",
      "- models/gemini-pro-vision\n",
      "- models/gemini-1.5-pro-latest\n",
      "- models/gemini-1.5-pro-002\n",
      "- models/gemini-1.5-pro\n",
      "- models/gemini-1.5-flash-latest\n",
      "- models/gemini-1.5-flash\n",
      "- models/gemini-1.5-flash-002\n",
      "- models/gemini-1.5-flash-8b\n",
      "- models/gemini-1.5-flash-8b-001\n",
      "- models/gemini-1.5-flash-8b-latest\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash-preview-05-20\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-flash-lite-preview-06-17\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "print(\"Available Gemini models:\")\n",
    "for model in generativeai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnOWfNV9p4kO"
   },
   "source": [
    "- `genai.list_models()` returns all available models\n",
    "\n",
    "We filter for models that support `generateContent` (text generation). This helps identify which models support function calling\n",
    "\n",
    "### Basic function calling\n",
    "\n",
    "#### Define a mock function\n",
    "\n",
    "Let‚Äôs simulate a weather API function. This mock implementation returns fixed values and serves as a placeholder during development or testing. Mock implementation means that it doesn't connect to a real weather API. It simulates behavior using hardcoded values (Mock functions are useful during prototyping, testing, or tutorials where external API setup is unnecessary or unavailable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kUD-tAvCBRKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è  Getting weather for London, UK...\n",
      "Test result: {'location': 'London, UK', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n"
     ]
    }
   ],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get current weather information for a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state/country (e.g., \"New York, NY\")\n",
    "        unit: Temperature unit (\"celsius\" or \"fahrenheit\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing weather information\n",
    "    \"\"\"\n",
    "    # Simulate weather API call (in real implementation, we will use actual weather API)\n",
    "    weather_data = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": 22 if unit == \"celsius\" else 72,\n",
    "        \"unit\": unit,\n",
    "        \"description\": \"Partly cloudy\",\n",
    "        \"humidity\": 65,\n",
    "        \"wind_speed\": 10\n",
    "    }\n",
    "\n",
    "    print(f\"üå§Ô∏è  Getting weather for {location}...\")\n",
    "    return weather_data\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_result = get_current_weather(\"London, UK\")\n",
    "print(f\"Test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq6EZby-BQ4q"
   },
   "source": [
    "This function acts as a standalone unit of business logic that Gemini will eventually be able to call.\n",
    "- The function includes proper type hints for parameters and return values.\n",
    "  - Input: `location` as `str`, `unit` as optional `str` (default \"celsius\").\n",
    "  - Output: `Dict[str, Any]` for flexible, JSON-like structured return data.\n",
    "- The triple-quoted string immediately under the function is called a docstring. Docstring provides clear description of purpose, parameters, and return format.\n",
    "- Instead of making an actual API call, it uses hardcoded values: temperature varies depending on unit, Weather data like humidity, description, and wind speed are fixed.\n",
    "\n",
    "The function simulates an API call (in production, we would call a real weather service). This lays the groundwork for enabling the model to interact with external logic, such as APIs or backend services.\n",
    "\n",
    "#### Creating function schema\n",
    "To enable Gemini to understand and call a function, we need to formally describe that function in a way the model can interpret. This is done using a JSON schema, which defines what the function is called, what it does, and what inputs it expects ‚Äî including their types, required fields, and constraints.\n",
    "\n",
    "We can think of the schema like a contract between our function and the model. Gemini uses this schema to decide whether the function is relevant to a given user query, what arguments should be passed into the function, and whether it has enough information to perform a call.\n",
    "\n",
    "The schema does not include the actual implementation of the function ‚Äî only its signature and input specification. This is what we register with Gemini's tool interface so that the model can intelligently match user prompts to external tool logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gOlXkAKB4XD"
   },
   "outputs": [],
   "source": [
    "# Define the function declaration using proper JSON Schema\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Fetches the full current weather report including temperature, humidity, wind speed, and conditions.\",  # Helps the model understand when this tool should be used\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",  # The input to the function is expected to be a single object (i.e., a dictionary of named parameters)\n",
    "        \"properties\": {  # This defines the structure and validation for each expected parameter\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country (e.g., 'New York, NY')\"  # Human-readable guidance for the model\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],  # Only allow two possible values for temperature unit\n",
    "                \"description\": \"Temperature unit\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # The 'location' field must be provided by the model for the function to be called\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7b11Zt5B9zy"
   },
   "source": [
    "* The `\"name\"` field must match the actual function name in our Python code (`get_current_weather`).\n",
    "* The `\"description\"` is used by the model during inference. It helps Gemini know when to use this tool and explain it in internal decision-making.\n",
    "* The `\"parameters\"` key defines the input structure the model needs to provide when calling the function. It uses the JSON Schema standard ‚Äî a widely-used convention for describing and validating structured data.\n",
    "  * Each entry under `\"properties\"` represents one expected input to the function.\n",
    "  * `\"location\"` is a required string input ‚Äî this will be something like `\"London, UK\"`.\n",
    "  * `\"unit\"` is an optional string that can only be `\"celsius\"` or `\"fahrenheit\"` (thanks to the `\"enum\"` constraint).\n",
    "  * The `enum` ensures the model won't make up invalid units like `\"kelvin\"` or `\"degrees\"`.\n",
    "* The `\"required\"` list enforces that the function must be called with a valid `\"location\"` field. If the user prompt lacks this, Gemini will either skip the function or ask a follow-up to gather that missing piece.\n",
    "\n",
    "This schema does not provide default values. If we wanted the model to assume defaults automatically, we could explicitly include `\"default\"` fields in the schema ‚Äî but here it is kept minimal and clean.\n",
    "\n",
    "\n",
    "- **Schema Format**: Uses JSON Schema specification for parameter validation\n",
    "- **name**: Must match the actual function name exactly\n",
    "- **description**: Helps the model understand when to use this function\n",
    "- **parameters**: Defines the expected input structure\n",
    "- **properties**: Specifies each parameter's type and description\n",
    "- **required**: Lists mandatory parameters\n",
    "- **enum**: Restricts values to specific options\n",
    "- **default**: Provides fallback values for optional parameters\n",
    "\n",
    "#### Basic function call example\n",
    "Now that we have defined a schema for our function and implemented the logic, it's time to bring the whole pipeline together: letting Gemini recognize when to call a tool and extract arguments from natural language input.\n",
    "\n",
    "Function calling adds reasoning and interactivity to static prompts. Instead of hardcoding logic into prompts, we allow Gemini to choose when to use tools ‚Äî making your application smarter, modular, and easier to maintain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n7fa-IbeCIu6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tokyo, Japan'}\n",
      "üå§Ô∏è  Getting weather for Tokyo, Japan...\n",
      "‚úÖ Function result: {'location': 'Tokyo, Japan', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tokyo, Japan is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n"
     ]
    }
   ],
   "source": [
    "  # Register the function schema as a Gemini tool\n",
    "  client = genai.Client()  # Initialize the Gemini API client\n",
    "  # Wrap the schema in a Tool object ‚Äì this tells Gemini what functions it can potentially call\n",
    "  weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "  # Configuration for content generation ‚Äì includes registered tools\n",
    "  config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "  # Prompt\n",
    "  prompt = \"What's the weather like in Tokyo, Japan?\"\n",
    "\n",
    "  # Step 1: Send the request to Gemini, including the prompt and tool configuration\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=prompt,\n",
    "      config=config,  # This allows Gemini to consider available tools\n",
    "  )\n",
    "\n",
    "  # Step 2: Extract the model's decision (whether it wants to call a function/tool)\n",
    "  function_call = response.candidates[0].content.parts[0].function_call  # This is where Gemini tells us if it wants to call a tool\n",
    "\n",
    "  # Step 3: If the model decided to call a tool\n",
    "  if function_call:\n",
    "      print(f\"üîß Function to call: {function_call.name}\")\n",
    "      print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "      # Step 4: Execute the function locally with extracted arguments\n",
    "      if function_call.name == \"get_current_weather\":\n",
    "          result = get_current_weather(**dict(function_call.args))\n",
    "          print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "          # Send the function result back to the model to get natural language response\n",
    "          # Step 5: Create a new conversation with the function result\n",
    "          conversation = [\n",
    "              {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
    "              {\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]},\n",
    "              {\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                  \"name\": function_call.name,\n",
    "                  \"response\": result\n",
    "              }}]}\n",
    "          ]\n",
    "\n",
    "          # Step 6: Get the final natural language response\n",
    "          final_response = client.models.generate_content(\n",
    "              model=\"gemini-2.5-flash\",\n",
    "              contents=conversation,\n",
    "              config=config,\n",
    "          )\n",
    "\n",
    "          print(\"üí¨ Final natural language response:\")\n",
    "          for part in final_response.candidates[0].content.parts:\n",
    "              if hasattr(part, 'text') and part.text:\n",
    "                  print(f\"   Text: {part.text}\")\n",
    "              elif hasattr(part, 'thought_signature'):\n",
    "                  print(f\"   Thought: {part.thought_signature}\")\n",
    "              else:\n",
    "                  print(f\"   Other part type: {type(part)}\")\n",
    "  # If the model responded with plain text instead of a function call\n",
    "  else:\n",
    "      print(\"üí¨ No function call. Model responded:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ1IYFAgCSy_"
   },
   "source": [
    "- The function schema (`weather_function`) is registered as a tool using the `Tool` wrapper. This acts as a contract the Gemini model uses to recognize when and how to call this function. The model won‚Äôt use any external logic unless we explicitly register it this way.\n",
    "- The `GenerateContentConfig` binds the tools to the generation context. This ensures the model is aware of available tools at the time of content generation.\n",
    "- The prompt is a regular natural language question about the weather. The model processes this and decides whether to respond with text or invoke a function. Behind the scenes, Gemini analyzes the tools available and tries to match them to the user intent.\n",
    "- If the function name matches our implementation, we call it locally using Python‚Äôs unpacking (`**dict(...)`). This simulates what would happen in a real system, where backend logic would be triggered based on Gemini‚Äôs output.\n",
    "- After executing the function, the result (a dictionary) is formatted into a `function_response` block. This is sent back to Gemini as part of a new `conversation` payload, mimicking how it would see the result if calling an external system.\n",
    "\n",
    "This pattern ‚Äî prompt ‚Üí tool call ‚Üí local execution ‚Üí structured result ‚Üí natural language summary ‚Äî is the essence of tool-augmented generation in Gemini. It allows the model to go beyond static language generation and become an intelligent agent connected to real-world logic and data.\n",
    "\n",
    "### Function calling integration with ongoing conversations\n",
    "In real-world applications, users interact with AI assistants through ongoing conversations rather than isolated prompts. Function calling in Gemini is designed to work within these multi-turn exchanges, preserving context and memory as the chat evolves.\n",
    "\n",
    "Instead of rebuilding the conversation manually each time a function is triggered, a more scalable approach is to maintain a single, persistent conversation history. Every new user message and model response‚Äîwhether plain text, function call, or function response‚Äîis appended to this history. This allows Gemini to reason over the entire interaction timeline and make decisions based on full conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Z3pFswAlj1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Model response (no function call):\n",
      "   Text: Hello! How can I help you today?\n",
      "\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tel Aviv, Israel'}\n",
      "üå§Ô∏è  Getting weather for Tel Aviv, Israel...\n",
      "‚úÖ Function result: {'location': 'Tel Aviv, Israel', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tel Aviv, Israel is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10.\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Mumbai, India'}\n",
      "üå§Ô∏è  Getting weather for Mumbai, India...\n",
      "‚úÖ Function result: {'location': 'Mumbai, India', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Mumbai, India is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini client and tool configuration\n",
    "client = genai.Client()\n",
    "\n",
    "# Register the function schema once\n",
    "weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "# Persistent conversation history\n",
    "conversation_history = []  # Keeps track of all chat turns\n",
    "\n",
    "def send_and_handle_message(message_text: str):\n",
    "    # Append the latest user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "    # Request model output with full chat history\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=conversation_history,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    candidate = response.candidates[0].content\n",
    "    function_call = candidate.parts[0].function_call if candidate.parts and hasattr(candidate.parts[0], 'function_call') else None\n",
    "\n",
    "    if function_call:\n",
    "        print(f\"üîß Function to call: {function_call.name}\")\n",
    "        print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "        if function_call.name == \"get_current_weather\":\n",
    "            result = get_current_weather(**dict(function_call.args))\n",
    "            print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "            # Add model's function call and user's function response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "            conversation_history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                \"name\": function_call.name,\n",
    "                \"response\": result\n",
    "            }}]})\n",
    "\n",
    "            # Ask Gemini to generate final reply considering everything so far\n",
    "            final_response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=conversation_history,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            print(\"üí¨ Final natural language response:\")\n",
    "            for part in final_response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   Text: {part.text}\")\n",
    "\n",
    "            # Add final model response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": final_response.candidates[0].content.parts})\n",
    "\n",
    "    else:\n",
    "        # If no function call, handle as a normal response\n",
    "        print(\"üí¨ Model response (no function call):\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   Text: {part.text}\")\n",
    "\n",
    "        conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage\n",
    "send_and_handle_message(\"Hi!\")\n",
    "send_and_handle_message(\"What's the weather like in Tel Aviv, Israel?\")\n",
    "send_and_handle_message(\"Thanks, and do you know what's the weather like in Mumbai, India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr64WZWpl0T-"
   },
   "source": [
    "- One persistent `conversation_history` list is used throughout multiple function calls and prompts.\n",
    "- All roles (`user`, `model`) and parts (`text`, `function_call`, `function_response`) are appended to this list as the conversation evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfOO_tKXn_aS"
   },
   "source": [
    "### Parallel function calling\n",
    "\n",
    "In many real-world scenarios, users ask questions that require multiple pieces of information simultaneously. For example, \"What's the weather in New York and London?\" or \"Get me the weather for Tokyo and also search for restaurants nearby.\" Instead of making sequential function calls, Gemini supports parallel function calling, which allows the model to invoke multiple functions at once, improving efficiency and user experience.\n",
    "\n",
    "Parallel function calling is particularly useful when:\n",
    "- Multiple independent data sources need to be queried\n",
    "- Functions don't depend on each other's results\n",
    "- We want to reduce response time by avoiding sequential execution\n",
    "- The user's query naturally requires multiple pieces of information\n",
    "\n",
    "#### Adding more functions for parallel calling\n",
    "Let's expand our toolkit by adding a restaurant search function alongside the existing weather function to demonstrate parallel calling with multiple different functions. As before, this is a simulated (mock) implementation, focusing on the interaction structure rather than using a real-world API. The goals here is to demonstrate how we can expand Gemini‚Äôs toolset with multiple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qi5sPawroE9h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "Test result: {'location': 'Paris, France', 'cuisine': 'french', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'french'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'french'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'french'}]}\n"
     ]
    }
   ],
   "source": [
    "def search_restaurants(location: str, cuisine: str = \"any\", price_range: str = \"moderate\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search for restaurants in a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state/country (e.g., \"New York, NY\")\n",
    "        cuisine: Type of cuisine (e.g., \"italian\", \"japanese\", \"mexican\")\n",
    "        price_range: Price range (\"budget\", \"moderate\", \"expensive\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing restaurant information\n",
    "    \"\"\"\n",
    "    # Mock restaurant data (in real implementation, use actual restaurant API)\n",
    "    restaurants = {\n",
    "        \"location\": location,\n",
    "        \"cuisine\": cuisine,\n",
    "        \"price_range\": price_range,\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"name\": \"Bella Vista\",\n",
    "                \"rating\": 4.5,\n",
    "                \"price\": \"$$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"Italian\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Golden Dragon\",\n",
    "                \"rating\": 4.2,\n",
    "                \"price\": \"$$$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"Chinese\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Local Bistro\",\n",
    "                \"rating\": 4.0,\n",
    "                \"price\": \"$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"American\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    print(f\"üçΩÔ∏è  Searching restaurants in {location}...\")\n",
    "    # Return structured data back to Gemini for integration into its reply\n",
    "    return restaurants\n",
    "\n",
    "# Test the function\n",
    "test_restaurants = search_restaurants(\"Paris, France\", \"french\")\n",
    "print(f\"Test result: {test_restaurants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S5-E28uoL2o"
   },
   "source": [
    "* `search_restaurants` is defined with three input parameters: `location` (required), and `cuisine` and `price_range` (both optional with default values). This structure mimics real-world function design for user customization.\n",
    "* Inside the function, a static dictionary simulates restaurant search results, returning structured information about three restaurant options. The cuisine type is dynamically adjusted if the user specifies it, otherwise default examples are used.\n",
    "* The function returns a Python dictionary formatted in a way Gemini can consume. This allows Gemini to turn structured function results into conversational text for the user.\n",
    "\n",
    "This prepares the foundation for integrating parallel tool use with Gemini: both weather lookup and restaurant search can now be included in the `GenerateContentConfig` tools list. Gemini can decide whether to call one or both tools depending on user input, rather than requiring sequential steps or manual triggers.\n",
    "\n",
    "#### Creating function schemas for parallel calling\n",
    "When building conversational systems that handle multiple function calls dynamically, it is essential that the model understands not just how to call a function, but exactly what parameters it expects. This ensures Gemini has a contract it can follow when suggesting arguments, validating input, and producing structured output that developers can trust.\n",
    "\n",
    "To achieve this, each function must be described using a function schema. These schemas specify the function name, a description of its purpose, the parameters it takes, and what types and constraints apply to those parameters. Now let's define the schema for our restaurant search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H5zshbxaoP-x"
   },
   "outputs": [],
   "source": [
    "# Define the schema for the restaurant search function ‚Äî tells Gemini how to call this tool correctly\n",
    "restaurant_function = {\n",
    "    \"name\": \"search_restaurants\",  # Internal function name Gemini uses to identify this tool\n",
    "    \"description\": \"Search for restaurants in a specific location with optional cuisine and price filters.\",  # Plain language description for reasoning\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",  # User must provide this as a plain text string\n",
    "                \"description\": \"City and country (e.g., 'Paris, France')\"\n",
    "            },\n",
    "            \"cuisine\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Type of cuisine (e.g., 'italian', 'japanese', 'mexican')\"  # Optional ‚Äî adds flexibility\n",
    "            },\n",
    "            \"price_range\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"budget\", \"moderate\", \"expensive\"],\n",
    "                \"description\": \"Price range preference\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # Only 'location' is mandatory for this function to work\n",
    "    }\n",
    "}\n",
    "\n",
    "# Register both weather and restaurant tools so Gemini can choose from both in a single conversation\n",
    "client = genai.Client()\n",
    "multi_tool = types.Tool(function_declarations=[weather_function, restaurant_function])  # Combines both schemas into one toolset\n",
    "config = types.GenerateContentConfig(tools=[multi_tool])  # Pass combined tools as part of generation configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzd-036xoVxJ"
   },
   "source": [
    "* `restaurant_function` contains metadata about the `search_restaurants` function, including the name Gemini uses to recognize and call it, a description that guides the model‚Äôs reasoning about when to use it, and the `parameters` structure, which follows a JSON Schema‚Äìstyle definition. This helps Gemini validate inputs automatically before even suggesting a function call.\n",
    "* Both `weather_function` (defined earlier) and `restaurant_function` are wrapped together in a `types.Tool` object. This tells Gemini it has two options available whenever content is generated.\n",
    "* `GenerateContentConfig` is initialized with the `multi_tool`. This configuration is passed to any content generation request where we want Gemini to have access to both tools simultaneously.\n",
    "\n",
    "These steps create a contract between our application logic and Gemini. If a parameter is left out, or a function isn‚Äôt registered, Gemini simply won‚Äôt use it ‚Äî making this schema step a critical part of maintaining reliability and predictable behavior.\n",
    "\n",
    "#### Parallel function calling example\n",
    "Here's how to handle parallel function calls when Gemini decides to call multiple functions simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iNaG6JtxoZL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parallel function calling:\n",
      "üìù User prompt: What's the weather like in Tokyo, Japan and can you also find some good sushi restaurants there?\n",
      "==================================================\n",
      "üîß Found 2 function call(s):\n",
      "\n",
      "  1. Function: get_current_weather\n",
      "     Arguments: {'location': 'Tokyo, Japan'}\n",
      "üå§Ô∏è  Getting weather for Tokyo, Japan...\n",
      "     Result: {'location': 'Tokyo, Japan', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "\n",
      "  2. Function: search_restaurants\n",
      "     Arguments: {'cuisine': 'sushi', 'location': 'Tokyo, Japan'}\n",
      "üçΩÔ∏è  Searching restaurants in Tokyo, Japan...\n",
      "     Result: {'location': 'Tokyo, Japan', 'cuisine': 'sushi', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'sushi'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'sushi'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'sushi'}]}\n",
      "\n",
      "üí¨ Final natural language response:\n",
      "   The weather in Tokyo, Japan is partly cloudy with a temperature of 22 degrees Celsius. The humidity is 65% and the wind speed is 10.\n",
      "\n",
      "Here are some sushi restaurants in Tokyo:\n",
      "\n",
      "*   **Bella Vista**: This restaurant has a rating of 4.5 stars and is in the moderate price range.\n",
      "*   **Golden Dragon**: This restaurant has a rating of 4.2 stars and is in the expensive price range.\n",
      "*   **Local Bistro**: This restaurant has a rating of 4 stars and is in the budget price range.\n"
     ]
    }
   ],
   "source": [
    "def handle_parallel_function_calls(prompt: str):\n",
    "    \"\"\"\n",
    "    Handle a prompt that might trigger multiple function calls simultaneously.\n",
    "    This function sends the prompt to Gemini, processes all detected function calls, executes them, and then asks Gemini to generate a final response based on all results.\n",
    "    \"\"\"\n",
    "    print(f\"üìù User prompt: {prompt}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Send the user prompt to Gemini with tool configuration enabled\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,  # Pass the raw user text as contents\n",
    "        config=config,  # Use the configuration with both tools registered\n",
    "    )\n",
    "\n",
    "    candidate = response.candidates[0].content  # Extract the primary response candidate\n",
    "\n",
    "    # Check if there are any function calls detected in the response parts\n",
    "    function_calls = []\n",
    "    for part in candidate.parts:\n",
    "        if hasattr(part, 'function_call') and part.function_call:\n",
    "            function_calls.append(part.function_call)\n",
    "\n",
    "    if function_calls:\n",
    "        print(f\"üîß Found {len(function_calls)} function call(s):\")\n",
    "\n",
    "        # Placeholder for collecting all results\n",
    "        function_results = []\n",
    "\n",
    "        # Execute the correct function based on function name\n",
    "        for i, function_call in enumerate(function_calls):\n",
    "            print(f\"\\n  {i+1}. Function: {function_call.name}\")\n",
    "            print(f\"     Arguments: {dict(function_call.args)}\")\n",
    "\n",
    "            # Execute the correct function based on function name\n",
    "            if function_call.name == \"get_current_weather\":\n",
    "                result = get_current_weather(**dict(function_call.args))\n",
    "            elif function_call.name == \"search_restaurants\":\n",
    "                result = search_restaurants(**dict(function_call.args))\n",
    "            else:\n",
    "                result = {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "            function_results.append((function_call, result))  # Store both call and result\n",
    "            print(f\"     Result: {result}\")\n",
    "\n",
    "        # Build conversation with all function calls and responses\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
    "            {\"role\": \"model\", \"parts\": [{\"function_call\": fc} for fc, _ in function_results]},\n",
    "        ]\n",
    "\n",
    "        # Prepare all function responses as user messages\n",
    "        function_response_parts = []\n",
    "        for function_call, result in function_results:\n",
    "            function_response_parts.append({\n",
    "                \"function_response\": {\n",
    "                    \"name\": function_call.name,\n",
    "                    \"response\": result\n",
    "                }\n",
    "            })\n",
    "\n",
    "        conversation.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "        # Ask Gemini to generate a final human-readable message based on all previous steps\n",
    "        final_response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=conversation,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        print(\"\\nüí¨ Final natural language response:\")\n",
    "        for part in final_response.candidates[0].content.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   {part.text}\")\n",
    "\n",
    "    else:\n",
    "        # Handle the case where Gemini produces only plain text with no function call\n",
    "        print(\"üí¨ No function calls. Model responded with:\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   {part.text}\")\n",
    "\n",
    "# Test parallel function calling with a query requiring both weather and restaurant info\n",
    "print(\"Testing parallel function calling:\")\n",
    "handle_parallel_function_calls(\"What's the weather like in Tokyo, Japan and can you also find some good sushi restaurants there?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdCAhqNFoiQN"
   },
   "source": [
    "Here, we define a function that takes a user prompt and sends it to Gemini, expecting that multiple function calls might be returned.\n",
    "* After sending the prompt, it loops through all response parts, looking for structured function\\_call objects Gemini might have generated. Each detected function call is appended to a list.\n",
    "* For each function call detected, it matches the function name against known handlers, and executes the corresponding function with the extracted arguments, capturing the result alongside the function call metadata.\n",
    "* Once all function calls have been handled, it constructs a structured conversation history, including the original user message, the model‚Äôs function call outputs, and the user‚Äôs function response messages.\n",
    "* Finally, it sends this complete conversation history back to Gemini to generate a single, coherent natural language response incorporating all retrieved data.\n",
    "\n",
    "#### Advanced parallel calling with conversation history\n",
    "For production applications, we will want to integrate parallel function calling with persistent conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uhF8TMjponzX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversation with Parallel Function Calling ===\n",
      "üí¨ Model response (no function calls):\n",
      "   Hello! How can I help you today?\n",
      "\n",
      "üîß Executing 2 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Rome, Italy'})\n",
      "üå§Ô∏è  Getting weather for Rome, Italy...\n",
      "  ‚Ä¢ search_restaurants({'cuisine': 'italian', 'location': 'Rome, Italy'})\n",
      "üçΩÔ∏è  Searching restaurants in Rome, Italy...\n",
      "üí¨ Model response:\n",
      "   The current weather in Rome, Italy is partly cloudy with a temperature of 22 degrees Celsius. The humidity is 65% and the wind speed is 10 km/h.\n",
      "\n",
      "Here are some Italian restaurants in Rome:\n",
      "\n",
      "*   **Bella Vista:** Rated 4.5 stars, with a moderate price range.\n",
      "*   **Local Bistro:** Rated 4 stars, with a budget-friendly price range.\n",
      "\n",
      "Please note that \"Golden Dragon\" was also listed, but it does not appear to be an Italian restaurant.\n",
      "üîß Executing 2 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Paris, France'})\n",
      "üå§Ô∏è  Getting weather for Paris, France...\n",
      "  ‚Ä¢ search_restaurants({'location': 'Paris, France', 'cuisine': 'french'})\n",
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "üí¨ Model response:\n",
      "   The current weather in Paris, France is partly cloudy with a temperature of 22 degrees Celsius. The humidity is 65% and the wind speed is 10 km/h.\n",
      "\n",
      "Here are some French restaurants in Paris:\n",
      "\n",
      "*   **Bella Vista:** Rated 4.5 stars, with a moderate price range.\n",
      "*   **Local Bistro:** Rated 4 stars, with a budget-friendly price range.\n",
      "\n",
      "Please note that \"Golden Dragon\" was also listed, but it does not appear to be a French restaurant.\n"
     ]
    }
   ],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self):\n",
    "        # Initialize the Gemini client and register both tools (functions)\n",
    "        self.client = genai.Client()\n",
    "        self.tools = types.Tool(function_declarations=[weather_function, restaurant_function])\n",
    "        self.config = types.GenerateContentConfig(tools=[self.tools])\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def execute_function(self, function_call):\n",
    "        \"\"\"Execute a function based on its name and arguments.\"\"\"\n",
    "        # Dispatch to the correct function based on the name provided by Gemini\n",
    "        if function_call.name == \"get_current_weather\":\n",
    "            return get_current_weather(**dict(function_call.args))\n",
    "        elif function_call.name == \"search_restaurants\":\n",
    "            return search_restaurants(**dict(function_call.args))\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "    def send_message(self, message_text: str):\n",
    "        \"\"\"Send a message and handle potential parallel function calls.\"\"\"\n",
    "        # Add the user's new message to the ongoing conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "        # Get model response within the context of the conversation so far\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=self.conversation_history,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        candidate = response.candidates[0].content  # Use the top response from Gemini\n",
    "\n",
    "        # Extract all function calls detected in the model‚Äôs response parts\n",
    "        function_calls = []\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'function_call') and part.function_call:\n",
    "                function_calls.append(part.function_call)\n",
    "\n",
    "        if function_calls:\n",
    "            print(f\"üîß Executing {len(function_calls)} function(s):\")\n",
    "\n",
    "            # Execute all functions\n",
    "            function_results = []  # Will store each function call with its execution result\n",
    "            for function_call in function_calls:\n",
    "                print(f\"  ‚Ä¢ {function_call.name}({dict(function_call.args)})\")\n",
    "                result = self.execute_function(function_call)  # Execute and capture result\n",
    "                function_results.append((function_call, result))\n",
    "\n",
    "            # Add model's function calls to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": [{\"function_call\": fc} for fc, _ in function_results]\n",
    "            })\n",
    "\n",
    "            # Add function responses to history\n",
    "            function_response_parts = []\n",
    "            for function_call, result in function_results:\n",
    "                function_response_parts.append({\n",
    "                    \"function_response\": {\n",
    "                        \"name\": function_call.name,\n",
    "                        \"response\": result\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            self.conversation_history.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "            # Ask Gemini again: now it has both the user‚Äôs original query and the function results in context\n",
    "            final_response = self.client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=self.conversation_history,\n",
    "                config=self.config,\n",
    "            )\n",
    "\n",
    "            print(\"üí¨ Model response:\")\n",
    "            for part in final_response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   {part.text}\")\n",
    "\n",
    "            # Add final response to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": final_response.candidates[0].content.parts\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # If Gemini just responds directly without asking to call any functions\n",
    "            print(\"üí¨ Model response (no function calls):\")\n",
    "            for part in candidate.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   {part.text}\")\n",
    "\n",
    "            # Add response to history\n",
    "            self.conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage of conversation manager with parallel function calling\n",
    "conversation_manager = ConversationManager()\n",
    "\n",
    "print(\"=== Conversation with Parallel Function Calling ===\")\n",
    "conversation_manager.send_message(\"Hello!\")\n",
    "conversation_manager.send_message(\"I'm planning a trip to Rome, Italy. Can you tell me the weather there and suggest some Italian restaurants?\")\n",
    "conversation_manager.send_message(\"Thanks! What about Paris, France? Weather and French restaurants please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY3DpC-hn8lm"
   },
   "source": [
    "#### Key points about parallel function calling\n",
    "\n",
    "- Efficiency: Multiple functions can be executed simultaneously, reducing overall response time\n",
    "- Natural language understanding: Gemini automatically identifies when multiple functions are needed from a single prompt\n",
    "- Conversation continuity: All function calls and responses are properly integrated into the conversation history\n",
    "\n",
    "Parallel function calling makes our AI assistant more capable of handling complex, multi-faceted queries while maintaining conversation flow and context. This is essential for building sophisticated AI applications that can efficiently interact with multiple data sources and services."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
