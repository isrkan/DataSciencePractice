{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fnmz_uC7xhG"
   },
   "source": [
    "# Function calling with Gemini\n",
    "\n",
    "This notebook provides a guide to using Google's Gemini API for function calling. We will explore how to enable AI models to interact with external functions, APIs, and tools.\n",
    "\n",
    "Function calling (also known as tool calling) allows language models to interact with external functions, APIs, and tools. Instead of just generating text, the model can call predefined functions with appropriate parameters and make decisions about which functions to call based on context. It can receive function results and incorporate them into responses and continue the conversation with context from function outputs.\n",
    "\n",
    "### How function calling works\n",
    "1. **Function definition**: Define functions with clear schemas.\n",
    "2. **Model decision**: The model decides when and which function to call.\n",
    "3. **Parameter extraction**: The model extracts appropriate parameters from context.\n",
    "4. **Function execution**: The function is called with extracted.parameters.\n",
    "5. **Result integration**: Function results are integrated back into the conversation\n",
    "\n",
    "Function calling is useful because it allows access to real-time data like weather or stocks, enables integration with APIs or databases, can trigger actions such as sending emails or booking appointments, and returns structured data instead of plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qbqT-6Ic7wKE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google import generativeai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API key\n",
    "generativeai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJpKGaRNBRXx"
   },
   "source": [
    "- `typing` imports provide type hints for better code clarity and IDE support.\n",
    "- `google.generativeai` is the official Google SDK for Gemini\n",
    "- `genai.configure()` sets up the API key for all subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-aBWPZOCp4BX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "- models/gemini-1.0-pro-vision-latest\n",
      "- models/gemini-pro-vision\n",
      "- models/gemini-1.5-pro-latest\n",
      "- models/gemini-1.5-pro-002\n",
      "- models/gemini-1.5-pro\n",
      "- models/gemini-1.5-flash-latest\n",
      "- models/gemini-1.5-flash\n",
      "- models/gemini-1.5-flash-002\n",
      "- models/gemini-1.5-flash-8b\n",
      "- models/gemini-1.5-flash-8b-001\n",
      "- models/gemini-1.5-flash-8b-latest\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash-preview-05-20\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-flash-lite-preview-06-17\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "print(\"Available Gemini models:\")\n",
    "for model in generativeai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnOWfNV9p4kO"
   },
   "source": [
    "- `genai.list_models()` returns all available models\n",
    "\n",
    "We filter for models that support `generateContent` (text generation). This helps identify which models support function calling\n",
    "\n",
    "### Basic function calling\n",
    "\n",
    "#### Define a mock function\n",
    "\n",
    "Let‚Äôs simulate a weather API function. This mock implementation returns fixed values and serves as a placeholder during development or testing. Mock implementation means that it doesn't connect to a real weather API. It simulates behavior using hardcoded values (Mock functions are useful during prototyping, testing, or tutorials where external API setup is unnecessary or unavailable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kUD-tAvCBRKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è  Getting weather for London, UK...\n",
      "Test result: {'location': 'London, UK', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n"
     ]
    }
   ],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get current weather information for a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state/country (e.g., \"New York, NY\")\n",
    "        unit: Temperature unit (\"celsius\" or \"fahrenheit\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing weather information\n",
    "    \"\"\"\n",
    "    # Simulate weather API call (in real implementation, we will use actual weather API)\n",
    "    weather_data = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": 22 if unit == \"celsius\" else 72,\n",
    "        \"unit\": unit,\n",
    "        \"description\": \"Partly cloudy\",\n",
    "        \"humidity\": 65,\n",
    "        \"wind_speed\": 10\n",
    "    }\n",
    "\n",
    "    print(f\"üå§Ô∏è  Getting weather for {location}...\")\n",
    "    return weather_data\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_result = get_current_weather(\"London, UK\")\n",
    "print(f\"Test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq6EZby-BQ4q"
   },
   "source": [
    "This function acts as a standalone unit of business logic that Gemini will eventually be able to call.\n",
    "- The function includes proper type hints for parameters and return values.\n",
    "  - Input: `location` as `str`, `unit` as optional `str` (default \"celsius\").\n",
    "  - Output: `Dict[str, Any]` for flexible, JSON-like structured return data.\n",
    "- The triple-quoted string immediately under the function is called a docstring. Docstring provides clear description of purpose, parameters, and return format.\n",
    "- Instead of making an actual API call, it uses hardcoded values: temperature varies depending on unit, Weather data like humidity, description, and wind speed are fixed.\n",
    "\n",
    "The function simulates an API call (in production, we would call a real weather service). This lays the groundwork for enabling the model to interact with external logic, such as APIs or backend services.\n",
    "\n",
    "#### Creating function schema\n",
    "To enable Gemini to understand and call a function, we need to formally describe that function in a way the model can interpret. This is done using a JSON schema, which defines what the function is called, what it does, and what inputs it expects ‚Äî including their types, required fields, and constraints.\n",
    "\n",
    "We can think of the schema like a contract between our function and the model. Gemini uses this schema to decide whether the function is relevant to a given user query, what arguments should be passed into the function, and whether it has enough information to perform a call.\n",
    "\n",
    "The schema does not include the actual implementation of the function ‚Äî only its signature and input specification. This is what we register with Gemini's tool interface so that the model can intelligently match user prompts to external tool logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5gOlXkAKB4XD"
   },
   "outputs": [],
   "source": [
    "# Define the function declaration using proper JSON Schema\n",
    "weather_function = {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Fetches the full current weather report including temperature, humidity, wind speed, and conditions.\",  # Helps the model understand when this tool should be used\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",  # The input to the function is expected to be a single object (i.e., a dictionary of named parameters)\n",
    "        \"properties\": {  # This defines the structure and validation for each expected parameter\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country (e.g., 'New York, NY')\"  # Human-readable guidance for the model\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],  # Only allow two possible values for temperature unit\n",
    "                \"description\": \"Temperature unit\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # The 'location' field must be provided by the model for the function to be called\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7b11Zt5B9zy"
   },
   "source": [
    "* The `\"name\"` field must match the actual function name in our Python code (`get_current_weather`).\n",
    "* The `\"description\"` is used by the model during inference. It helps Gemini know when to use this tool and explain it in internal decision-making.\n",
    "* The `\"parameters\"` key defines the input structure the model needs to provide when calling the function. It uses the JSON Schema standard ‚Äî a widely-used convention for describing and validating structured data.\n",
    "  * Each entry under `\"properties\"` represents one expected input to the function.\n",
    "  * `\"location\"` is a required string input ‚Äî this will be something like `\"London, UK\"`.\n",
    "  * `\"unit\"` is an optional string that can only be `\"celsius\"` or `\"fahrenheit\"` (thanks to the `\"enum\"` constraint).\n",
    "  * The `enum` ensures the model won't make up invalid units like `\"kelvin\"` or `\"degrees\"`.\n",
    "* The `\"required\"` list enforces that the function must be called with a valid `\"location\"` field. If the user prompt lacks this, Gemini will either skip the function or ask a follow-up to gather that missing piece.\n",
    "\n",
    "This schema does not provide default values. If we wanted the model to assume defaults automatically, we could explicitly include `\"default\"` fields in the schema ‚Äî but here it is kept minimal and clean.\n",
    "\n",
    "\n",
    "- **Schema Format**: Uses JSON Schema specification for parameter validation\n",
    "- **name**: Must match the actual function name exactly\n",
    "- **description**: Helps the model understand when to use this function\n",
    "- **parameters**: Defines the expected input structure\n",
    "- **properties**: Specifies each parameter's type and description\n",
    "- **required**: Lists mandatory parameters\n",
    "- **enum**: Restricts values to specific options\n",
    "- **default**: Provides fallback values for optional parameters\n",
    "\n",
    "#### Basic function call example\n",
    "Now that we have defined a schema for our function and implemented the logic, it's time to bring the whole pipeline together: letting Gemini recognize when to call a tool and extract arguments from natural language input.\n",
    "\n",
    "Function calling adds reasoning and interactivity to static prompts. Instead of hardcoding logic into prompts, we allow Gemini to choose when to use tools ‚Äî making your application smarter, modular, and easier to maintain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n7fa-IbeCIu6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tokyo, Japan'}\n",
      "üå§Ô∏è  Getting weather for Tokyo, Japan...\n",
      "‚úÖ Function result: {'location': 'Tokyo, Japan', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tokyo, Japan is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n"
     ]
    }
   ],
   "source": [
    "  # Register the function schema as a Gemini tool\n",
    "  client = genai.Client()  # Initialize the Gemini API client\n",
    "  # Wrap the schema in a Tool object ‚Äì this tells Gemini what functions it can potentially call\n",
    "  weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "  # Configuration for content generation ‚Äì includes registered tools\n",
    "  config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "  # Prompt\n",
    "  prompt = \"What's the weather like in Tokyo, Japan?\"\n",
    "\n",
    "  # Step 1: Send the request to Gemini, including the prompt and tool configuration\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=prompt,\n",
    "      config=config,  # This allows Gemini to consider available tools\n",
    "  )\n",
    "\n",
    "  # Step 2: Extract the model's decision (whether it wants to call a function/tool)\n",
    "  function_call = response.candidates[0].content.parts[0].function_call  # This is where Gemini tells us if it wants to call a tool\n",
    "\n",
    "  # Step 3: If the model decided to call a tool\n",
    "  if function_call:\n",
    "      print(f\"üîß Function to call: {function_call.name}\")\n",
    "      print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "      # Step 4: Execute the function locally with extracted arguments\n",
    "      if function_call.name == \"get_current_weather\":\n",
    "          result = get_current_weather(**dict(function_call.args))\n",
    "          print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "          # Send the function result back to the model to get natural language response\n",
    "          # Step 5: Create a new conversation with the function result\n",
    "          conversation = [\n",
    "              {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
    "              {\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]},\n",
    "              {\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                  \"name\": function_call.name,\n",
    "                  \"response\": result\n",
    "              }}]}\n",
    "          ]\n",
    "\n",
    "          # Step 6: Get the final natural language response\n",
    "          final_response = client.models.generate_content(\n",
    "              model=\"gemini-2.5-flash\",\n",
    "              contents=conversation,\n",
    "              config=config,\n",
    "          )\n",
    "\n",
    "          print(\"üí¨ Final natural language response:\")\n",
    "          for part in final_response.candidates[0].content.parts:\n",
    "              if hasattr(part, 'text') and part.text:\n",
    "                  print(f\"   Text: {part.text}\")\n",
    "              elif hasattr(part, 'thought_signature'):\n",
    "                  print(f\"   Thought: {part.thought_signature}\")\n",
    "              else:\n",
    "                  print(f\"   Other part type: {type(part)}\")\n",
    "  # If the model responded with plain text instead of a function call\n",
    "  else:\n",
    "      print(\"üí¨ No function call. Model responded:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ1IYFAgCSy_"
   },
   "source": [
    "- The function schema (`weather_function`) is registered as a tool using the `Tool` wrapper. This acts as a contract the Gemini model uses to recognize when and how to call this function. The model won‚Äôt use any external logic unless we explicitly register it this way.\n",
    "- The `GenerateContentConfig` binds the tools to the generation context. This ensures the model is aware of available tools at the time of content generation.\n",
    "- The prompt is a regular natural language question about the weather. The model processes this and decides whether to respond with text or invoke a function. Behind the scenes, Gemini analyzes the tools available and tries to match them to the user intent.\n",
    "- If the function name matches our implementation, we call it locally using Python‚Äôs unpacking (`**dict(...)`). This simulates what would happen in a real system, where backend logic would be triggered based on Gemini‚Äôs output.\n",
    "- After executing the function, the result (a dictionary) is formatted into a `function_response` block. This is sent back to Gemini as part of a new `conversation` payload, mimicking how it would see the result if calling an external system.\n",
    "\n",
    "This pattern ‚Äî prompt ‚Üí tool call ‚Üí local execution ‚Üí structured result ‚Üí natural language summary ‚Äî is the essence of tool-augmented generation in Gemini. It allows the model to go beyond static language generation and become an intelligent agent connected to real-world logic and data.\n",
    "\n",
    "### Function calling integration with ongoing conversations\n",
    "In real-world applications, users interact with AI assistants through ongoing conversations rather than isolated prompts. Function calling in Gemini is designed to work within these multi-turn exchanges, preserving context and memory as the chat evolves.\n",
    "\n",
    "Instead of rebuilding the conversation manually each time a function is triggered, a more scalable approach is to maintain a single, persistent conversation history. Every new user message and model response‚Äîwhether plain text, function call, or function response‚Äîis appended to this history. This allows Gemini to reason over the entire interaction timeline and make decisions based on full conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_Z3pFswAlj1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Model response (no function call):\n",
      "   Text: Hello! How can I help you today?\n",
      "\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Tel Aviv, Israel'}\n",
      "üå§Ô∏è  Getting weather for Tel Aviv, Israel...\n",
      "‚úÖ Function result: {'location': 'Tel Aviv, Israel', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Tel Aviv, Israel is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n",
      "\n",
      "üîß Function to call: get_current_weather\n",
      "üìã Arguments: {'location': 'Mumbai, India'}\n",
      "üå§Ô∏è  Getting weather for Mumbai, India...\n",
      "‚úÖ Function result: {'location': 'Mumbai, India', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "üí¨ Final natural language response:\n",
      "   Text: The weather in Mumbai, India is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini client and tool configuration\n",
    "client = genai.Client()\n",
    "\n",
    "# Register the function schema once\n",
    "weather_tool = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[weather_tool])\n",
    "\n",
    "# Persistent conversation history\n",
    "conversation_history = []  # Keeps track of all chat turns\n",
    "\n",
    "def send_and_handle_message(message_text: str):\n",
    "    # Append the latest user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "    # Request model output with full chat history\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=conversation_history,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    candidate = response.candidates[0].content\n",
    "    function_call = candidate.parts[0].function_call if candidate.parts and hasattr(candidate.parts[0], 'function_call') else None\n",
    "\n",
    "    if function_call:\n",
    "        print(f\"üîß Function to call: {function_call.name}\")\n",
    "        print(f\"üìã Arguments: {function_call.args}\")\n",
    "\n",
    "        if function_call.name == \"get_current_weather\":\n",
    "            result = get_current_weather(**dict(function_call.args))\n",
    "            print(\"‚úÖ Function result:\", result)\n",
    "\n",
    "            # Add model's function call and user's function response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": [{\"function_call\": function_call}]})\n",
    "            conversation_history.append({\"role\": \"user\", \"parts\": [{\"function_response\": {\n",
    "                \"name\": function_call.name,\n",
    "                \"response\": result\n",
    "            }}]})\n",
    "\n",
    "            # Ask Gemini to generate final reply considering everything so far\n",
    "            final_response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=conversation_history,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "            print(\"üí¨ Final natural language response:\")\n",
    "            for part in final_response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   Text: {part.text}\")\n",
    "\n",
    "            # Add final model response to history\n",
    "            conversation_history.append({\"role\": \"model\", \"parts\": final_response.candidates[0].content.parts})\n",
    "\n",
    "    else:\n",
    "        # If no function call, handle as a normal response\n",
    "        print(\"üí¨ Model response (no function call):\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   Text: {part.text}\")\n",
    "\n",
    "        conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage\n",
    "send_and_handle_message(\"Hi!\")\n",
    "send_and_handle_message(\"What's the weather like in Tel Aviv, Israel?\")\n",
    "send_and_handle_message(\"Thanks, and do you know what's the weather like in Mumbai, India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr64WZWpl0T-"
   },
   "source": [
    "- One persistent `conversation_history` list is used throughout multiple function calls and prompts.\n",
    "- All roles (`user`, `model`) and parts (`text`, `function_call`, `function_response`) are appended to this list as the conversation evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfOO_tKXn_aS"
   },
   "source": [
    "### Parallel function calling\n",
    "\n",
    "In many real-world scenarios, users ask questions that require multiple pieces of information simultaneously. For example, \"What's the weather in New York and London?\" or \"Get me the weather for Tokyo and also search for restaurants nearby.\" Instead of making sequential function calls, Gemini supports parallel function calling, which allows the model to invoke multiple functions at once, improving efficiency and user experience.\n",
    "\n",
    "Parallel function calling is particularly useful when:\n",
    "- Multiple independent data sources need to be queried\n",
    "- Functions don't depend on each other's results\n",
    "- We want to reduce response time by avoiding sequential execution\n",
    "- The user's query naturally requires multiple pieces of information\n",
    "\n",
    "#### Adding more functions for parallel calling\n",
    "Let's expand our toolkit by adding a restaurant search function alongside the existing weather function to demonstrate parallel calling with multiple different functions. As before, this is a simulated (mock) implementation, focusing on the interaction structure rather than using a real-world API. The goals here is to demonstrate how we can expand Gemini‚Äôs toolset with multiple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qi5sPawroE9h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "Test result: {'location': 'Paris, France', 'cuisine': 'french', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'french'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'french'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'french'}]}\n"
     ]
    }
   ],
   "source": [
    "def search_restaurants(location: str, cuisine: str = \"any\", price_range: str = \"moderate\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search for restaurants in a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state/country (e.g., \"New York, NY\")\n",
    "        cuisine: Type of cuisine (e.g., \"italian\", \"japanese\", \"mexican\")\n",
    "        price_range: Price range (\"budget\", \"moderate\", \"expensive\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing restaurant information\n",
    "    \"\"\"\n",
    "    # Mock restaurant data (in real implementation, use actual restaurant API)\n",
    "    restaurants = {\n",
    "        \"location\": location,\n",
    "        \"cuisine\": cuisine,\n",
    "        \"price_range\": price_range,\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"name\": \"Bella Vista\",\n",
    "                \"rating\": 4.5,\n",
    "                \"price\": \"$$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"Italian\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Golden Dragon\",\n",
    "                \"rating\": 4.2,\n",
    "                \"price\": \"$$$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"Chinese\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Local Bistro\",\n",
    "                \"rating\": 4.0,\n",
    "                \"price\": \"$\",\n",
    "                \"cuisine\": cuisine if cuisine != \"any\" else \"American\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    print(f\"üçΩÔ∏è  Searching restaurants in {location}...\")\n",
    "    # Return structured data back to Gemini for integration into its reply\n",
    "    return restaurants\n",
    "\n",
    "# Test the function\n",
    "test_restaurants = search_restaurants(\"Paris, France\", \"french\")\n",
    "print(f\"Test result: {test_restaurants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S5-E28uoL2o"
   },
   "source": [
    "* `search_restaurants` is defined with three input parameters: `location` (required), and `cuisine` and `price_range` (both optional with default values). This structure mimics real-world function design for user customization.\n",
    "* Inside the function, a static dictionary simulates restaurant search results, returning structured information about three restaurant options. The cuisine type is dynamically adjusted if the user specifies it, otherwise default examples are used.\n",
    "* The function returns a Python dictionary formatted in a way Gemini can consume. This allows Gemini to turn structured function results into conversational text for the user.\n",
    "\n",
    "This prepares the foundation for integrating parallel tool use with Gemini: both weather lookup and restaurant search can now be included in the `GenerateContentConfig` tools list. Gemini can decide whether to call one or both tools depending on user input, rather than requiring sequential steps or manual triggers.\n",
    "\n",
    "#### Creating function schemas for parallel calling\n",
    "When building conversational systems that handle multiple function calls dynamically, it is essential that the model understands not just how to call a function, but exactly what parameters it expects. This ensures Gemini has a contract it can follow when suggesting arguments, validating input, and producing structured output that developers can trust.\n",
    "\n",
    "To achieve this, each function must be described using a function schema. These schemas specify the function name, a description of its purpose, the parameters it takes, and what types and constraints apply to those parameters. Now let's define the schema for our restaurant search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H5zshbxaoP-x"
   },
   "outputs": [],
   "source": [
    "# Define the schema for the restaurant search function ‚Äî tells Gemini how to call this tool correctly\n",
    "restaurant_function = {\n",
    "    \"name\": \"search_restaurants\",  # Internal function name Gemini uses to identify this tool\n",
    "    \"description\": \"Search for restaurants in a specific location with optional cuisine and price filters.\",  # Plain language description for reasoning\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",  # User must provide this as a plain text string\n",
    "                \"description\": \"City and country (e.g., 'Paris, France')\"\n",
    "            },\n",
    "            \"cuisine\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Type of cuisine (e.g., 'italian', 'japanese', 'mexican')\"  # Optional ‚Äî adds flexibility\n",
    "            },\n",
    "            \"price_range\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"budget\", \"moderate\", \"expensive\"],\n",
    "                \"description\": \"Price range preference\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # Only 'location' is mandatory for this function to work\n",
    "    }\n",
    "}\n",
    "\n",
    "# Register both weather and restaurant tools so Gemini can choose from both in a single conversation\n",
    "client = genai.Client()\n",
    "multi_tool = types.Tool(function_declarations=[weather_function, restaurant_function])  # Combines both schemas into one toolset\n",
    "config = types.GenerateContentConfig(tools=[multi_tool])  # Pass combined tools as part of generation configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzd-036xoVxJ"
   },
   "source": [
    "* `restaurant_function` contains metadata about the `search_restaurants` function, including the name Gemini uses to recognize and call it, a description that guides the model‚Äôs reasoning about when to use it, and the `parameters` structure, which follows a JSON Schema‚Äìstyle definition. This helps Gemini validate inputs automatically before even suggesting a function call.\n",
    "* Both `weather_function` (defined earlier) and `restaurant_function` are wrapped together in a `types.Tool` object. This tells Gemini it has two options available whenever content is generated.\n",
    "* `GenerateContentConfig` is initialized with the `multi_tool`. This configuration is passed to any content generation request where we want Gemini to have access to both tools simultaneously.\n",
    "\n",
    "These steps create a contract between our application logic and Gemini. If a parameter is left out, or a function isn‚Äôt registered, Gemini simply won‚Äôt use it ‚Äî making this schema step a critical part of maintaining reliability and predictable behavior.\n",
    "\n",
    "#### Parallel function calling example\n",
    "Here's how to handle parallel function calls when Gemini decides to call multiple functions simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iNaG6JtxoZL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parallel function calling:\n",
      "üìù User prompt: What's the weather like in Tokyo, Japan and can you also find some good sushi restaurants there?\n",
      "==================================================\n",
      "üîß Found 2 function call(s):\n",
      "\n",
      "  1. Function: get_current_weather\n",
      "     Arguments: {'location': 'Tokyo, Japan'}\n",
      "üå§Ô∏è  Getting weather for Tokyo, Japan...\n",
      "     Result: {'location': 'Tokyo, Japan', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "\n",
      "  2. Function: search_restaurants\n",
      "     Arguments: {'cuisine': 'sushi', 'location': 'Tokyo, Japan'}\n",
      "üçΩÔ∏è  Searching restaurants in Tokyo, Japan...\n",
      "     Result: {'location': 'Tokyo, Japan', 'cuisine': 'sushi', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'sushi'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'sushi'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'sushi'}]}\n",
      "\n",
      "üí¨ Final natural language response:\n",
      "   The weather in Tokyo, Japan is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n",
      "\n",
      "Here are some good sushi restaurants in Tokyo:\n",
      "\n",
      "* Bella Vista ($$) - Rating: 4.5\n",
      "* Golden Dragon ($$$) - Rating: 4.2\n",
      "* Local Bistro ($) - Rating: 4.0\n"
     ]
    }
   ],
   "source": [
    "def handle_parallel_function_calls(prompt: str):\n",
    "    \"\"\"\n",
    "    Handle a prompt that might trigger multiple function calls simultaneously.\n",
    "    This function sends the prompt to Gemini, processes all detected function calls, executes them, and then asks Gemini to generate a final response based on all results.\n",
    "    \"\"\"\n",
    "    print(f\"üìù User prompt: {prompt}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Send the user prompt to Gemini with tool configuration enabled\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,  # Pass the raw user text as contents\n",
    "        config=config,  # Use the configuration with both tools registered\n",
    "    )\n",
    "\n",
    "    candidate = response.candidates[0].content  # Extract the primary response candidate\n",
    "\n",
    "    # Check if there are any function calls detected in the response parts\n",
    "    function_calls = []\n",
    "    for part in candidate.parts:\n",
    "        if hasattr(part, 'function_call') and part.function_call:\n",
    "            function_calls.append(part.function_call)\n",
    "\n",
    "    if function_calls:\n",
    "        print(f\"üîß Found {len(function_calls)} function call(s):\")\n",
    "\n",
    "        # Placeholder for collecting all results\n",
    "        function_results = []\n",
    "\n",
    "        # Execute the correct function based on function name\n",
    "        for i, function_call in enumerate(function_calls):\n",
    "            print(f\"\\n  {i+1}. Function: {function_call.name}\")\n",
    "            print(f\"     Arguments: {dict(function_call.args)}\")\n",
    "\n",
    "            # Execute the correct function based on function name\n",
    "            if function_call.name == \"get_current_weather\":\n",
    "                result = get_current_weather(**dict(function_call.args))\n",
    "            elif function_call.name == \"search_restaurants\":\n",
    "                result = search_restaurants(**dict(function_call.args))\n",
    "            else:\n",
    "                result = {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "            function_results.append((function_call, result))  # Store both call and result\n",
    "            print(f\"     Result: {result}\")\n",
    "\n",
    "        # Build conversation with all function calls and responses\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": prompt}]},\n",
    "            {\"role\": \"model\", \"parts\": [{\"function_call\": fc} for fc, _ in function_results]},\n",
    "        ]\n",
    "\n",
    "        # Prepare all function responses as user messages\n",
    "        function_response_parts = []\n",
    "        for function_call, result in function_results:\n",
    "            function_response_parts.append({\n",
    "                \"function_response\": {\n",
    "                    \"name\": function_call.name,\n",
    "                    \"response\": result\n",
    "                }\n",
    "            })\n",
    "\n",
    "        conversation.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "        # Ask Gemini to generate a final human-readable message based on all previous steps\n",
    "        final_response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=conversation,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        print(\"\\nüí¨ Final natural language response:\")\n",
    "        for part in final_response.candidates[0].content.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   {part.text}\")\n",
    "\n",
    "    else:\n",
    "        # Handle the case where Gemini produces only plain text with no function call\n",
    "        print(\"üí¨ No function calls. Model responded with:\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   {part.text}\")\n",
    "\n",
    "# Test parallel function calling with a query requiring both weather and restaurant info\n",
    "print(\"Testing parallel function calling:\")\n",
    "handle_parallel_function_calls(\"What's the weather like in Tokyo, Japan and can you also find some good sushi restaurants there?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdCAhqNFoiQN"
   },
   "source": [
    "Here, we define a function that takes a user prompt and sends it to Gemini, expecting that multiple function calls might be returned.\n",
    "* After sending the prompt, it loops through all response parts, looking for structured function\\_call objects Gemini might have generated. Each detected function call is appended to a list.\n",
    "* For each function call detected, it matches the function name against known handlers, and executes the corresponding function with the extracted arguments, capturing the result alongside the function call metadata.\n",
    "* Once all function calls have been handled, it constructs a structured conversation history, including the original user message, the model‚Äôs function call outputs, and the user‚Äôs function response messages.\n",
    "* Finally, it sends this complete conversation history back to Gemini to generate a single, coherent natural language response incorporating all retrieved data.\n",
    "\n",
    "#### Advanced parallel calling with conversation history\n",
    "For production applications, we will want to integrate parallel function calling with persistent conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uhF8TMjponzX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversation with Parallel Function Calling ===\n",
      "üí¨ Model response (no function calls):\n",
      "   Hello! How can I help you today?\n",
      "\n",
      "üîß Executing 2 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Rome, Italy'})\n",
      "üå§Ô∏è  Getting weather for Rome, Italy...\n",
      "  ‚Ä¢ search_restaurants({'cuisine': 'italian', 'location': 'Rome, Italy'})\n",
      "üçΩÔ∏è  Searching restaurants in Rome, Italy...\n",
      "üí¨ Model response:\n",
      "   The current weather in Rome, Italy is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n",
      "\n",
      "Here are some Italian restaurants in Rome:\n",
      "* Bella Vista: This restaurant has a rating of 4.5 stars and is in the moderate price range.\n",
      "* Golden Dragon: This restaurant has a rating of 4.2 stars and is in the expensive price range.\n",
      "* Local Bistro: This restaurant has a rating of 4 stars and is in the budget price range.\n",
      "üîß Executing 2 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Paris, France'})\n",
      "üå§Ô∏è  Getting weather for Paris, France...\n",
      "  ‚Ä¢ search_restaurants({'location': 'Paris, France', 'cuisine': 'french'})\n",
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "üí¨ Model response:\n",
      "   The current weather in Paris, France is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n",
      "\n",
      "Here are some French restaurants in Paris:\n",
      "* Bella Vista: This restaurant has a rating of 4.5 stars and is in the moderate price range.\n",
      "* Golden Dragon: This restaurant has a rating of 4.2 stars and is in the expensive price range.\n",
      "* Local Bistro: This restaurant has a rating of 4 stars and is in the budget price range.\n"
     ]
    }
   ],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self):\n",
    "        # Initialize the Gemini client and register both tools (functions)\n",
    "        self.client = genai.Client()\n",
    "        self.tools = types.Tool(function_declarations=[weather_function, restaurant_function])\n",
    "        self.config = types.GenerateContentConfig(tools=[self.tools])\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def execute_function(self, function_call):\n",
    "        \"\"\"Execute a function based on its name and arguments.\"\"\"\n",
    "        # Dispatch to the correct function based on the name provided by Gemini\n",
    "        if function_call.name == \"get_current_weather\":\n",
    "            return get_current_weather(**dict(function_call.args))\n",
    "        elif function_call.name == \"search_restaurants\":\n",
    "            return search_restaurants(**dict(function_call.args))\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "    def send_message(self, message_text: str):\n",
    "        \"\"\"Send a message and handle potential parallel function calls.\"\"\"\n",
    "        # Add the user's new message to the ongoing conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "        # Get model response within the context of the conversation so far\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=self.conversation_history,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        candidate = response.candidates[0].content  # Use the top response from Gemini\n",
    "\n",
    "        # Extract all function calls detected in the model‚Äôs response parts\n",
    "        function_calls = []\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'function_call') and part.function_call:\n",
    "                function_calls.append(part.function_call)\n",
    "\n",
    "        if function_calls:\n",
    "            print(f\"üîß Executing {len(function_calls)} function(s):\")\n",
    "\n",
    "            # Execute all functions\n",
    "            function_results = []  # Will store each function call with its execution result\n",
    "            for function_call in function_calls:\n",
    "                print(f\"  ‚Ä¢ {function_call.name}({dict(function_call.args)})\")\n",
    "                result = self.execute_function(function_call)  # Execute and capture result\n",
    "                function_results.append((function_call, result))\n",
    "\n",
    "            # Add model's function calls to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": [{\"function_call\": fc} for fc, _ in function_results]\n",
    "            })\n",
    "\n",
    "            # Add function responses to history\n",
    "            function_response_parts = []\n",
    "            for function_call, result in function_results:\n",
    "                function_response_parts.append({\n",
    "                    \"function_response\": {\n",
    "                        \"name\": function_call.name,\n",
    "                        \"response\": result\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            self.conversation_history.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "            # Ask Gemini again: now it has both the user‚Äôs original query and the function results in context\n",
    "            final_response = self.client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=self.conversation_history,\n",
    "                config=self.config,\n",
    "            )\n",
    "\n",
    "            print(\"üí¨ Model response:\")\n",
    "            for part in final_response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   {part.text}\")\n",
    "\n",
    "            # Add final response to history\n",
    "            self.conversation_history.append({\n",
    "                \"role\": \"model\",\n",
    "                \"parts\": final_response.candidates[0].content.parts\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # If Gemini just responds directly without asking to call any functions\n",
    "            print(\"üí¨ Model response (no function calls):\")\n",
    "            for part in candidate.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   {part.text}\")\n",
    "\n",
    "            # Add response to history\n",
    "            self.conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage of conversation manager with parallel function calling\n",
    "conversation_manager = ConversationManager()\n",
    "\n",
    "print(\"=== Conversation with Parallel Function Calling ===\")\n",
    "conversation_manager.send_message(\"Hello!\")\n",
    "conversation_manager.send_message(\"I'm planning a trip to Rome, Italy. Can you tell me the weather there and suggest some Italian restaurants?\")\n",
    "conversation_manager.send_message(\"Thanks! What about Paris, France? Weather and French restaurants please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY3DpC-hn8lm"
   },
   "source": [
    "#### Key points about parallel function calling\n",
    "\n",
    "- Efficiency: Multiple functions can be executed simultaneously, reducing overall response time\n",
    "- Natural language understanding: Gemini automatically identifies when multiple functions are needed from a single prompt\n",
    "- Conversation continuity: All function calls and responses are properly integrated into the conversation history\n",
    "\n",
    "Parallel function calling makes our AI assistant more capable of handling complex, multi-faceted queries while maintaining conversation flow and context. This is essential for building sophisticated AI applications that can efficiently interact with multiple data sources and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck5ozXwPt0Em"
   },
   "source": [
    "### Sequential function calling\n",
    "Sequential function calling occurs when the output of one function is needed as input for another function, creating a chain of dependent operations. Unlike parallel function calling where functions execute independently, sequential calling involves functions that depend on each other's results. This is common in workflows where we need to:\n",
    "- First search for a location, then get weather for that location\n",
    "- Get user preferences, then search for recommendations based on those preferences\n",
    "- Retrieve data from one API, then process it with another function\n",
    "- Perform multi-step calculations where each step depends on the previous result\n",
    "\n",
    "Sequential function calling enables complex workflows where Gemini can orchestrate multiple operations in the correct order, handling dependencies automatically.\n",
    "\n",
    "#### Adding functions for sequential calling\n",
    "Let's create functions that naturally work together in sequence. We will add a location search function and a travel recommendations function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mu9wUt0tt9EJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è  Searching for locations: romantic European cities\n",
      "Location search result: {'query': 'romantic European cities', 'location_type': 'city', 'results': [{'name': 'Paris, France', 'coordinates': {'lat': 48.8566, 'lng': 2.3522}, 'country': 'France', 'description': 'City of Love and lights'}, {'name': 'Rome, Italy', 'coordinates': {'lat': 41.9028, 'lng': 12.4964}, 'country': 'Italy', 'description': 'Eternal City with rich history'}, {'name': 'Prague, Czech Republic', 'coordinates': {'lat': 50.0755, 'lng': 14.4378}, 'country': 'Czech Republic', 'description': 'Beautiful medieval architecture'}]}\n",
      "üß≥ Getting travel recommendations for Paris, France...\n",
      "Travel recommendations result: {'location': 'Paris, France', 'interests': 'culture', 'duration': '3-5 days', 'attractions': ['Top cultural site in Paris, France', 'Famous museum in Paris, France', 'Historic landmark in Paris, France'], 'activities': ['Walking tour of Paris, France', 'Food tasting experience', 'Local art gallery visit'], 'restaurants': ['Michelin-starred restaurant in Paris, France', 'Traditional local cuisine spot', 'Trendy cafe for breakfast'], 'estimated_budget': '$500-800 per person'}\n"
     ]
    }
   ],
   "source": [
    "def search_locations(query: str, location_type: str = \"city\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search for locations based on a query.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (e.g., \"romantic cities in Europe\")\n",
    "        location_type: Type of location to search for (\"city\", \"country\", \"region\")\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing location search results\n",
    "    \"\"\"\n",
    "    # Mock location data (in real implementation, we will use actual location API)\n",
    "    location_results = {\n",
    "        \"query\": query,\n",
    "        \"location_type\": location_type,\n",
    "        \"results\": [\n",
    "            {\n",
    "                \"name\": \"Paris, France\",\n",
    "                \"coordinates\": {\"lat\": 48.8566, \"lng\": 2.3522},\n",
    "                \"country\": \"France\",\n",
    "                \"description\": \"City of Love and lights\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Rome, Italy\",\n",
    "                \"coordinates\": {\"lat\": 41.9028, \"lng\": 12.4964},\n",
    "                \"country\": \"Italy\",\n",
    "                \"description\": \"Eternal City with rich history\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Prague, Czech Republic\",\n",
    "                \"coordinates\": {\"lat\": 50.0755, \"lng\": 14.4378},\n",
    "                \"country\": \"Czech Republic\",\n",
    "                \"description\": \"Beautiful medieval architecture\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    print(f\"üó∫Ô∏è  Searching for locations: {query}\")\n",
    "    return location_results\n",
    "\n",
    "def get_travel_recommendations(location: str, interests: str = \"general\", duration: str = \"3-5 days\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get travel recommendations for a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and country (e.g., \"Paris, France\")\n",
    "        interests: Type of interests (\"culture\", \"food\", \"nightlife\", \"history\", \"nature\")\n",
    "        duration: Expected duration of stay\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing travel recommendations\n",
    "    \"\"\"\n",
    "    # Mock travel recommendations (in real implementation, we will use actual travel API)\n",
    "    recommendations = {\n",
    "        \"location\": location,\n",
    "        \"interests\": interests,\n",
    "        \"duration\": duration,\n",
    "        \"attractions\": [\n",
    "            f\"Top cultural site in {location}\",\n",
    "            f\"Famous museum in {location}\",\n",
    "            f\"Historic landmark in {location}\"\n",
    "        ],\n",
    "        \"activities\": [\n",
    "            f\"Walking tour of {location}\",\n",
    "            f\"Food tasting experience\",\n",
    "            f\"Local art gallery visit\"\n",
    "        ],\n",
    "        \"restaurants\": [\n",
    "            f\"Michelin-starred restaurant in {location}\",\n",
    "            f\"Traditional local cuisine spot\",\n",
    "            f\"Trendy cafe for breakfast\"\n",
    "        ],\n",
    "        \"estimated_budget\": \"$500-800 per person\"\n",
    "    }\n",
    "    print(f\"üß≥ Getting travel recommendations for {location}...\")\n",
    "    return recommendations\n",
    "\n",
    "# Test the functions\n",
    "test_locations = search_locations(\"romantic European cities\")\n",
    "print(f\"Location search result: {test_locations}\")\n",
    "\n",
    "test_recommendations = get_travel_recommendations(\"Paris, France\", \"culture\")\n",
    "print(f\"Travel recommendations result: {test_recommendations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tch-WeXTuGNm"
   },
   "source": [
    "Here, we defined Python functions returning mock dictionaries simulating real API responses.\n",
    "* `search_locations`:\n",
    "  * Accepts a query string and an optional location type (defaulting to \"city\").\n",
    "  * Returns a dictionary containing a fixed set of three mocked location results. This dictionary includes structured fields like name, coordinates, country, and description‚Äîmimicking how real location APIs might structure responses.\n",
    "* `get_travel_recommendations`:\n",
    "  * Accepts a specific location along with optional interest type and trip duration.\n",
    "  * Returns a mock dictionary representing travel recommendations, including lists of attractions, activities, restaurants, and a budget estimate.\n",
    "\n",
    "In a production Gemini setup, these two functions would be registered as tools, and Gemini would orchestrate the chaining automatically when user prompts indicate multi-step dependencies. This pattern sets the foundation for that kind of orchestrated, context-aware interaction.\n",
    "\n",
    "\n",
    "#### Creating function schemas for sequential calling\n",
    "When working with Gemini's function calling system, we don't just provide Python functions - we must also describe each function‚Äôs input parameters, constraints, and expected usage to the model. This is done using function schemas. These schemas let Gemini know what functions are available, what arguments each function accepts, what data types are involved, and which parameters are required.\n",
    "\n",
    "This is especially important when chaining functions sequentially. The model needs to understand exactly what information it must produce in its first function call (like a location name) so that it can properly use that output as input for the next function (such as getting travel recommendations). Let's define schemas for our new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SMEszbYvuMrR"
   },
   "outputs": [],
   "source": [
    "# Location search function schema\n",
    "location_search_function = {\n",
    "    \"name\": \"search_locations\",  # Function name Gemini will reference\n",
    "    \"description\": \"Search for locations based on a query and location type.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {  # Required user-provided search query\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query describing the type of location (e.g., 'romantic cities in Europe')\"\n",
    "            },\n",
    "            \"location_type\": {  # Optional parameter with fixed allowed values\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"city\", \"country\", \"region\"],\n",
    "                \"description\": \"Type of location to search for\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"]  # 'query' must always be provided when calling this function\n",
    "    }\n",
    "}\n",
    "\n",
    "# Travel recommendations function schema\n",
    "travel_recommendations_function = {\n",
    "    \"name\": \"get_travel_recommendations\",\n",
    "    \"description\": \"Get detailed travel recommendations for a specific location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {  # Required parameter defining the destination\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and country (e.g., 'Paris, France')\"\n",
    "            },\n",
    "            \"interests\": {  # Optional interest category with fixed choices\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"culture\", \"food\", \"nightlife\", \"history\", \"nature\", \"general\"],\n",
    "                \"description\": \"Type of interests for travel recommendations\"\n",
    "            },\n",
    "            \"duration\": {  # Optional trip duration parameter\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Expected duration of stay (e.g., '3-5 days', '1 week')\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # 'location' is mandatory when using this function\n",
    "    }\n",
    "}\n",
    "\n",
    "# Register all functions as tools\n",
    "client = genai.Client()  # Initialize the Gemini client\n",
    "# Combine all function schemas including weather, restaurant, and the two new sequential ones\n",
    "all_functions = [weather_function, restaurant_function, location_search_function, travel_recommendations_function]\n",
    "# Define a tool with all these functions so Gemini knows which are available for use\n",
    "sequential_tool = types.Tool(function_declarations=all_functions)\n",
    "# Configuration object that tells Gemini to use this toolset for content generation\n",
    "config = types.GenerateContentConfig(tools=[sequential_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95qCwMN1uP_m"
   },
   "source": [
    "Here, we define two function schemas as Python dictionaries, both following the expected OpenAPI-style format required by Gemini‚Äôs function calling API:\n",
    "  * name: Identifies the function Gemini can call.\n",
    "  * description: Helps Gemini understand when to use this function by clarifying its purpose.\n",
    "  * parameters: Defines the structure and constraints for the arguments the function accepts.\n",
    "    * Uses JSON Schema rules such as type, properties, required fields, and enum constraints where needed.\n",
    "\n",
    "* `search_locations` schema: Has two parameters: `query` (required) and `location_type` (optional, with restricted values).\n",
    "* `get_travel_recommendations` schema: Requires `location`, and optionally accepts `interests` (with predefined categories) and `duration`.\n",
    "* After both schemas are created, they are combined with two previously defined function schemas: `weather_function` and `restaurant_function`. These combined schemas are passed into `types.Tool` as a list. This creates a tool that exposes all four functions to Gemini.\n",
    "* Finally, this tool is packaged into a `GenerateContentConfig` object: This config is used whenever we call `generate_content` with Gemini so it knows which functions it can call, how to call them, and what inputs to expect.\n",
    "\n",
    "\n",
    "#### Basic sequential function calling example\n",
    "When orchestrating multiple function calls in a conversational flow, especially with models like Gemini, it is crucial to manage those calls systematically. This is where a control loop comes in. Gemini can suggest function calls based on user prompts, but to execute them‚Äîand feed the results back to the model - we need to handle both:\n",
    "- Detecting which function Gemini wants to call next.\n",
    "- Running that function in our Python environment and returning the result to Gemini.\n",
    "\n",
    "This becomes particularly important for sequential tasks. For example, a user might ask for a travel plan. Gemini might first call a location search, then use the location result to check the weather, and finally fetch travel recommendations. All those steps require managing the back-and-forth between user input, model output, function execution, and response building. Here is how to handle sequential function calls where one function's output feeds into another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "En5-mh0JuSPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sequential function calling:\n",
      "üìù User prompt: I want to plan a romantic trip to Europe for 5 days. Can you help me find a good destination, check the weather there, and give me travel recommendations?\n",
      "============================================================\n",
      "\n",
      "üîÑ Iteration 1\n",
      "------------------------------\n",
      "üîß Executing 1 function(s):\n",
      "  ‚Ä¢ search_locations({'location_type': 'city', 'query': 'romantic cities in Europe'})\n",
      "üó∫Ô∏è  Searching for locations: romantic cities in Europe\n",
      "    Result: {'query': 'romantic cities in Europe', 'location_type': 'city', 'results': [{'name': 'Paris, France', 'coordinates': {'lat': 48.8566, 'lng': 2.3522}, 'country': 'France', 'description': 'City of Love and lights'}, {'name': 'Rome, Italy', 'coordinates': {'lat': 41.9028, 'lng': 12.4964}, 'country': 'Italy', 'description': 'Eternal City with rich history'}, {'name': 'Prague, Czech Republic', 'coordinates': {'lat': 50.0755, 'lng': 14.4378}, 'country': 'Czech Republic', 'description': 'Beautiful medieval architecture'}]}\n",
      "\n",
      "üîÑ Iteration 2\n",
      "------------------------------\n",
      "üí≠ Model thinking:\n",
      "   Great choices! I found a few romantic cities in Europe:\n",
      "\n",
      "*   **Paris, France:** The City of Love and Lights.\n",
      "*   **Rome, Italy:** The Eternal City with a rich history.\n",
      "*   **Prague, Czech Republic:** Known for its beautiful medieval architecture.\n",
      "\n",
      "Which one would you like to choose for your 5-day trip? Once you decide, I can check the weather and give you some travel recommendations.\n",
      "üí¨ Final response:\n",
      "   Great choices! I found a few romantic cities in Europe:\n",
      "\n",
      "*   **Paris, France:** The City of Love and Lights.\n",
      "*   **Rome, Italy:** The Eternal City with a rich history.\n",
      "*   **Prague, Czech Republic:** Known for its beautiful medieval architecture.\n",
      "\n",
      "Which one would you like to choose for your 5-day trip? Once you decide, I can check the weather and give you some travel recommendations.\n"
     ]
    }
   ],
   "source": [
    "def handle_sequential_function_calls(prompt: str, max_iterations: int = 5):\n",
    "    \"\"\"\n",
    "    Handle a prompt that might trigger sequential function calls.\n",
    "\n",
    "    Args:\n",
    "        prompt: User's initial prompt\n",
    "        max_iterations: Maximum number of function call iterations to prevent infinite loops\n",
    "    \"\"\"\n",
    "    print(f\"üìù User prompt: {prompt}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize conversation list with user input as the first message\n",
    "    conversation = [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
    "\n",
    "    iteration = 0  # Counter to track iterations for safety\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\nüîÑ Iteration {iteration}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Send conversation history to Gemini and get response\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=conversation,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        # Extract the first candidate response from Gemini\n",
    "        candidate = response.candidates[0].content\n",
    "\n",
    "        # Prepare lists to hold function calls and plain text parts\n",
    "        function_calls = []\n",
    "        text_parts = []\n",
    "\n",
    "        # Loop through all response parts from Gemini\n",
    "        for part in candidate.parts:\n",
    "            # Check if Gemini requested a function call\n",
    "            if hasattr(part, 'function_call') and part.function_call:\n",
    "                function_calls.append(part.function_call)\n",
    "            # Check for regular text content from Gemini\n",
    "            elif hasattr(part, 'text') and part.text:\n",
    "                text_parts.append(part.text)\n",
    "\n",
    "        # If there are text parts, display them\n",
    "        if text_parts:\n",
    "            print(\"üí≠ Model thinking:\")\n",
    "            for text in text_parts:\n",
    "                print(f\"   {text}\")\n",
    "\n",
    "        # If there are function calls, execute them\n",
    "        if function_calls:\n",
    "            print(f\"üîß Executing {len(function_calls)} function(s):\")\n",
    "\n",
    "            # Add Gemini‚Äôs latest response to conversation history\n",
    "            conversation.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "            # Execute functions and collect results\n",
    "            function_results = []  # List to store function call results\n",
    "            for function_call in function_calls:\n",
    "                print(f\"  ‚Ä¢ {function_call.name}({dict(function_call.args)})\")\n",
    "\n",
    "                # Match Gemini‚Äôs function call to the correct Python function\n",
    "                if function_call.name == \"get_current_weather\":\n",
    "                    result = get_current_weather(**dict(function_call.args))\n",
    "                elif function_call.name == \"search_restaurants\":\n",
    "                    result = search_restaurants(**dict(function_call.args))\n",
    "                elif function_call.name == \"search_locations\":\n",
    "                    result = search_locations(**dict(function_call.args))\n",
    "                elif function_call.name == \"get_travel_recommendations\":\n",
    "                    result = get_travel_recommendations(**dict(function_call.args))\n",
    "                else:\n",
    "                    result = {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "                function_results.append((function_call, result))\n",
    "                print(f\"    Result: {result}\")\n",
    "\n",
    "            # Package function responses so Gemini can use them in the next step\n",
    "            function_response_parts = []\n",
    "            for function_call, result in function_results:\n",
    "                function_response_parts.append({\n",
    "                    \"function_response\": {\n",
    "                        \"name\": function_call.name,\n",
    "                        \"response\": result\n",
    "                    }\n",
    "                })\n",
    "            # Feed the function responses back into the conversation history\n",
    "            conversation.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "            # Continue to next iteration to see if more functions are needed\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # No function calls detected means Gemini has completed its workflow\n",
    "            print(\"üí¨ Final response:\")\n",
    "            for part in candidate.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(f\"   {part.text}\")\n",
    "\n",
    "            # Add final text output to conversation history\n",
    "            conversation.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "            break\n",
    "\n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"\\n‚ö†Ô∏è  Reached maximum iterations ({max_iterations})\")\n",
    "\n",
    "    return conversation\n",
    "\n",
    "# Test sequential function calling\n",
    "print(\"Testing sequential function calling:\")\n",
    "conversation = handle_sequential_function_calls(\n",
    "    \"I want to plan a romantic trip to Europe for 5 days. Can you help me find a good destination, check the weather there, and give me travel recommendations?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XWlPQOTuYbJ"
   },
   "source": [
    "In this function, we:\n",
    "* Initialize a structured chat history list using a dictionary format required by Gemini: user messages, model responses, and function calls are all recorded as conversation steps.\n",
    "* Enter a controlled while-loop capped by `max_iterations` to avoid runaway loops where the model keeps asking for more functions indefinitely.\n",
    "* For each iteration:\n",
    "  * Sends the full conversation context so far to Gemini via `generate_content()`.\n",
    "  * Parses Gemini‚Äôs response, separating plain text and `function_call` parts.\n",
    "  * If `function_call` parts exist:\n",
    "    * Matches the function call name against locally defined Python functions.\n",
    "    * Executes the matched function using the arguments Gemini provided.\n",
    "    * Packages the function‚Äôs result into a `function_response` format that Gemini expects and adds it to the conversation.\n",
    "  * If no function calls are detected, prints Gemini‚Äôs final text response.\n",
    "\n",
    "By the end, the conversation variable holds the full structured history of the interaction, including prompts, Gemini‚Äôs intermediate outputs, function calls, and final results. This is an essential pattern for managing complex multi-step workflows in production scenarios involving AI function calling systems.\n",
    "\n",
    "\n",
    "#### Advanced sequential calling with conversation management\n",
    "When building chat applications with multi-step workflows, maintaining conversation state becomes essential. In the earlier examples, function calling was handled in a one-off loop, but in practical use cases, we would want a reusable, structured way to manage both the conversation history and function execution. That‚Äôs where encapsulating everything into a class helps.\n",
    "\n",
    "Here is a more sophisticated approach that integrates sequential function calling with conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VKJLv8bruhoF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sequential Function Calling Demo ===\n",
      "üí¨ Model response:\n",
      "   Hi there! I can definitely help you with your trip planning. To give you the best recommendations, could you please tell me:\n",
      "\n",
      "1.  **Where are you planning to go?** (e.g., \"Paris, France\")\n",
      "2.  **How long will you be there?** (e.g., \"5 days\", \"a week\")\n",
      "3.  **What are your interests?** (e.g., \"culture\", \"food\", \"nightlife\", \"history\", \"nature\", or \"general sightseeing\")\n",
      "üìù User message: Please automatically pick the best romantic city in Europe for a 4-day trip, check its current weather, and provide me with detailed travel recommendations including restaurants. No need to ask me to choose; select one for me.\n",
      "============================================================\n",
      "\n",
      "üîÑ Processing iteration 1\n",
      "üîß Executing 1 function(s):\n",
      "  ‚Ä¢ search_locations({'location_type': 'city', 'query': 'romantic cities in Europe'})\n",
      "üó∫Ô∏è  Searching for locations: romantic cities in Europe\n",
      "    ‚úÖ Result: {'query': 'romantic cities in Europe', 'location_type': 'city', 'results': [{'name': 'Paris, France', 'coordinates': {'lat': 48.8566, 'lng': 2.3522}, 'country': 'France', 'description': 'City of Love and lights'}, {'name': 'Rome, Italy', 'coordinates': {'lat': 41.9028, 'lng': 12.4964}, 'country': 'Italy', 'description': 'Eternal City with rich history'}, {'name': 'Prague, Czech Republic', 'coordinates': {'lat': 50.0755, 'lng': 14.4378}, 'country': 'Czech Republic', 'description': 'Beautiful medieval architecture'}]}\n",
      "\n",
      "üîÑ Processing iteration 2\n",
      "üí≠ Model reasoning:\n",
      "   Okay, I've selected **Paris, France** as the romantic city for your 4-day trip.\n",
      "\n",
      "First, let's get the current weather in Paris:\n",
      "\n",
      "üîß Executing 1 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Paris, France'})\n",
      "üå§Ô∏è  Getting weather for Paris, France...\n",
      "    ‚úÖ Result: {'location': 'Paris, France', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "\n",
      "üîÑ Processing iteration 3\n",
      "üí≠ Model reasoning:\n",
      "   The current weather in Paris, France is partly cloudy with a temperature of 22 degrees Celsius, 65% humidity, and a wind speed of 10 km/h.\n",
      "\n",
      "Now, let's get you some travel recommendations for your 4-day trip to Paris and find some restaurants:\n",
      "\n",
      "üîß Executing 3 function(s):\n",
      "  ‚Ä¢ get_travel_recommendations({'location': 'Paris, France', 'duration': '4 days', 'interests': 'general'})\n",
      "üß≥ Getting travel recommendations for Paris, France...\n",
      "    ‚úÖ Result: {'location': 'Paris, France', 'interests': 'general', 'duration': '4 days', 'attractions': ['Top cultural site in Paris, France', 'Famous museum in Paris, France', 'Historic landmark in Paris, France'], 'activities': ['Walking tour of Paris, France', 'Food tasting experience', 'Local art gallery visit'], 'restaurants': ['Michelin-starred restaurant in Paris, France', 'Traditional local cuisine spot', 'Trendy cafe for breakfast'], 'estimated_budget': '$500-800 per person'}\n",
      "  ‚Ä¢ search_restaurants({'cuisine': 'French', 'location': 'Paris, France'})\n",
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "    ‚úÖ Result: {'location': 'Paris, France', 'cuisine': 'French', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'French'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'French'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'French'}]}\n",
      "  ‚Ä¢ search_restaurants({'location': 'Paris, France', 'cuisine': 'Italian'})\n",
      "üçΩÔ∏è  Searching restaurants in Paris, France...\n",
      "    ‚úÖ Result: {'location': 'Paris, France', 'cuisine': 'Italian', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'Italian'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'Italian'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'Italian'}]}\n",
      "\n",
      "üîÑ Processing iteration 4\n",
      "üí≠ Model reasoning:\n",
      "   Okay, for your romantic 4-day trip to Europe, I've chosen **Paris, France**!\n",
      "\n",
      "Here's a summary to help you plan:\n",
      "\n",
      "**Current Weather in Paris:**\n",
      "It's currently partly cloudy in Paris with a comfortable temperature of 22 degrees Celsius. The humidity is at 65%, and there's a light breeze with a wind speed of 10 km/h.\n",
      "\n",
      "**Travel Recommendations for Your 4-Day Trip to Paris:**\n",
      "*   **Activities:** Consider a romantic walking tour of Paris, a delightful food tasting experience, or a visit to a local art gallery to soak in the culture.\n",
      "*   **Attractions:** You'll definitely want to visit a top cultural site, a famous museum, and a historic landmark. (Think Eiffel Tower, The Louvre, Notre Dame Cathedral, etc.)\n",
      "*   **Estimated Budget:** For a general trip with these activities, an estimated budget would be around $500-800 per person.\n",
      "*   **Restaurants to Explore:**\n",
      "    *   **Recommended by Travel Guide:** Look out for a Michelin-starred restaurant for a special evening, a spot serving traditional local cuisine, or a trendy cafe for breakfast or brunch.\n",
      "    *   **French Cuisine Options:**\n",
      "        *   **Bella Vista:** Price: $$, Rating: 4.5\n",
      "        *   **Golden Dragon:** Price: $$$, Rating: 4.2\n",
      "        *   **Local Bistro:** Price: $, Rating: 4\n",
      "    *   **Italian Cuisine Options:**\n",
      "        *   **Bella Vista:** Price: $$, Rating: 4.5\n",
      "        *   **Golden Dragon:** Price: $$$, Rating: 4.2\n",
      "        *   **Local Bistro:** Price: $, Rating: 4\n",
      "\n",
      "Enjoy your romantic trip to Paris!\n",
      "üí¨ Final response:\n",
      "   Okay, for your romantic 4-day trip to Europe, I've chosen **Paris, France**!\n",
      "\n",
      "Here's a summary to help you plan:\n",
      "\n",
      "**Current Weather in Paris:**\n",
      "It's currently partly cloudy in Paris with a comfortable temperature of 22 degrees Celsius. The humidity is at 65%, and there's a light breeze with a wind speed of 10 km/h.\n",
      "\n",
      "**Travel Recommendations for Your 4-Day Trip to Paris:**\n",
      "*   **Activities:** Consider a romantic walking tour of Paris, a delightful food tasting experience, or a visit to a local art gallery to soak in the culture.\n",
      "*   **Attractions:** You'll definitely want to visit a top cultural site, a famous museum, and a historic landmark. (Think Eiffel Tower, The Louvre, Notre Dame Cathedral, etc.)\n",
      "*   **Estimated Budget:** For a general trip with these activities, an estimated budget would be around $500-800 per person.\n",
      "*   **Restaurants to Explore:**\n",
      "    *   **Recommended by Travel Guide:** Look out for a Michelin-starred restaurant for a special evening, a spot serving traditional local cuisine, or a trendy cafe for breakfast or brunch.\n",
      "    *   **French Cuisine Options:**\n",
      "        *   **Bella Vista:** Price: $$, Rating: 4.5\n",
      "        *   **Golden Dragon:** Price: $$$, Rating: 4.2\n",
      "        *   **Local Bistro:** Price: $, Rating: 4\n",
      "    *   **Italian Cuisine Options:**\n",
      "        *   **Bella Vista:** Price: $$, Rating: 4.5\n",
      "        *   **Golden Dragon:** Price: $$$, Rating: 4.2\n",
      "        *   **Local Bistro:** Price: $, Rating: 4\n",
      "\n",
      "Enjoy your romantic trip to Paris!\n",
      "\n",
      "‚úÖ Completed after 4 iteration(s)\n",
      "üí¨ Model response:\n",
      "   You're very welcome! I'm glad I could help.\n",
      "\n",
      "If you think of anything else you need for your trip, like more specific restaurant recommendations, or perhaps some ideas for day trips from Paris, just let me know! Enjoy your romantic getaway!\n"
     ]
    }
   ],
   "source": [
    "class SequentialConversationManager:\n",
    "    def __init__(self):\n",
    "        # Initialize Gemini client and configure available functions as tools\n",
    "        self.client = genai.Client()\n",
    "        self.all_functions = [\n",
    "            weather_function,\n",
    "            restaurant_function,\n",
    "            location_search_function,\n",
    "            travel_recommendations_function\n",
    "        ]\n",
    "        self.tools = types.Tool(function_declarations=self.all_functions)\n",
    "        self.config = types.GenerateContentConfig(tools=[self.tools])\n",
    "        # Store full conversation history including all prompts and responses\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def execute_function(self, function_call):\n",
    "        \"\"\"Execute a function based on its name and arguments.\"\"\"\n",
    "        function_map = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "            \"search_restaurants\": search_restaurants,\n",
    "            \"search_locations\": search_locations,\n",
    "            \"get_travel_recommendations\": get_travel_recommendations\n",
    "        }\n",
    "\n",
    "        # Match the function call name to the actual function and execute it\n",
    "        if function_call.name in function_map:\n",
    "            return function_map[function_call.name](**dict(function_call.args))\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown function: {function_call.name}\"}\n",
    "\n",
    "    def process_with_sequential_calls(self, message_text: str, max_iterations: int = 5):\n",
    "        \"\"\"\n",
    "        Process a message that might require sequential function calls.\n",
    "        \"\"\"\n",
    "        print(f\"üìù User message: {message_text}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Add user message to history for context tracking\n",
    "        self.conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\nüîÑ Processing iteration {iteration}\")\n",
    "\n",
    "            # Send full conversation so far to Gemini and get new content\n",
    "            response = self.client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=self.conversation_history,\n",
    "                config=self.config,\n",
    "            )\n",
    "\n",
    "            candidate = response.candidates[0].content\n",
    "\n",
    "            # Separate Gemini's plain text responses from its function call instructions\n",
    "            function_calls = []\n",
    "            text_parts = []\n",
    "\n",
    "            for part in candidate.parts:\n",
    "                if hasattr(part, 'function_call') and part.function_call:\n",
    "                    function_calls.append(part.function_call)\n",
    "                elif hasattr(part, 'text') and part.text:\n",
    "                    text_parts.append(part.text)\n",
    "\n",
    "            # Display any text reasoning\n",
    "            if text_parts:\n",
    "                print(\"üí≠ Model reasoning:\")\n",
    "                for text in text_parts:\n",
    "                    print(f\"   {text}\")\n",
    "\n",
    "            # If there are function calls, execute them\n",
    "            if function_calls:\n",
    "                print(f\"üîß Executing {len(function_calls)} function(s):\")\n",
    "\n",
    "                # Add model response to history before function execution\n",
    "                self.conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "                # Execute each requested function and collect results\n",
    "                function_results = []\n",
    "                for function_call in function_calls:\n",
    "                    print(f\"  ‚Ä¢ {function_call.name}({dict(function_call.args)})\")\n",
    "                    result = self.execute_function(function_call)\n",
    "                    function_results.append((function_call, result))\n",
    "                    print(f\"    ‚úÖ Result: {result}\")\n",
    "\n",
    "                # Add function results back to conversation so Gemini can use them in follow-ups\n",
    "                function_response_parts = []\n",
    "                for function_call, result in function_results:\n",
    "                    function_response_parts.append({\n",
    "                        \"function_response\": {\n",
    "                            \"name\": function_call.name,\n",
    "                            \"response\": result\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "                self.conversation_history.append({\"role\": \"user\", \"parts\": function_response_parts})\n",
    "\n",
    "                # Proceed to next iteration to check if further steps are needed\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # No more function calls‚ÄîGemini is ready to finalize its reply\n",
    "                print(\"üí¨ Final response:\")\n",
    "                for part in candidate.parts:\n",
    "                    if hasattr(part, 'text') and part.text:\n",
    "                        print(f\"   {part.text}\")\n",
    "\n",
    "                # Save final model message in history\n",
    "                self.conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "                break\n",
    "\n",
    "        if iteration >= max_iterations:\n",
    "            print(f\"\\n‚ö†Ô∏è  Reached maximum iterations ({max_iterations})\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Completed after {iteration} iteration(s)\")\n",
    "\n",
    "    def send_simple_message(self, message_text: str):\n",
    "        \"\"\"Send a simple message without sequential processing.\"\"\"\n",
    "        # Store plain user message in history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"parts\": [{\"text\": message_text}]})\n",
    "\n",
    "        # Get model response with no expectation of function calls\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=self.conversation_history,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        candidate = response.candidates[0].content\n",
    "        print(\"üí¨ Model response:\")\n",
    "        for part in candidate.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(f\"   {part.text}\")\n",
    "\n",
    "        # Update conversation history with model's plain text response\n",
    "        self.conversation_history.append({\"role\": \"model\", \"parts\": candidate.parts})\n",
    "\n",
    "# Example usage of sequential conversation manager\n",
    "sequential_manager = SequentialConversationManager()\n",
    "\n",
    "print(\"=== Sequential Function Calling Demo ===\")\n",
    "sequential_manager.send_simple_message(\"Hello! I'm planning a trip and need help.\")\n",
    "\n",
    "sequential_manager.process_with_sequential_calls(\n",
    "    \"Please automatically pick the best romantic city in Europe for a 4-day trip, check its current weather, and provide me with detailed travel recommendations including restaurants. No need to ask me to choose; select one for me.\"\n",
    ")\n",
    "\n",
    "sequential_manager.send_simple_message(\"Thanks! That sounds perfect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izfl_-egukkq"
   },
   "source": [
    "`SequentialConversationManager` is defined, encapsulating all related functionality:\n",
    "  * Initializing the Gemini client and registering all available function schemas as tools.\n",
    "  * Maintaining an internal list called `conversation_history` to record all user inputs, model outputs, function calls, and function results.\n",
    "* The `execute_function` method handles mapping Gemini‚Äôs requested function call names to actual Python function definitions, keeping the logic clean and centralized.\n",
    "* The core logic is in `process_with_sequential_calls`:\n",
    "  * Adds each user message to the conversation history.\n",
    "  * Iteratively sends updated conversation history to Gemini until either:\n",
    "    * No more function calls are detected (Gemini has provided its final output).\n",
    "    * The loop hits a maximum iteration count to avoid infinite chaining.\n",
    "  * For each iteration:\n",
    "    * Detects and executes function calls Gemini suggests.\n",
    "    * Packages function results in the correct format so Gemini can use them in the next model call.\n",
    "* The `send_simple_message` method provides a lighter interaction mode, sending plain prompts without managing function chaining.\n",
    "\n",
    "This setup supports building more advanced applications by providing clean separation of concerns, persistent state management, and modular function handling. It models a professional-grade approach, suitable for chatbot frameworks or service orchestration using Gemini‚Äôs function calling features.\n",
    "\n",
    "#### Real-world sequential calling example\n",
    "Here's a practical example showing how sequential function calling works in a realistic scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eCWSIdk4umdN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complex Travel Planning Workflow ===\n",
      "üìù User message: I want to visit a beautiful beach destination in the Mediterranean. Please suggest some options, check the current weather there to get an idea of the usual conditions, and provide me with detailed travel recommendations including restaurants and activities.\n",
      "============================================================\n",
      "\n",
      "üîÑ Processing iteration 1\n",
      "üîß Executing 1 function(s):\n",
      "  ‚Ä¢ search_locations({'location_type': 'city', 'query': 'beautiful beach destinations in the Mediterranean'})\n",
      "üó∫Ô∏è  Searching for locations: beautiful beach destinations in the Mediterranean\n",
      "    ‚úÖ Result: {'query': 'beautiful beach destinations in the Mediterranean', 'location_type': 'city', 'results': [{'name': 'Paris, France', 'coordinates': {'lat': 48.8566, 'lng': 2.3522}, 'country': 'France', 'description': 'City of Love and lights'}, {'name': 'Rome, Italy', 'coordinates': {'lat': 41.9028, 'lng': 12.4964}, 'country': 'Italy', 'description': 'Eternal City with rich history'}, {'name': 'Prague, Czech Republic', 'coordinates': {'lat': 50.0755, 'lng': 14.4378}, 'country': 'Czech Republic', 'description': 'Beautiful medieval architecture'}]}\n",
      "\n",
      "üîÑ Processing iteration 2\n",
      "üîß Executing 3 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Santorini, Greece'})\n",
      "üå§Ô∏è  Getting weather for Santorini, Greece...\n",
      "    ‚úÖ Result: {'location': 'Santorini, Greece', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "  ‚Ä¢ get_travel_recommendations({'interests': 'nature', 'location': 'Santorini, Greece'})\n",
      "üß≥ Getting travel recommendations for Santorini, Greece...\n",
      "    ‚úÖ Result: {'location': 'Santorini, Greece', 'interests': 'nature', 'duration': '3-5 days', 'attractions': ['Top cultural site in Santorini, Greece', 'Famous museum in Santorini, Greece', 'Historic landmark in Santorini, Greece'], 'activities': ['Walking tour of Santorini, Greece', 'Food tasting experience', 'Local art gallery visit'], 'restaurants': ['Michelin-starred restaurant in Santorini, Greece', 'Traditional local cuisine spot', 'Trendy cafe for breakfast'], 'estimated_budget': '$500-800 per person'}\n",
      "  ‚Ä¢ search_restaurants({'location': 'Santorini, Greece', 'price_range': 'moderate'})\n",
      "üçΩÔ∏è  Searching restaurants in Santorini, Greece...\n",
      "    ‚úÖ Result: {'location': 'Santorini, Greece', 'cuisine': 'any', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'Italian'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'Chinese'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'American'}]}\n",
      "\n",
      "üîÑ Processing iteration 3\n",
      "üîß Executing 3 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Ibiza, Spain'})\n",
      "üå§Ô∏è  Getting weather for Ibiza, Spain...\n",
      "    ‚úÖ Result: {'location': 'Ibiza, Spain', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "  ‚Ä¢ get_travel_recommendations({'interests': 'nightlife', 'location': 'Ibiza, Spain'})\n",
      "üß≥ Getting travel recommendations for Ibiza, Spain...\n",
      "    ‚úÖ Result: {'location': 'Ibiza, Spain', 'interests': 'nightlife', 'duration': '3-5 days', 'attractions': ['Top cultural site in Ibiza, Spain', 'Famous museum in Ibiza, Spain', 'Historic landmark in Ibiza, Spain'], 'activities': ['Walking tour of Ibiza, Spain', 'Food tasting experience', 'Local art gallery visit'], 'restaurants': ['Michelin-starred restaurant in Ibiza, Spain', 'Traditional local cuisine spot', 'Trendy cafe for breakfast'], 'estimated_budget': '$500-800 per person'}\n",
      "  ‚Ä¢ search_restaurants({'location': 'Ibiza, Spain', 'price_range': 'moderate'})\n",
      "üçΩÔ∏è  Searching restaurants in Ibiza, Spain...\n",
      "    ‚úÖ Result: {'location': 'Ibiza, Spain', 'cuisine': 'any', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'Italian'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'Chinese'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'American'}]}\n",
      "\n",
      "üîÑ Processing iteration 4\n",
      "üîß Executing 3 function(s):\n",
      "  ‚Ä¢ get_current_weather({'location': 'Nice, France'})\n",
      "üå§Ô∏è  Getting weather for Nice, France...\n",
      "    ‚úÖ Result: {'location': 'Nice, France', 'temperature': 22, 'unit': 'celsius', 'description': 'Partly cloudy', 'humidity': 65, 'wind_speed': 10}\n",
      "  ‚Ä¢ get_travel_recommendations({'interests': 'culture', 'location': 'Nice, France'})\n",
      "üß≥ Getting travel recommendations for Nice, France...\n",
      "    ‚úÖ Result: {'location': 'Nice, France', 'interests': 'culture', 'duration': '3-5 days', 'attractions': ['Top cultural site in Nice, France', 'Famous museum in Nice, France', 'Historic landmark in Nice, France'], 'activities': ['Walking tour of Nice, France', 'Food tasting experience', 'Local art gallery visit'], 'restaurants': ['Michelin-starred restaurant in Nice, France', 'Traditional local cuisine spot', 'Trendy cafe for breakfast'], 'estimated_budget': '$500-800 per person'}\n",
      "  ‚Ä¢ search_restaurants({'location': 'Nice, France', 'price_range': 'moderate'})\n",
      "üçΩÔ∏è  Searching restaurants in Nice, France...\n",
      "    ‚úÖ Result: {'location': 'Nice, France', 'cuisine': 'any', 'price_range': 'moderate', 'results': [{'name': 'Bella Vista', 'rating': 4.5, 'price': '$$', 'cuisine': 'Italian'}, {'name': 'Golden Dragon', 'rating': 4.2, 'price': '$$$', 'cuisine': 'Chinese'}, {'name': 'Local Bistro', 'rating': 4.0, 'price': '$', 'cuisine': 'American'}]}\n",
      "\n",
      "üîÑ Processing iteration 5\n",
      "üí≠ Model reasoning:\n",
      "   Here are some beautiful beach destinations in the Mediterranean, along with their current weather, travel recommendations, and restaurant suggestions:\n",
      "\n",
      "**1. Santorini, Greece**\n",
      "*   **Current Weather:** The weather in Santorini is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "**2. Ibiza, Spain**\n",
      "*   **Current Weather:** The weather in Ibiza is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks, with a focus on nightlife interests.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "**3. Nice, France**\n",
      "*   **Current Weather:** The weather in Nice is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks, with a focus on cultural interests.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "Do any of these destinations sound appealing, or would you like me to explore other options?\n",
      "\n",
      "üí¨ Final response:\n",
      "   Here are some beautiful beach destinations in the Mediterranean, along with their current weather, travel recommendations, and restaurant suggestions:\n",
      "\n",
      "**1. Santorini, Greece**\n",
      "*   **Current Weather:** The weather in Santorini is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "**2. Ibiza, Spain**\n",
      "*   **Current Weather:** The weather in Ibiza is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks, with a focus on nightlife interests.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "**3. Nice, France**\n",
      "*   **Current Weather:** The weather in Nice is partly cloudy, with a temperature of 22¬∞C, 65% humidity, and a wind speed of 10 km/h.\n",
      "*   **Travel Recommendations:** For a 3-5 day trip, an estimated budget is $500-800 per person. Activities include a walking tour, food tasting, and local art gallery visits. You can explore cultural sites, famous museums, and historic landmarks, with a focus on cultural interests.\n",
      "*   **Restaurant Suggestions (Moderate Price Range):**\n",
      "    *   Bella Vista (Italian, $$) - Rated 4.5 stars\n",
      "    *   Golden Dragon (Chinese, $$$) - Rated 4.2 stars\n",
      "    *   Local Bistro (American, $) - Rated 4 stars\n",
      "\n",
      "Do any of these destinations sound appealing, or would you like me to explore other options?\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Reached maximum iterations (5)\n",
      "\n",
      "‚úÖ Completed after 5 iteration(s)\n"
     ]
    }
   ],
   "source": [
    "# Example: Complex travel planning workflow\n",
    "print(\"\\n=== Complex Travel Planning Workflow ===\")\n",
    "\n",
    "# Create a new instance of SequentialConversationManager\n",
    "sequential_manager_2 = SequentialConversationManager()\n",
    "\n",
    "# This query will trigger multiple sequential function calls:\n",
    "# 1. First, search for locations matching \"beach destinations in Mediterranean\"\n",
    "# 2. Then, get weather for one or more of those locations\n",
    "# 3. Finally, get travel recommendations for the selected location\n",
    "sequential_manager_2.process_with_sequential_calls(\n",
    "    \"I want to visit a beautiful beach destination in the Mediterranean. \"\n",
    "    \"Please suggest some options, check the current weather there to get an idea of the usual conditions, \"\n",
    "    \"and provide me with detailed travel recommendations including restaurants and activities.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsQGLPlQtv2F"
   },
   "source": [
    "#### Key points about sequential function calling\n",
    "- **Dependency management**: Functions are called in the correct order based on their dependencies\n",
    "- **Iterative processing**: The model can make multiple rounds of function calls as needed\n",
    "- **Context preservation**: Each function call has access to the results of previous calls\n",
    "- **Flexible workflows**: The model determines the sequence dynamically based on the user's request\n",
    "- **Error handling**: If one function fails, the model can adapt and continue with available information\n",
    "- **Conversation continuity**: All function calls and responses are integrated into the conversation history\n",
    "\n",
    "Sequential function calling enables sophisticated AI workflows where multiple operations are orchestrated automatically, making our assistant capable of handling complex, multi-step tasks that require careful coordination between different functions and data sources."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
