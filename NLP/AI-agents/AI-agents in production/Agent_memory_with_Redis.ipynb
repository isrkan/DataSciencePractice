{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7Wg_mvqQvH4"
   },
   "source": [
    "# AI agent memory with Redis\n",
    "\n",
    "Modern AI agents face a fundamental limitation: they operate in isolation, treating each conversation as an entirely new interaction without any recollection of past exchanges or learned user preferences. This creates a frustrating user experience where agents repeatedly ask for the same information and fail to build upon previous interactions.\n",
    "\n",
    "The solution lies in implementing memory systems that mirror human cognitive architecture. Just as humans maintain both working memory (for immediate tasks) and long-term memory (for accumulated knowledge and experiences), AI agents need dual memory systems to become truly intelligent and personalized.\n",
    "\n",
    "This notebook demonstrates a practical, production-minded pattern for dual-memory agents: short-term memory for immediate context (what just happened in the conversation) and long-term memory for persistent facts (user preferences, episodic experiences, and domain knowledge). We will implement both episodic memory (personal experiences and preferences) and semantic memory (general knowledge), while managing conversation context efficiently to prevent information overload.\n",
    "\n",
    "We will use Redis for fast storage and indexing, LangGraph (and its checkpointer) for short-term check-pointing in agent pipelines, and a vector-enabled Redis client for semantic search.\n",
    "\n",
    "\n",
    "### Dual memory systems\n",
    "Our system implements two complementary memory types that work together to create intelligent, context-aware agents.\n",
    "\n",
    "#### Short-term memory (working memory)\n",
    "- Manages immediate conversation context and temporary state\n",
    "- Implemented using LangGraph's checkpointer with Redis backend\n",
    "- Automatically summarizes long conversations to prevent context overflow\n",
    "- Enables agents to maintain coherent dialogue within sessions\n",
    "\n",
    "#### Long-term memory (persistent knowledge)\n",
    "- Stores accumulated knowledge across all interactions\n",
    "- Uses vector embeddings for semantic similarity search\n",
    "- Categorized into episodic (personal) and semantic (general) knowledge\n",
    "- Enables personalization and continuous learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qss1kYuaJDZ-"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install langchain-openai langgraph-checkpoint langgraph langgraph-checkpoint-redis langchain-redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5waigKtNXToY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from redis import Redis\n",
    "import getpass\n",
    "import ulid\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Union, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from redisvl.index import SearchIndex\n",
    "from redisvl.schema.schema import IndexSchema\n",
    "from redisvl.utils.vectorize.text.openai import OpenAITextVectorizer\n",
    "from redisvl.query import VectorRangeQuery\n",
    "from redisvl.query.filter import Tag\n",
    "import logging\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage, RemoveMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt.chat_agent_executor import create_react_agent\n",
    "from langgraph.checkpoint.redis import RedisSaver\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUTTmHezWxP2"
   },
   "source": [
    "- `langgraph-checkpoint`: Persistent state management across conversations.\n",
    "- `langgraph-checkpoint-redis`: Redis backend for checkpoint storage.\n",
    "- `langchain-redis`: Vector storage and similarity search capabilities.\n",
    "\n",
    "#### API key configuration\n",
    "LLM calls require an API key. We avoid hard-coding secrets in notebooks. The small helper below prompts for the key (secure prompt in a Jupyter environment) and stores it in `os.environ` for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPVzyop9jAn6",
    "outputId": "cc52360f-e0aa-4ca5-ac10-f7daf8b4f691"
   },
   "outputs": [],
   "source": [
    "def _set_env(key: str):\n",
    "    \"\"\"\n",
    "    Securely set environment variables for API access. Prompts for input if the key doesn't already exist in the environment.\n",
    "    \"\"\"\n",
    "    if key not in os.environ:\n",
    "        # In notebooks we use getpass.getpass to avoid echoing secrets into the output.\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "# Configure OpenAI API access for LLM operations and embeddings\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtVWL1IGYoWa"
   },
   "source": [
    "### Redis infrastructure setup\n",
    "Redis serves as the backbone of our memory system, providing both high-performance caching for short-term memory and vector storage capabilities for long-term memory. We will set up a local Redis instance for development, though the same code works with Redis Cloud for production deployments.\n",
    "\n",
    "The cell below shows a shell snippet that installs and starts `redis-stack-server` on Debian/Ubuntu systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uIHJh4Aip0-",
    "outputId": "6ee25b3a-a05a-41dd-9940-c956ee8c2f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb jammy main\n",
      "Starting redis-stack-server, database path /var/lib/redis-stack\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# Install and configure Redis Stack Server for local development\n",
    "# Redis Stack includes vector search capabilities needed for semantic memory\n",
    "\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
    "\n",
    "# Update package list and install Redis Stack (includes RedisSearch for vector operations)\n",
    "sudo apt-get update  > /dev/null 2>&1\n",
    "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
    "\n",
    "# Start Redis as a background daemon\n",
    "redis-stack-server --daemonize yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34UW_sqSZ2ta"
   },
   "source": [
    "Redis Stack includes RedisSearch and RedisJSON modules that provide native vector search capabilities. This eliminates the need for separate vector databases while maintaining the performance benefits of in-memory operations for frequently accessed data.\n",
    "\n",
    "Now let's establish Python Redis client and verify our Redis connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Es5xfbhvi3PT",
    "outputId": "aff14015-95a2-4d80-a1b6-7c2ddb2a7d92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the environment variable if set, otherwise default to localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "\n",
    "# Create and test the Redis client\n",
    "redis_client = Redis.from_url(REDIS_URL)\n",
    "redis_client.ping()  # ping() will raise a ConnectionError if Redis is not reachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59x643BbbOdT"
   },
   "source": [
    "We use the `redis` Python client and support an environment variable `REDIS_URL` so you can point to Redis Cloud or a local host:port.\n",
    "\n",
    "### Memory data models\n",
    "A robust agent memory system starts with clear data structures. We will create a comprehensive type hierarchy that ensures data consistency and provides clear interfaces for memory operations throughout our agent system.\n",
    "\n",
    "#### Memory types\n",
    "Before implementing our models, it's important to understand the cognitive science concepts we're modeling. Human memory research distinguishes between different types of long-term memory, and we're implementing two key categories that are most relevant for AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J-WZA2Eedj1I"
   },
   "outputs": [],
   "source": [
    "class MemoryType(str, Enum):\n",
    "    \"\"\"\n",
    "    Defines the type of long-term memory for categorization and retrieval.\n",
    "\n",
    "    EPISODIC: Personal experiences and user-specific preferences\n",
    "              (e.g., \"User prefers Delta airlines\", \"User visited Paris last year\")\n",
    "\n",
    "    SEMANTIC: General domain knowledge and facts on top of the user's preferences and LLM's training data.\n",
    "              (e.g., \"Singapore requires passport\", \"Tokyo has excellent public transit\")\n",
    "    \"\"\"\n",
    "\n",
    "    EPISODIC = \"episodic\"\n",
    "    SEMANTIC = \"semantic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvXhB-3cdxgG"
   },
   "source": [
    "The distinction between episodic and semantic memory is crucial for agent behavior. Episodic memories drive personalization, while semantic memories provide factual grounding. This separation allows us to weight these memory types differently during retrieval and update them through different mechanisms. `MemoryType` is an `Enum[str]` so the values are serializable and easy to compare.\n",
    "\n",
    "#### Core memory models\n",
    "Now we will implement the data structures that represent individual memories and collections of memories throughout their lifecycle. Using Pydantic gives us validation, helpful error messages, and straightforward JSON (de)serialization for storing/retrieving from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7eFaADwljD5q"
   },
   "outputs": [],
   "source": [
    "class Memory(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single long-term memory item.\n",
    "\n",
    "    - content: textual content of the memory\n",
    "    - memory_type: either EPISODIC or SEMANTIC\n",
    "    - metadata: JSON-serializable string for arbitrary metadata (we keep it string-typed here for simplicity)\n",
    "    \"\"\"\n",
    "\n",
    "    content: str  # The actual memory content in natural language\n",
    "    memory_type: MemoryType  # Classification for retrieval and processing\n",
    "    metadata: str  # Additional context information for the memory\n",
    "\n",
    "\n",
    "class Memories(BaseModel):\n",
    "    \"\"\"\n",
    "    A list of memories extracted from a conversation by an LLM.\n",
    "\n",
    "    NOTE: OpenAI's structured output requires us to wrap the list in an object.\n",
    "    \"\"\"\n",
    "\n",
    "    memories: List[Memory]\n",
    "\n",
    "\n",
    "class StoredMemory(Memory):\n",
    "    \"\"\"\n",
    "    A stored long-term memory\n",
    "\n",
    "    - id: unique Redis key (string) used as the primary key in Redis.\n",
    "    - memory_id: a ULID for lexicographically sortable unique identifiers.\n",
    "    - created_at: timestamp for when it was stored.\n",
    "    - user_id: optional, to scope memories to a particular user.\n",
    "    - thread_id: optional, to group memories by conversation threads.\n",
    "    \"\"\"\n",
    "\n",
    "    id: str  # The redis key\n",
    "    memory_id: ulid.ULID = Field(default_factory=lambda: ulid.ULID())  # Unique identifier using ULID for sortable, distributed generation\n",
    "    created_at: datetime = Field(default_factory=datetime.now)  # Timestamp tracking for memory lifecycle management\n",
    "    user_id: Optional[str] = None  # Enable user-specific memory isolation\n",
    "    thread_id: Optional[str] = None  # Track conversation context\n",
    "    memory_type: Optional[MemoryType] = None  # Allow override of memory type during storage processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuBb-nXIeikn"
   },
   "source": [
    "- `Memory` captures the basic content and metadata. `metadata` should ideally be a small JSON blob (stringified) describing provenance, confidence, or tags used for filtering.\n",
    "- `Memories` is simply a wrapper for structured outputs - many LLMs / structured-output parsers require top-level objects, not raw lists.\n",
    "- `StoredMemory` extends Memory rather than composing it, which simplifies serialization and API consistency while adding storage-specific metadata. It is a specialized model for memories that have been persisted to Redis.\n",
    "  - ULID vs UUID: ULIDs provide lexicographical sorting while maintaining uniqueness, which is valuable for memory chronological analysis and debugging.\n",
    "  - `user_id` and `thread_id`: These fields enable both single-user agents and multi-tenant systems where memory isolation is crucial for privacy and personalization.\n",
    "\n",
    "\n",
    "### Memory storage\n",
    "Now that we have our data models defined, we need to create the infrastructure for storing and retrieving memories based on semantic similarity. This involves setting up a vector search index in Redis that can handle high-dimensional embeddings while maintaining fast query performance.\n",
    "\n",
    "The key challenge in long-term memory systems is not just storing information, but making it discoverable when it's relevant. Unlike traditional databases that rely on exact matches, our memory system needs to understand semantic relationships - recognizing that a query about \"flight preferences\" should retrieve memories about \"airline choices\" even if the exact words don't match.\n",
    "\n",
    "#### Create the Redis vector search index (schema)\n",
    "The foundation of our semantic search capability lies in properly structuring our Redis index. We need to define a schema that balances searchability, performance, and flexibility for different memory types and user contexts.\n",
    "\n",
    "The index schema defines how data is structured. Long-term memories must be stored with both textual fields (content, metadata) and a dense vector embedding for semantic search. RedisVL lets us define an index schema that includes regular fields and a vector field (embedding). The schema should include:\n",
    "- An index name and key prefix so we can find documents belonging to this index.\n",
    "- Text fields for content & metadata (for exact/tag filtering and fallback).\n",
    "- Tag fields for `memory_type`, `user_id`, `memory_id`, and optionally `thread_id` to enable efficient tag filtering.\n",
    "- A vector field with `dims` matching our embedding model (OpenAI `text-embedding-ada-002` → 1536 dims).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xavb-CfTjdTV"
   },
   "outputs": [],
   "source": [
    "# Define a RedisVL index schema for storing long-term memories and vector search index.\n",
    "# This creates the structure for storing and querying memories with semantic similarity search\n",
    "memory_schema = IndexSchema.from_dict({\n",
    "        \"index\": {\n",
    "            \"name\": \"agent_memories\",  # Index name for identification\n",
    "            \"prefix\": \"memory\",  # Redis key prefix (memory:1, memory:2, etc.)\n",
    "            \"key_separator\": \":\",  # Separator used between prefix and id (e.g., \"memory:01F...\")\n",
    "            \"storage_type\": \"json\",  # We will store documents as JSON objects in Redis so fields are addressable\n",
    "        },\n",
    "        \"fields\": [\n",
    "            # Text fields for full-text search and display\n",
    "            {\"name\": \"content\", \"type\": \"text\"},  # The actual memory content\n",
    "            {\"name\": \"metadata\", \"type\": \"text\"},  # Additional context information\n",
    "            {\"name\": \"created_at\", \"type\": \"text\"},  # Timestamp for temporal filtering\n",
    "\n",
    "            # Tag fields for exact-match filtering (indexed for fast lookups)\n",
    "            {\"name\": \"memory_type\", \"type\": \"tag\"},  # EPISODIC or SEMANTIC classification\n",
    "            {\"name\": \"user_id\", \"type\": \"tag\"},  # User isolation and personalization\n",
    "            {\"name\": \"memory_id\", \"type\": \"tag\"},  # Unique memory identifier\n",
    "            # Vector field for semantic similarity search\n",
    "            {\n",
    "                \"name\": \"embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"algorithm\": \"flat\",  # FLAT algorithm for exact similarity\n",
    "                    \"dims\": 1536,  # OpenAI embedding dimension\n",
    "                    \"distance_metric\": \"cosine\",  # Cosine similarity for semantic matching\n",
    "                    \"datatype\": \"float32\",  # Balance between precision and performance\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw9h9RIbDGAf"
   },
   "source": [
    "The schema design reflects several important architectural decisions. We assemble a JSON-like schema for RedisVL that instructs Redis how to index, store, and query memory documents.\n",
    "- `storage_type: \"json\"` means each memory will be stored as a JSON object (convenient for downstream reads).\n",
    "- Tag fields like `user_id` and `memory_type` enable fast exact-match filtering, which is crucial for multi-tenant systems where memory isolation is essential.\n",
    "- The `embedding` field is the important vector index: later we will store a list/array of floats under this key and run vector-based searches against it. `dims` needs to match the embedding model we use; otherwise searches will fail.\n",
    "- The vector field configuration uses cosine similarity, which works well for text embeddings as it measures angular similarity rather than Euclidean distance, making it more suitable for high-dimensional semantic spaces.\n",
    "\n",
    "#### Create and load the SearchIndex in Redis\n",
    "After defining the schema, we instantiate a `SearchIndex` backed by our `redis_client` and create the index. For repeated notebook runs it’s convenient to `overwrite=True` - that lets us iterate during development. In production, we would guard against accidental overwrites. Let's construct the index object and ensure the index exists in the Redis instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwvJDfLxjkXm",
    "outputId": "9fc56fd4-d840-41c3-827d-91a95cfd6efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term memory index ready\n"
     ]
    }
   ],
   "source": [
    "# Create the long-term memory index in Redis. If the index already exists, we overwrite the specification (useful during dev).\n",
    "try:\n",
    "    # Initialize the search index with validation\n",
    "    long_term_memory_index = SearchIndex(\n",
    "        schema=memory_schema,  # Use our defined schema\n",
    "        redis_client=redis_client,  # Connect to our Redis instance\n",
    "        validate_on_load=True  # validate index specs when loading (helps catch config drift)\n",
    "    )\n",
    "    # Create the index, overwriting any existing version\n",
    "    long_term_memory_index.create(overwrite=True)\n",
    "    print(\"Long-term memory index ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhqH8IBRGBJD"
   },
   "source": [
    "This creates a Python `SearchIndex` wrapper that knows how to talk to our Redis instance and perform operations (load docs, query vectors, etc.). The validation step ensures that our schema is compatible with the Redis version and available modules. In production systems, we would typically want to implement more sophisticated error handling and potentially schema migration logic.\n",
    "\n",
    "##### Inspect the index\n",
    "We can verify our index creation and inspect its structure using the RedisVL command-line interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEcEr7HKjnIf",
    "outputId": "062de94d-abbf-4979-c500-d84b85401274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Index Information:\n",
      "╭────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬\b╮\n",
      "│ Index Name     │ Storage Type   │ Prefixes       │ Index Options  │ Indexing       │\n",
      "├────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼\b┤\n",
      "| agent_memories | JSON           | ['memory']     | []             | 0              |\n",
      "╰────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴\b╯\n",
      "Index Fields:\n",
      "╭─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬\b╮\n",
      "│ Name            │ Attribute       │ Type            │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │\n",
      "├─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼\b┤\n",
      "│ $.content       │ content         │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.metadata      │ metadata        │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.created_at    │ created_at      │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.memory_type   │ memory_type     │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.user_id       │ user_id         │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.memory_id     │ memory_id       │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │\n",
      "│ $.embedding     │ embedding       │ VECTOR          │ algorithm       │ FLAT            │ data_type       │ FLOAT32         │ dim             │ 1536            │ distance_metric │ COSINE          │\n",
      "╰─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴\b╯\n"
     ]
    }
   ],
   "source": [
    "!rvl index info -i agent_memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6z_CjUSGwP1"
   },
   "source": [
    "This command provides detailed information about the index configuration, field mappings, and current document count for debugging and monitoring. `rvl index info` prints index metadata, fields, vector algorithm, and other config. Use this to confirm that embedding dimension, distance metric, and field definitions match expectations.\n",
    "\n",
    "### Memory access layer: Storage and retrieval functions\n",
    "With our vector index established, we can now implement the functions that handle memory lifecycle operations. These functions form the foundation of our agent's memory capabilities and will eventually be exposed as tools that the LLM can use autonomously.\n",
    "\n",
    "##### Initialize the OpenAI-based vectorizer\n",
    "We need a vectorizer to convert text to embeddings. RedisVL provides a convenience wrapper that uses OpenAI embeddings under the hood. Pick a model and keep embeddings consistent with the index `dims`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XgAd-DQjjpxI"
   },
   "outputs": [],
   "source": [
    "# Create an OpenAI-based vectorizer for turning text to vector embeddings.\n",
    "openai_embed = OpenAITextVectorizer(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IvRnz3zj8q1"
   },
   "source": [
    "`OpenAITextVectorizer` wraps calls to the OpenAI embeddings API and returns vectors in the format expected by RedisVL. Keep in mind embedding API usage costs and rate limits; for production we may want batching and retry logic.\n",
    "\n",
    "We will also set up logging to track memory operations, which is essential for debugging and monitoring memory system behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "123DyIDVjtp1"
   },
   "outputs": [],
   "source": [
    "# Set up a logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNxT-K6akqYc"
   },
   "source": [
    "The module-level logger is used in the memory functions to emit info/debug/error messages. In production we would attach file handlers, structured logging, and correlation IDs.\n",
    "\n",
    "#### Check for similar memories function\n",
    "One of the biggest challenges in persistent memory systems is preventing the accumulation of redundant or near-duplicate information. Without deduplication, agents quickly become overwhelmed with repetitive memories that dilute the signal-to-noise ratio in retrievals.\n",
    "\n",
    "Before writing a memory to the index, we usually want to avoid duplicate or near-duplicate memories. Deduplication is performed by embedding the candidate content and running a 1-NN vector search constrained by `user_id` and `memory_type`. If the nearest neighbor is within a small distance threshold, skip storage. This goes beyond simple string matching to understand conceptual similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XCqBxHX8jw0e"
   },
   "outputs": [],
   "source": [
    "# Default user identifier for system-wide memories not associated with specific users.\n",
    "# If we have any memories that are not associated with a user, we will use this ID.\n",
    "SYSTEM_USER_ID = \"system\"\n",
    "\n",
    "def similar_memory_exists(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if a similar long-term memory already exists in Redis.\n",
    "\n",
    "    Args:\n",
    "        content: The memory content to check for similarity\n",
    "        memory_type: Classification of the memory (EPISODIC or SEMANTIC)\n",
    "        user_id: User context for memory isolation\n",
    "        thread_id: Optional conversation thread for additional filtering\n",
    "        distance_threshold: Cosine similarity threshold (lower = more similar)\n",
    "\n",
    "    Returns:\n",
    "        True if a similar memory exists, False otherwise\n",
    "    \"\"\"\n",
    "    # Generate embedding for the candidate memory content\n",
    "    content_embedding = openai_embed.embed(content)\n",
    "\n",
    "    # Build filter constraints to search within appropriate context. Tags compare to string values.\n",
    "    filters = (Tag(\"user_id\") == user_id) & (Tag(\"memory_type\") == memory_type)\n",
    "\n",
    "    # Optionally narrow to a thread\n",
    "    if thread_id:\n",
    "        filters = filters & (Tag(\"thread_id\") == thread_id)\n",
    "\n",
    "    # Perform vector similarity search with distance threshold\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=content_embedding,\n",
    "        num_results=1,  # We only need to know if ANY similar memory exists\n",
    "        vector_field_name=\"embedding\",\n",
    "        filter_expression=filters,\n",
    "        distance_threshold=distance_threshold,  # Similarity cutoff point\n",
    "        return_fields=[\"id\"],  # Minimal return fields for efficiency\n",
    "    )\n",
    "    # Execute the similarity search\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "    logger.debug(f\"Similar memory search results: {results}\")\n",
    "\n",
    "    # If any results found, a similar memory exists and consider it a duplicate\n",
    "    if results:\n",
    "        logger.debug(\n",
    "            f\"{len(results)} similar {'memory' if results.count == 1 else 'memories'} found. First: \"\n",
    "            f\"{results[0]['id']}. Skipping storage.\"\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRdOHZJDny1s"
   },
   "source": [
    "The similarity detection uses cosine distance in the embedding space to measure semantic similarity. A threshold of 0.1 means that memories with cosine similarity > 0.9 are considered duplicates. This threshold balances between preventing true duplicates and allowing for natural variation in how similar concepts might be expressed.\n",
    "\n",
    "The filtering approach ensures we only compare memories within the same context (same user, same memory type), which prevents cross-contamination between different users or between personal preferences and general knowledge.\n",
    "\n",
    "#### Store long-term memories function\n",
    "Storing a memory means embedding the content, building a JSON doc with metadata, and indexing it into Redis. This function wraps deduplication, embedding generation, and insertion. For robust production usage we should add retries, batching, and metrics. For the notebook, we will keep the function synchronous and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "okoK4EpSj6QJ"
   },
   "outputs": [],
   "source": [
    "# Store a single long-term memory into Redis with deduplication.\n",
    "def store_memory(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    metadata: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Store a long-term memory in Redis with deduplication.\n",
    "\n",
    "    This function:\n",
    "    1. Checks for similar existing memories to avoid duplicates\n",
    "    2. Generates vector embeddings for semantic search\n",
    "    3. Stores the memory with metadata for retrieval\n",
    "\n",
    "    Args:\n",
    "        content: The actual memory content in natural language\n",
    "        memory_type: Classification (EPISODIC for personal, SEMANTIC for general)\n",
    "        user_id: User identifier for memory isolation\n",
    "        thread_id: Optional conversation thread association\n",
    "        metadata: Additional context information as JSON string\n",
    "    \"\"\"\n",
    "    # Set default metadata if none provided\n",
    "    if metadata is None:\n",
    "        metadata = \"{}\"\n",
    "\n",
    "    logger.info(f\"Preparing to store memory: {content}\")\n",
    "\n",
    "    # Perform deduplication check before expensive operations\n",
    "    if similar_memory_exists(content, memory_type, user_id, thread_id):\n",
    "        logger.info(\"Similar memory found, skipping storage\")\n",
    "        return\n",
    "\n",
    "    # Create vector embedding for the content\n",
    "    embedding = openai_embed.embed(content)\n",
    "\n",
    "    # Build the memory document to persist as JSON. memory_id is ULID (lexicographically sortable).\n",
    "    memory_data = {\n",
    "        \"user_id\": user_id or SYSTEM_USER_ID,  # Ensure user context is always set\n",
    "        \"content\": content,  # The actual memory content\n",
    "        \"memory_type\": memory_type.value,  # Enum converted to string\n",
    "        \"metadata\": metadata,  # Additional context information\n",
    "        \"created_at\": datetime.now().isoformat(),  # ISO timestamp for sorting\n",
    "        \"embedding\": embedding,  # Vector for similarity search\n",
    "        \"memory_id\": str(ulid.ULID()),  # Unique, sortable identifier\n",
    "        \"thread_id\": thread_id,  # Optional conversation context\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Store in Redis vector index\n",
    "        long_term_memory_index.load([memory_data])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing memory: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Stored {memory_type} memory: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoneglZBqvSG"
   },
   "source": [
    "- The embedding generation is performed after deduplication checking to avoid unnecessary API calls to OpenAI when memories will be skipped anyway.\n",
    "- `long_term_memory_index.load()` writes the JSON document into Redis and ensures the vector gets indexed.\n",
    "- The use of ULID for memory IDs provides both uniqueness and chronological ordering, which is valuable for debugging and memory lifecycle management.\n",
    "\n",
    "Because embedding generation entails API calls, consider batching multiple contents together if you plan to store many memories at once.\n",
    "\n",
    "#### Retrieve relevant long-term memories function\n",
    "The retrieval function implements vector search with multiple filtering capabilities, enabling agents to find relevant memories based on semantic similarity while respecting user context and memory type boundaries.\n",
    "\n",
    "The retrieval here is symmetric to storage: transform the query text into an embedding, run a vector similarity search, and return structured `StoredMemory` objects. We allow optional filtering by `memory_type` (single or list), `user_id`, and `thread_id`. We also accept a `distance_threshold` to limit results to sufficiently similar items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MZYZzQ6ZkC1n"
   },
   "outputs": [],
   "source": [
    "# Retrieve top-k memories relevant to a query using vector retrieval and tag filters.\n",
    "def retrieve_memories(\n",
    "    query: str,\n",
    "    memory_type: Union[Optional[MemoryType], List[MemoryType]] = None,\n",
    "    user_id: str = SYSTEM_USER_ID,\n",
    "    thread_id: Optional[str] = None,\n",
    "    distance_threshold: float = 0.1,\n",
    "    limit: int = 5,\n",
    ") -> List[StoredMemory]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant memories from Redis using vector similarity search.\n",
    "\n",
    "    Args:\n",
    "        query: Natural language query describing what memories to find\n",
    "        memory_type: Filter by specific memory type(s) or None for all types\n",
    "        user_id: User context for memory isolation\n",
    "        thread_id: Optional thread filtering for conversation-specific memories\n",
    "        distance_threshold: Semantic similarity threshold (lower = more similar)\n",
    "        limit: Maximum number of memories to return\n",
    "\n",
    "    Returns:\n",
    "        List of StoredMemory objects ranked by relevance\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Retrieving memories for query: {query}\")\n",
    "\n",
    "    # Generate query embedding for semantic comparison\n",
    "    query_embedding = openai_embed.embed(query)\n",
    "\n",
    "    # Configure vector similarity search\n",
    "    vector_query = VectorRangeQuery(\n",
    "        vector=query_embedding,\n",
    "        return_fields=[\n",
    "            \"content\",\n",
    "            \"memory_type\",\n",
    "            \"metadata\",\n",
    "            \"created_at\",\n",
    "            \"memory_id\",\n",
    "            \"thread_id\",\n",
    "            \"user_id\",\n",
    "        ],\n",
    "        num_results=limit,\n",
    "        vector_field_name=\"embedding\",\n",
    "        dialect=2,  # Use Redis query dialect v2 for advanced filter semantics\n",
    "        distance_threshold=distance_threshold,\n",
    "    )\n",
    "\n",
    "    # Build filter conditions\n",
    "    base_filters = [f\"@user_id:{{{user_id or SYSTEM_USER_ID}}}\"]\n",
    "\n",
    "    # Handle memory type filtering (single type or multiple types)\n",
    "    if memory_type:\n",
    "        if isinstance(memory_type, list):\n",
    "            # Multiple memory types: create OR condition\n",
    "            base_filters.append(f\"@memory_type:{{{'|'.join(memory_type)}}}\")\n",
    "        else:\n",
    "            # Single memory type: exact match\n",
    "            base_filters.append(f\"@memory_type:{{{memory_type.value}}}\")\n",
    "\n",
    "    # Add thread-specific filtering if requested\n",
    "    if thread_id:\n",
    "        base_filters.append(f\"@thread_id:{{{thread_id}}}\")\n",
    "\n",
    "    # Apply all filter conditions\n",
    "    vector_query.set_filter(\" \".join(base_filters))\n",
    "\n",
    "    # Execute vector similarity search with filters\n",
    "    results = long_term_memory_index.query(vector_query)\n",
    "\n",
    "    # Parse results into StoredMemory objects\n",
    "    memories = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            # Construct validated StoredMemory object\n",
    "            memory = StoredMemory(\n",
    "                id=doc[\"id\"],\n",
    "                memory_id=doc[\"memory_id\"],\n",
    "                user_id=doc[\"user_id\"],\n",
    "                thread_id=doc.get(\"thread_id\", None),  # Handle optional field\n",
    "                memory_type=MemoryType(doc[\"memory_type\"]),\n",
    "                content=doc[\"content\"],\n",
    "                created_at=doc[\"created_at\"],\n",
    "                metadata=doc[\"metadata\"],\n",
    "            )\n",
    "            memories.append(memory)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing memory: {e}\")\n",
    "            continue  # Skip malformed documents rather than failing entire query\n",
    "\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H7MBTmuuDoi"
   },
   "source": [
    "The retrieval function allows for queries like \"find all episodic memories from this user in this conversation thread that are semantically similar to travel preferences.\"\n",
    "- The use of Redis query dialect v2 enables advanced filtering capabilities that wouldn't be available with basic vector search. The combination of semantic similarity and structured filtering provides the flexibility needed for real-world agent applications.\n",
    "  - The `vector_query.set_filter` uses RedisVL filter syntax (`@field:{value|value2}`). Ensure to escape or sanitize tag values in production to avoid injection-like issues.\n",
    "  - This code assumes the `query` embeddings and stored embeddings use the same model & normalization; mixing models will degrade similarity.\n",
    "\n",
    "### Managing long-term memory with tools\n",
    "So far, we have implemented the ability to store, deduplicate, and retrieve memories directly using RedisVL and vector search. But when building an AI agent, we to hard-code when and what to remember. Instead, we want the LLM itself to decide when and how to store or retrieve memories, exposing memory operations as tools.\n",
    "\n",
    "This approach offers several architectural advantages while introducing some important trade-offs that we need to understand when designing production systems.\n",
    "- Tool-based memory management (our approach):The LLM chooses when to call a memory operation. This results in fewer Redis calls, but might occasionally miss relevant context. Since it introduces an extra reasoning step, it can add a bit of latency.\n",
    "- Manual memory management (alternative): The system designer controls when to store or retrieve memories. This is faster and extracts more context but can increase token usage and cost.\n",
    "\n",
    "For most production applications, the tool-based approach provides the optimal balance between intelligent memory management and system efficiency. The LLM's natural language understanding capabilities make it exceptionally well-suited to identifying what information deserves persistent storage.\n",
    "\n",
    "### Defining agent tools\n",
    "The next step is to implement memory management through LangChain tools that expose our memory functions to the LLM. This design pattern treats memory operations as natural capabilities that agents can use autonomously, similar to how they might call external APIs or perform calculations.\n",
    "\n",
    "#### Store memory tool\n",
    "The storage tool needs to handle the complexity of memory persistence while providing a simple, intuitive interface for the LLM. It must bridge between natural language instructions and our typed data models while handling edge cases gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "k_g-DvgIkQgb"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def store_memory_tool(\n",
    "    content: str,\n",
    "    memory_type: MemoryType,\n",
    "    metadata: Optional[Dict[str, str]] = None,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Store a long-term memory in the system.\n",
    "\n",
    "    Use this tool to save important information about user preferences,\n",
    "    experiences, or general knowledge that might be useful in future\n",
    "    interactions.\n",
    "    \"\"\"\n",
    "    # Extract context from configuration (provided by agent framework)\n",
    "    config = config or RunnableConfig()\n",
    "    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n",
    "    thread_id = config.get(\"thread_id\")\n",
    "\n",
    "    try:\n",
    "        # Store in long-term memory\n",
    "        store_memory(\n",
    "            content=content,\n",
    "            memory_type=memory_type,\n",
    "            user_id=user_id,\n",
    "            thread_id=thread_id,\n",
    "            metadata=str(metadata) if metadata else None,\n",
    "        )\n",
    "\n",
    "        return f\"Successfully stored {memory_type} memory: {content}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error storing memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ekBcTb70vHz"
   },
   "source": [
    "- This defines a LangChain-compatible `@tool`, making `store_memory_tool` available as a callable tool inside an agent pipeline.\n",
    "- The docstring provides clear guidance to the LLM about when and how to use the tool, which significantly improves the quality of autonomous memory decisions.\n",
    "- It uses the `store_memory()` function we implemented earlier, ensuring deduplication and vector embedding generation, and wraps the result in a user-friendly message for the LLM.\n",
    "\n",
    "Let’s try storing an example memory to test the store memory tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_w8ujTDRkX2y",
    "outputId": "cb16a9af-7347-47dd-c1c9-6a9c2001a74e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Successfully stored MemoryType.EPISODIC memory: I like flying on Delta when possible'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_memory_tool.invoke({\"content\": \"I like flying on Delta when possible\", \"memory_type\": \"episodic\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmZ2PNWC1WWM"
   },
   "source": [
    "This call instructs the tool to save an episodic memory about airline preference. If the memory is new, it will be embedded and stored in Redis. If it already exists (based on similarity search), the storage will be skipped.\n",
    "\n",
    "#### Retrieve memories tool\n",
    "Now that we can save memories, the next step is retrieving them. This tool performs semantic search against stored memories, filtered by user, memory type, or thread if provided.\n",
    "\n",
    "The retrieval tool is more complex than storage because it needs to translate natural language queries into effective memory searches while providing results in a format that's useful for ongoing conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ODfgDBK_kayo"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_memories_tool(\n",
    "    query: str,\n",
    "    memory_type: List[MemoryType],\n",
    "    limit: int = 5,\n",
    "    config: Optional[RunnableConfig] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Retrieve long-term memories relevant to the query.\n",
    "\n",
    "    Use this tool to access previously stored information about user\n",
    "    preferences, experiences, or general knowledge.\n",
    "    \"\"\"\n",
    "    # Extract user context for personalized retrieval\n",
    "    config = config or RunnableConfig()\n",
    "    user_id = config.get(\"user_id\", SYSTEM_USER_ID)\n",
    "\n",
    "    try:\n",
    "        # Get long-term memories with semantic search\n",
    "        stored_memories = retrieve_memories(\n",
    "            query=query,\n",
    "            memory_type=memory_type,\n",
    "            user_id=user_id,\n",
    "            limit=limit,\n",
    "            distance_threshold=0.3,\n",
    "        )\n",
    "\n",
    "        # Format the response\n",
    "        response = []\n",
    "\n",
    "        if stored_memories:\n",
    "            response.append(\"Long-term memories:\")\n",
    "            for memory in stored_memories:\n",
    "                # Format each memory with type indicator and content\n",
    "                response.append(f\"- [{memory.memory_type}] {memory.content}\")\n",
    "\n",
    "        return \"\\n\".join(response) if response else \"No relevant memories found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving memories: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn9c1vOB2t-w"
   },
   "source": [
    "- This tool calls our earlier `retrieve_memories()` function, performs a vector similarity search, and then formats the results as a readable bullet list.\n",
    "- By wrapping this in `@tool`, we allow the LLM to naturally “ask” for past information as needed during a conversation.\n",
    "\n",
    "Let's query the memory store to test the retrieve memories tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "g8gQBalvkj81",
    "outputId": "1f6a8017-5e97-48ad-a851-d89e446abcb5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Long-term memories:\\n- [MemoryType.EPISODIC] I like flying on Delta when possible'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_memories_tool.invoke({\"query\": \"Airline preferences\", \"memory_type\": [\"episodic\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ymq5g1n3Rxq"
   },
   "source": [
    "This should return the Delta Airlines preference we stored earlier (unless it was skipped as a duplicate).\n",
    "\n",
    "## Building the travel agent workflow\n",
    "Now, we will take the next step: assembling these components into a LangGraph workflow. This workflow is designed to simulate a conversational agent that balances short-term memory (conversation thread), long-term memory (episodic and semantic knowledge), and summarization (to manage context efficiently). Our goal is to create a ReAct-style travel assistant that not only chats naturally but also remembers, recalls, and condenses information over time.\n",
    "\n",
    "### Short-term memory infrastructure\n",
    "The foundation of our agent's conversational abilities lies in robust short-term memory management. This system needs to persist conversation state across sessions while providing fast access to recent context. Redis serves as an excellent backend for this purpose, offering both persistence and performance.\n",
    "\n",
    "We start by configuring the Redis checkpointer. This will act as the agent’s short-term memory manager, keeping track of the current conversation history.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UbUqQ7q1kmHO"
   },
   "outputs": [],
   "source": [
    "# Set up the Redis checkpointer for short term memory\n",
    "redis_saver = RedisSaver(redis_client=redis_client)  # This enables agents to resume conversations exactly where they left off\n",
    "redis_saver.setup()  # Setup creates necessary Redis data structures for checkpoint storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-2MNTH_6d6X"
   },
   "source": [
    "The RedisSaver creates a checkpointing system that stores conversation state at multiple granularities. Each message exchange creates a checkpoint, enabling rollback capabilities and conversation branching. The setup process initializes Redis data structures optimized for both sequential access (conversation flow) and random access (specific checkpoint retrieval).\n",
    "\n",
    "#### LLM and tool configuration\n",
    "Next, we define the set of tools that the agent can use. These are the same `store_memory_tool` and `retrieve_memories_tool` we built earlier. Then we configure the LLM. We will use OpenAI’s gpt-4o model with a higher temperature to make responses slightly more creative. Importantly, we bind the tools so the model can call them when reasoning about user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tUNPxLrGkqzD"
   },
   "outputs": [],
   "source": [
    "# Define the set of tools\n",
    "tools = [store_memory_tool, retrieve_memories_tool]\n",
    "\n",
    "# Configure an LLM for the agent with a more creative temperature.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", temperature=0.7).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adfYZwt17eH3"
   },
   "source": [
    "Binding tools here means the model can decide - during its reasoning process - whether to call one of the memory tools instead of (or in addition to) responding directly.\n",
    "\n",
    "### ReAct agent assembly\n",
    "The ReAct (Reasoning and Acting) framework enables agents to iteratively reason about problems and take actions based on that reasoning. Our implementation combines this with memory management to create intelligent conversational agents. This agent is where everything comes together:\n",
    "- Short-term memory via the RedisSaver.\n",
    "- Long-term memory via the custom tools.\n",
    "- System prompt to set the assistant’s persona and memory strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-oaCWoY9kvIy"
   },
   "outputs": [],
   "source": [
    "# Define the travel agent\n",
    "travel_agent = create_react_agent(\n",
    "    model=llm,  # LLM with tool-calling capabilities\n",
    "    tools=tools,  # Long-term memory: provided as a set of custom tools\n",
    "    checkpointer=redis_saver,  # Short-term memory: the conversation history\n",
    "    prompt=SystemMessage(\n",
    "        content=\"\"\"\n",
    "        You are a travel assistant helping users plan their trips. You remember user preferences\n",
    "        and provide personalized recommendations based on past interactions.\n",
    "\n",
    "        You have access to the following types of memory:\n",
    "        1. Short-term memory: The current conversation thread\n",
    "        2. Long-term memory:\n",
    "           - Episodic: User preferences and past trip experiences (e.g., \"User prefers window seats\")\n",
    "           - Semantic: General knowledge about travel destinations and requirements\n",
    "\n",
    "        Your procedural knowledge (how to search, book flights, etc.) is built into your tools and prompts.\n",
    "\n",
    "        Always be helpful, personal, and context-aware in your responses.\n",
    "        \"\"\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsQvKJEg8TdQ"
   },
   "source": [
    "The system prompt provides clear guidance about memory usage patterns, encouraging proactive memory management rather than reactive responses. The prompt emphasizes personalization and learning, which encourages the agent to build rich user profiles over time. At this point we have a fully functional travel agent - but it still needs a workflow to structure its behavior. That’s what we will build next.\n",
    "\n",
    "### Node 1: Respond to the user\n",
    "The first node, `respond_to_user`, handles the actual interaction with the travel agent:\n",
    "- It takes the current conversation state\n",
    "- Extracts the human messages\n",
    "- Calls the travel agent to generate a response\n",
    "- Appends the agent’s reply to the conversation history\n",
    "\n",
    "To support this, we define a `RuntimeState` class that inherits from `MessagesState`. This allows us to track all conversation messages across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iYoNvJzyk5z8"
   },
   "outputs": [],
   "source": [
    "class RuntimeState(MessagesState):\n",
    "    \"\"\"Runtime state for the travel agent.\"\"\"\n",
    "    # Inherits 'messages' field from MessagesState\n",
    "    pass\n",
    "\n",
    "\n",
    "def respond_to_user(state: RuntimeState, config: RunnableConfig) -> RuntimeState:\n",
    "    \"\"\"Invoke the travel agent to generate a response.\"\"\"\n",
    "    # Extract the human messages from state\n",
    "    human_messages = [m for m in state[\"messages\"] if isinstance(m, HumanMessage)]\n",
    "    if not human_messages:\n",
    "        logger.warning(\"No HumanMessage found in state\")\n",
    "        return state\n",
    "\n",
    "    try:\n",
    "        # Single agent invocation, not streamed (simplified for reliability) - call the travel agent with full conversation context\n",
    "        result = travel_agent.invoke({\"messages\": state[\"messages\"]}, config=config)  # This includes automatic memory retrieval, reasoning, and response generation\n",
    "        # Extract the agent's response from the complete result\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        # Append the agent’s reply to state\n",
    "        state[\"messages\"].append(agent_message)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error invoking travel agent: {e}\")\n",
    "        # Provide user-friendly error response while maintaining conversation flow\n",
    "        agent_message = AIMessage(\n",
    "            content=\"I'm sorry, I encountered an error processing your request.\"\n",
    "        )\n",
    "        state[\"messages\"].append(agent_message)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJUME-QW-5-b"
   },
   "source": [
    "- The node always ensures the conversation continues smoothly, even if the model or tools fail.\n",
    "- The conversation history (`state[\"messages\"]`) is continuously updated, preserving context across turns.\n",
    "\n",
    "### Node 2: Execute Tools\n",
    "The second node bridges the LLM’s reasoning with actual tool execution.\n",
    "When the model decides it needs to call a tool (e.g., store a memory), it encodes this request in its last `AIMessage`. This node then:\n",
    "- Parses the tool calls\n",
    "- Executes the correct tool with given arguments\n",
    "- Appends the tool results back into the conversation\n",
    "\n",
    "This ensures that tool calls are not just “pretend actions” but actually performed and reflected in the agent’s state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nl-1bxSblAUq"
   },
   "outputs": [],
   "source": [
    "def execute_tools(state: RuntimeState, config: RunnableConfig) -> RuntimeState:\n",
    "    \"\"\"Execute tools specified in the latest AIMessage and append ToolMessages.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # Look for the latest AIMessage with tool calls\n",
    "    latest_ai_message = next(\n",
    "        (m for m in reversed(messages) if isinstance(m, AIMessage) and m.tool_calls),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    # If no tool calls are present, no action needed\n",
    "    if not latest_ai_message:\n",
    "        return state  # No tool calls to process\n",
    "\n",
    "    tool_messages = []\n",
    "    # Process each tool call individually with comprehensive error handling\n",
    "    for tool_call in latest_ai_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "\n",
    "        # Find the corresponding tool from our set\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        if not tool:\n",
    "            continue  # Skip if tool not found\n",
    "\n",
    "        try:\n",
    "            # Execute the tool with the provided arguments\n",
    "            result = tool.invoke(tool_args, config=config)\n",
    "            # Create a ToolMessage with the result\n",
    "            tool_message = ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_id,\n",
    "                name=tool_name\n",
    "            )\n",
    "            tool_messages.append(tool_message)\n",
    "        except Exception as e:\n",
    "            # Handle tool execution errors\n",
    "            error_message = ToolMessage(\n",
    "                content=f\"Error executing tool '{tool_name}': {str(e)}\",\n",
    "                tool_call_id=tool_id,\n",
    "                name=tool_name\n",
    "            )\n",
    "            tool_messages.append(error_message)\n",
    "\n",
    "    # Append the ToolMessages to the message history\n",
    "    messages.extend(tool_messages)\n",
    "    state[\"messages\"] = messages\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "695uEaCHAb_s"
   },
   "source": [
    "Here, we are creating a loop between agent reasoning → tool execution → updated state. This design ensures the LLM can plan actions and actually see their effects in the conversation history.\n",
    "\n",
    "### Node 3: Conversation summarization\n",
    "Over time, the conversation history can grow too large to fit into the LLM’s context window. To handle this, we introduce a summarization node.\n",
    "\n",
    "This node monitors the number of messages. After a threshold (here, every 6 messages), it generates a summary of the entire conversation so far and replaces older messages with this summary. The summary is carefully crafted to preserve user preferences, important decisions, open questions while discarding unnecessary details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6v0pbFHelKrq"
   },
   "outputs": [],
   "source": [
    "# An LLM configured for summarization.\n",
    "summarizer = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", temperature=0.3)\n",
    "\n",
    "# The number of messages after which we will summarize the conversation.\n",
    "MESSAGE_SUMMARIZATION_THRESHOLD = 6\n",
    "\n",
    "\n",
    "def summarize_conversation(\n",
    "    state: RuntimeState, config: RunnableConfig\n",
    ") -> RuntimeState:\n",
    "    \"\"\"\n",
    "    Summarize a list of messages into a concise summary to reduce context length\n",
    "    while preserving important information.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    current_message_count = len(messages)\n",
    "\n",
    "    # Check if summarization is needed based on message threshold\n",
    "    if current_message_count < MESSAGE_SUMMARIZATION_THRESHOLD:\n",
    "        logger.debug(f\"Not summarizing conversation: {current_message_count}\")\n",
    "        return state\n",
    "\n",
    "    # Specialized system prompt for conversation summarization\n",
    "    system_prompt = \"\"\"\n",
    "    You are a conversation summarizer. Create a concise summary of the previous\n",
    "    conversation between a user and a travel assistant.\n",
    "\n",
    "    The summary should:\n",
    "    1. Highlight key topics, preferences, and decisions\n",
    "    2. Include any specific trip details (destinations, dates, preferences)\n",
    "    3. Note any outstanding questions or topics that need follow-up\n",
    "    4. Be concise but informative\n",
    "\n",
    "    Format your summary as a brief narrative paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct formatted conversation history for summarization\n",
    "    message_content = \"\\n\".join(\n",
    "        [\n",
    "            f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content}\"\n",
    "            for msg in messages\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Invoke the summarizer and generate the summary\n",
    "    summary_messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(\n",
    "            content=f\"Please summarize this conversation:\\n\\n{message_content}\"\n",
    "        ),\n",
    "    ]\n",
    "    summary_response = summarizer.invoke(summary_messages)\n",
    "\n",
    "    logger.info(f\"Summarized {len(messages)} messages into a conversation summary\")\n",
    "\n",
    "    # Create structured summary message for conversation integration\n",
    "    summary_message = SystemMessage(\n",
    "        content=f\"\"\"\n",
    "        Summary of the conversation so far:\n",
    "\n",
    "        {summary_response.content}\n",
    "\n",
    "        Please continue the conversation based on this summary and the recent messages.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Remove older messages while preserving the most recent exchange\n",
    "    remove_messages = [\n",
    "        RemoveMessage(id=msg.id) for msg in messages if msg.id is not None\n",
    "    ]\n",
    "\n",
    "    # Replace full history with summary + most recent message\n",
    "    state[\"messages\"] = [  # type: ignore\n",
    "        *remove_messages,  # Remove old messages\n",
    "        summary_message,  # Insert conversation summary\n",
    "        state[\"messages\"][-1],  # Preserve most recent message for context\n",
    "    ]\n",
    "\n",
    "    return state.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc3dsL7aBUhw"
   },
   "source": [
    "Technically speaking, this ensures the conversation never exceeds the model’s context window, while keeping all critical details alive in compressed form. This design is crucial for long-running conversations with the travel assistant.\n",
    "\n",
    "With these core workflow components implemented, we now have a complete foundation for memory-enabled agent systems that can handle complex, long-term user relationships while maintaining performance and reliability. The next phase will integrate these components into a complete working system.\n",
    "\n",
    "### Build the agent graph\n",
    "We need to glue the three nodes into a directed graph so they run in the right order:\n",
    "- The `agent` node (calls the LLM) is the entry point.\n",
    "- After the LLM runs, the graph must decide whether the LLM requested tool calls; if so, we should run `execute_tools`.\n",
    "- If there are no tool calls, we continue to `summarize_conversation` (which may compress history).\n",
    "- After `execute_tools` finishes, we return to the `agent` node so the LLM can react to the tool outputs.\n",
    "- After summarization we end the current workflow run (we keep a summary in the state so future runs still have context).\n",
    "\n",
    "The routing function `decide_next_step` inspects the most recent AI message to choose the next node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fy6DpFtulVNU"
   },
   "outputs": [],
   "source": [
    "def decide_next_step(state):\n",
    "    # Find the most recent AIMessage in the conversation (if any)\n",
    "    latest_ai_message = next((m for m in reversed(state[\"messages\"]) if isinstance(m, AIMessage)), None)\n",
    "    # If the AI requested tools, continue to execute them\n",
    "    if latest_ai_message and latest_ai_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    # Otherwise, move to summarization\n",
    "    return \"summarize_conversation\"\n",
    "\n",
    "\n",
    "# Create the workflow\n",
    "workflow = StateGraph(RuntimeState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"agent\", respond_to_user)\n",
    "workflow.add_node(\"execute_tools\", execute_tools)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "\n",
    "# Declare the entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "# Conditional transition from the agent node\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    decide_next_step,\n",
    "    {\"execute_tools\": \"execute_tools\", \"summarize_conversation\": \"summarize_conversation\"}, # - If the LLM produced a tool call, go to execute_tools, else go to summarization\n",
    ")\n",
    "# After executing tools, go back to the agent so it can respond to tool results\n",
    "workflow.add_edge(\"execute_tools\", \"agent\")\n",
    "# After summarization we end the workflow run\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile the graph and attach the Redis checkpointer for state persistence\n",
    "graph = workflow.compile(checkpointer=redis_saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkJaXPWDIsci"
   },
   "source": [
    "This builds a `StateGraph` describing the workflow and control flow.\n",
    "- `decide_next_step` inspects the last AI message for `tool_calls` (a pattern provided by your LLM/tool integration) to determine whether tool execution is required.\n",
    "- The graph is compiled with the `redis_saver` checkpointer so any state transitions will be persisted into Redis; this allows conversation threads to survive process restarts and multiple runs.\n",
    "\n",
    "#### Visualizing the workflow\n",
    "Let’s display the graph visually to better understand the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "4vNZszRNItcA",
    "outputId": "cbb21408-c7cf-4d30-c521-5040be571802"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmMgvSbLEu6P"
   },
   "source": [
    "### Test of the agent with interactive main loop\n",
    "The `main()` loop provides a simple console-based REPL that uses the compiled graph to process user messages in an ongoing thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Lf0yoHK6lchX"
   },
   "outputs": [],
   "source": [
    "def main(thread_id: str = \"book_flight\", user_id: str = \"demo_user\"):\n",
    "    \"\"\"Main interaction loop for the travel agent\"\"\"\n",
    "\n",
    "    print(\"Welcome to the Travel Assistant! (Type 'exit' to quit)\")\n",
    "\n",
    "    # RunnableConfig is used by LangGraph to pass runtime values into tools and checkpointer\n",
    "    config = RunnableConfig(configurable={\"thread_id\": thread_id, \"user_id\": user_id})\n",
    "    # Start with an empty runtime state; the checkpointer/Redis may repopulate from prior runs.\n",
    "    state = RuntimeState(messages=[])\n",
    "\n",
    "    while True:\n",
    "        # Read user input\n",
    "        user_input = input(\"\\nYou (type 'quit' to quit): \")\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Thank you for using the Travel Assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Append the human message to the runtime state\n",
    "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "        try:\n",
    "            # Process user input through the graph. The graph updates and persists state via the checkpointer.\n",
    "            for result in graph.stream(state, config=config, stream_mode=\"values\"):\n",
    "                state = RuntimeState(**result)\n",
    "\n",
    "            logger.debug(f\"# of messages after run: {len(state['messages'])}\")\n",
    "\n",
    "            # Find the most recent AI message, so we can print the response\n",
    "            ai_messages = [m for m in state[\"messages\"] if isinstance(m, AIMessage)]\n",
    "            if ai_messages:\n",
    "                message = ai_messages[-1].content\n",
    "            else:\n",
    "                # Fallback if no AIMessage produced\n",
    "                logger.error(\"No AI messages after run\")\n",
    "                message = \"I'm sorry, I couldn't process your request properly.\"\n",
    "                # Add the error message to the state\n",
    "                state[\"messages\"].append(AIMessage(content=message))\n",
    "\n",
    "            print(f\"\\nAssistant: {message}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Keep the loop alive and give user-friendly feedback on errors\n",
    "            logger.exception(f\"Error processing request: {e}\")\n",
    "            error_message = \"I'm sorry, I encountered an error processing your request.\"\n",
    "            print(f\"\\nAssistant: {error_message}\")\n",
    "            # Add the error message to the state\n",
    "            state[\"messages\"].append(AIMessage(content=error_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMJiZHiNGOcD"
   },
   "source": [
    "This interactive loop appends the user’s message to runtime state and executes the compiled graph for that turn. `graph.stream()` handles node invocation, checkpointer persistence, and yields serialized state snapshots.\n",
    "\n",
    "Now, Let's launch the interactive chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JC6xIWOtllL-",
    "outputId": "500582e0-24fd-437d-c830-f1e2acae85fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a user ID: 123\n",
      "Enter a thread ID: 123\n",
      "Welcome to the Travel Assistant! (Type 'exit' to quit)\n",
      "\n",
      "You (type 'quit' to quit): Hi I plan to go to singapore with my wife this summer. We love outdoors activities and trying new kinds of foods. Any good recommendations?\n",
      "\n",
      "Assistant: That sounds like an exciting trip! Singapore offers a fantastic mix of outdoor activities and diverse food experiences. Here are some recommendations for you and your wife:\n",
      "\n",
      "### Outdoor Activities:\n",
      "1. **Hiking in MacRitchie Reservoir**: Enjoy scenic trails with beautiful views. The treetop walk offers a unique perspective of the forest canopy.\n",
      "2. **Gardens by the Bay**: Explore the stunning gardens, including the iconic Supertree Grove and the Cloud Forest Dome. The outdoor areas are perfect for strolling and enjoying the scenery.\n",
      "3. **Sentosa Island**: This resort island has beautiful beaches, adventure parks, and nature trails. You can relax on the beach or try activities like zip-lining and hiking.\n",
      "4. **Cycling at East Coast Park**: Rent bicycles and ride along the coast. The park has plenty of food stalls where you can stop for local snacks.\n",
      "\n",
      "### Food Experiences:\n",
      "1. **Hawker Centers**: Don't miss trying local dishes at hawker centers like Lau Pa Sat or Maxwell Food Centre. Look for Hainanese chicken rice, laksa, and satay.\n",
      "2. **Chinatown Food Street**: A great place to sample various types of local food in a vibrant atmosphere.\n",
      "3. **Little India**: Experience the flavors of Indian cuisine, from biryani to roti prata. The Tekka Centre offers a mix of Indian and Malay foods.\n",
      "4. **Clarke Quay**: After a day of activities, enjoy dinner along the riverside with restaurants offering both local and international dishes.\n",
      "\n",
      "Feel free to ask if you have any specific interests or if you'd like more details about any of these recommendations!\n",
      "\n",
      "You (type 'quit' to quit): Excellent thank you. I would love help booking flights. What are the best routes typically flown from Atlanta to Singapore?\n",
      "\n",
      "Assistant: To book flights from Atlanta to Singapore, here are some common routes typically flown:\n",
      "\n",
      "1. **Delta Air Lines**: As you mentioned a preference for Delta, they often provide direct flights from Hartsfield-Jackson Atlanta International Airport (ATL) to Singapore Changi Airport (SIN). This is usually the most convenient option.\n",
      "\n",
      "2. **Connecting Flights**: If a direct flight is not available or if you are looking for more options, you can consider flights with connections:\n",
      "   - **Via Los Angeles (LAX)**: Some flights connect through LAX before heading to Singapore.\n",
      "   - **Via Tokyo (NRT/HND)**: Flights can also connect in Tokyo, giving you the chance to explore another city if you have a long layover.\n",
      "   - **Via Hong Kong (HKG)**: Another common route involves connecting through Hong Kong.\n",
      "\n",
      "3. **Other Airlines**: In addition to Delta, you might find flights through other major carriers, such as:\n",
      "   - **United Airlines**: Typically connects through San Francisco (SFO) or Newark (EWR).\n",
      "   - **Singapore Airlines**: Offers flights connecting through cities like Tokyo or San Francisco.\n",
      "\n",
      "I recommend checking flight comparison websites to find the best options, including prices and layover times. Would you like me to assist you with specific flight searches or upgrades for your trip?\n",
      "\n",
      "You (type 'quit' to quit): As you know we like Delta. But we also prefer to sit first class if possible. Do these kinds of routes offer upgrades normally?\n",
      "\n",
      "Assistant: Yes, Delta Airlines typically offers first-class seating on their routes to Singapore, including both direct and connecting flights. Here are some points regarding upgrades:\n",
      "\n",
      "1. **Upgrade Availability**: Delta often has availability for first-class upgrades, especially for long-haul international flights. However, this can vary based on demand and how far in advance you book.\n",
      "\n",
      "2. **SkyMiles Program**: If you're a member of Delta's SkyMiles program, you may be eligible for complimentary upgrades based on your frequent flyer status. Higher-tier members (like Platinum and Diamond) usually have better chances of securing upgrades.\n",
      "\n",
      "3. **Paid Upgrades**: Delta also offers the option to purchase upgrades to first-class at the time of booking or during check-in. If you see that first-class seats are available, you might be able to upgrade for a fee.\n",
      "\n",
      "4. **Booking Early**: To maximize your chances of getting a first-class seat, it's best to book your flights as early as possible. This allows you to take advantage of any upgrade opportunities that may arise.\n",
      "\n",
      "If you'd like, I can help you search for specific flights and check availability for first-class seating. Would you like to proceed with that?\n",
      "\n",
      "You (type 'quit' to quit): Let's hold on booking for now. Back to activities. Based on what you know about me, what do you think we should do? Design the perfect Sunday for me and my wife in Singapore.\n",
      "\n",
      "Assistant: Designing the perfect Sunday for you and your wife in Singapore, focusing on outdoor activities and culinary experiences, sounds delightful! Here’s a suggested itinerary:\n",
      "\n",
      "### Perfect Sunday in Singapore\n",
      "\n",
      "**Morning: Scenic Start**\n",
      "- **Breakfast at a Local Hawker Center**: Start your day with a hearty breakfast at a hawker center. Try local favorites like kaya toast with soft-boiled eggs and kopi (coffee).\n",
      "- **Visit Gardens by the Bay**: After breakfast, head to Gardens by the Bay. Stroll through the Flower Dome and Cloud Forest, and don’t miss the Supertree Grove. If you're up for it, take the OCBC Skyway for stunning views of the gardens and city skyline.\n",
      "\n",
      "**Midday: Adventure and Relaxation**\n",
      "- **Lunch at Satay by the Bay**: Enjoy some local satay and other grilled delights at this riverside eatery within Gardens by the Bay. The outdoor seating offers a great view.\n",
      "- **Explore Sentosa Island**: After lunch, make your way to Sentosa Island. You can relax on the beach, take a leisurely walk along the coastline, or try the SkyPark for panoramic views. If you're feeling adventurous, consider the zip line or other fun activities.\n",
      "\n",
      "**Afternoon: Culture and Nature**\n",
      "- **Visit Fort Siloso**: Explore Singapore’s military history at Fort Siloso, where you can learn about the island's past through exhibits and preserved fortifications.\n",
      "- **Cycle at East Coast Park**: Rent bicycles and take a scenic ride along East Coast Park. Enjoy the sea breeze and beautiful views as you cycle through this iconic park.\n",
      "\n",
      "**Evening: Culinary Delights**\n",
      "- **Dinner at Clarke Quay**: Head back to the city for dinner at Clarke Quay, where you can choose from a variety of restaurants offering local and international cuisine. Enjoy a riverside dining experience as the sun sets.\n",
      "- **Night Walk at Marina Bay**: Conclude your day with a leisurely walk around Marina Bay. The area comes alive with lights, and you can catch the Spectra light and water show at Marina Bay Sands.\n",
      "\n",
      "### Optional Add-ons:\n",
      "- **Visit Little India**: If time permits, you can explore Little India in the afternoon, sampling some Indian snacks or visiting the vibrant Sri Veeramakaliamman Temple.\n",
      "- **Cocktail at a Rooftop Bar**: Finish your night with a cocktail at one of Singapore’s renowned rooftop bars, like 1-Altitude or Ce La Vi, for breathtaking views of the skyline.\n",
      "\n",
      "This itinerary combines outdoor activities with culinary experiences, allowing you to enjoy the best of what Singapore has to offer. Let me know if you'd like any adjustments or if you want to explore specific activities further!\n",
      "\n",
      "You (type 'quit' to quit): Great as long as there are no shellfish items featured as my wife is very allergic.\n",
      "\n",
      "Assistant: Got it! I've noted that your wife is allergic to shellfish, and I will ensure that all recommendations exclude shellfish items. \n",
      "\n",
      "The suggested itinerary remains the same, but I’ll make sure to highlight dining options that are shellfish-free:\n",
      "\n",
      "### Adjusted Dining Recommendations:\n",
      "- **Breakfast**: At a local hawker center, opt for dishes like kaya toast, roti prata, or mee siam (without seafood).\n",
      "- **Lunch at Satay by the Bay**: Enjoy chicken or beef satay, as well as other grilled options that do not include shellfish.\n",
      "- **Dinner at Clarke Quay**: Choose restaurants that offer a variety of non-seafood dishes. Look for options like chicken rice, vegetarian dishes, or beef rendang.\n",
      "\n",
      "If you have any specific preferences for cuisines or types of dishes, let me know, and I can refine the recommendations further!\n",
      "\n",
      "You (type 'quit' to quit): Excellent. Back to booking flights. For our preferred airline, will you recommend best route to get there from the US?\n",
      "\n",
      "Assistant: Since you prefer flying with Delta, the best route from the US to Singapore typically involves a layover, as there are no direct flights from many US cities. Here are some common options:\n",
      "\n",
      "### Recommended Routes:\n",
      "1. **Delta via Los Angeles (LAX)**:\n",
      "   - **Flight**: Fly from your nearest airport to LAX.\n",
      "   - **Layover**: Connect to a Delta flight from LAX to Singapore (SIN).\n",
      "   - **Duration**: Approximately 20-24 hours total travel time, including layover.\n",
      "\n",
      "2. **Delta via Seattle (SEA)**:\n",
      "   - **Flight**: Depart from your nearest airport to SEA.\n",
      "   - **Layover**: Connect to a Delta flight from SEA to Singapore (SIN).\n",
      "   - **Duration**: Similar to the LAX route, around 20-24 hours total.\n",
      "\n",
      "3. **Delta via Tokyo (NRT)**:\n",
      "   - **Flight**: Fly from your nearest airport to Tokyo Narita (NRT).\n",
      "   - **Layover**: Connect to a Delta flight from NRT to Singapore (SIN).\n",
      "   - **Duration**: Total travel time can vary, generally around 18-22 hours.\n",
      "\n",
      "4. **Delta via New York (JFK)**:\n",
      "   - **Flight**: Fly from your nearest airport to JFK.\n",
      "   - **Layover**: Connect to a Delta flight from JFK to Singapore (SIN).\n",
      "   - **Duration**: Total travel time is typically around 20-23 hours.\n",
      "\n",
      "### Next Steps:\n",
      "- **Check Availability**: You can check Delta's website or a flight aggregator for the best prices and schedules.\n",
      "- **Book Early**: Since you're planning for summer, it's advisable to book your flights soon to get the best rates.\n",
      "\n",
      "Let me know if you need help with anything else related to the flight booking process!\n",
      "\n",
      "You (type 'quit' to quit): Sounds great. We will go with this one. I havent been to Singapore since I was 15 years old when I went with my family during my dads business trip. We had such a great time. I think you are going to help us have a fantastic trip!!\n",
      "\n",
      "Assistant: I'm thrilled to hear that you're excited about your upcoming trip! It's wonderful that you have fond memories of Singapore from your childhood, and I'm here to help you create even more amazing experiences with your wife.\n",
      "\n",
      "If you have any specific places you want to revisit or new experiences you want to try, just let me know! Whether it's more dining recommendations, activities, or tips for navigating the city, I'm here to assist you.\n",
      "\n",
      "You (type 'quit' to quit): quit\n",
      "Thank you for using the Travel Assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_id = input(\"Enter a user ID: \") or \"demo_user\"  # Collect user id for demo\n",
    "    thread_id = input(\"Enter a thread ID: \") or \"demo_thread\"  # Collect thread id for demo\n",
    "except Exception:\n",
    "    # If we're running in CI, we don't have a terminal to input from, so just exit\n",
    "    exit()\n",
    "else:\n",
    "    main(thread_id, user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: This work is inspired by and builds upon the notebook 'agent_memory_tutorial.ipynb' demonstrated in the excellent 'agents-towards-production' repository of Nir Diamant"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
