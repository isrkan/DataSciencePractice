{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1aGZwcomAZ1"
   },
   "source": [
    "# AI-based web research assistant\n",
    "\n",
    "In today's digital landscape, researchers, analysts, and professionals face the challenge of quickly extracting meaningful insights from vast amounts of online information. Traditional web searching often results in information scattered across multiple sources, requiring significant time to read through and synthesize. This notebook presents an intelligent solution that automates the research process by combining web search capabilities with language model-based summarization.\n",
    "\n",
    "Our system addresses the core problem of information overload by automatically searching the web for relevant content and generating concise, actionable summaries. Rather than manually browsing through dozens of search results, users can obtain key insights from multiple sources in a structured format, dramatically reducing research time while maintaining information quality. The tool leverages DuckDuckGo's search API for web queries and OpenAI's language models for intelligent summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "llKwN5U-lscX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import re\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Download necessary NLTK data for text processing\n",
    "nltk.download('punkt', quiet=True)  # Sentence tokenizer\n",
    "nltk.download('stopwords', quiet=True)  # Common stopwords\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su3Ht4PumEIr"
   },
   "source": [
    "### Initialize DuckDuckGo search engine\n",
    "Here we initialize our web search capabilities using DuckDuckGo's search API. DuckDuckGo provides privacy-focused search without requiring API keys and offers reliable results for automated research tasks. This component will serve as our primary interface to the web for gathering information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vepZUIdPmDwn"
   },
   "outputs": [],
   "source": [
    "# Initialize DuckDuckGo Search tool\n",
    "search = DuckDuckGoSearchResults(backend=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtH4Ez3MmDkL"
   },
   "source": [
    "The search object now serves as our gateway to web information. DuckDuckGo's API through LangChain provides a clean interface that returns structured search results including snippets, titles, and links, which we will process in subsequent steps.\n",
    "\n",
    "### Define data models\n",
    "To ensure consistent data handling throughout our application, we define structured data models using Pydantic. This approach provides type safety, validation, and clear documentation of our data structures, making the code more maintainable and less prone to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VcKnQcy1mDVr"
   },
   "outputs": [],
   "source": [
    "class SummarizeText(BaseModel):\n",
    "    \"\"\"Pydantic model for text summarization input validation.\"\"\"\n",
    "    text: str = Field(..., title=\"Text to summarize\", description=\"The text to be summarized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl1j7d3smDIJ"
   },
   "source": [
    "By defining this data model, we create a contract for how text data should be structured when passed to our summarization functions. The `Field` definition provides metadata that can be used for validation and documentation, ensuring robust data handling throughout the application.\n",
    "\n",
    "\n",
    "### Search result processing functions\n",
    "The raw output from search engines requires parsing and structuring to be useful for our summarization pipeline. This section implements functions to perform web searches and transform unstructured search results into clean, organized data that can be easily processed by subsequent components.\n",
    "- Parsing search results: The output from the DuckDuckGo search API is a long, unstructured string. To make it usable, we break it into structured dictionaries that contain the snippet (a short excerpt from the page), the title, and the URL. This gives us clarity and easy access to the key parts of each result.\n",
    "- Performing a web search: A function that conducts a search based on a user query. If a specific website is provided, it performs two searches:\n",
    "  - One focused on the given site (`site:example.com`)\n",
    "  - Another excluding it (`-site:example.com`) to diversify the perspective\n",
    "\n",
    "It then combines and returns a subset of the most relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cYreqPbWmC6G"
   },
   "outputs": [],
   "source": [
    "# Parse raw search results string into structured dictionaries with title, snippet, and link\n",
    "def parse_search_results(results_string: str) -> List[dict]:\n",
    "    \"\"\"Parse a string representation of search results into a list of dictionaries.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Each search result is embedded in a long string separated by ', snippet: '\n",
    "    # Split the results string using the snippet delimiter - this separates individual search results from the concatenated string\n",
    "    entries = results_string.split(', snippet: ')\n",
    "\n",
    "    # Process each entry\n",
    "    for entry in entries[1:]:  # Skip the first split as it's empty\n",
    "        # Extract snippet and title-link portion\n",
    "        parts = entry.split(', title: ')\n",
    "        if len(parts) == 2:\n",
    "            snippet = parts[0]\n",
    "\n",
    "            # Further split to separate title and link\n",
    "            title_link = parts[1].split(', link: ')\n",
    "            if len(title_link) == 2:\n",
    "                title, link = title_link\n",
    "\n",
    "                # Create structured result dictionary\n",
    "                results.append({\n",
    "                    'snippet': snippet,\n",
    "                    'title': title,\n",
    "                    'link': link\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# Execute web search with optional site-specific filtering.\n",
    "def perform_web_search(query: str, specific_site: Optional[str] = None) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "    \"\"\"Perform a web search based on a query, optionally including a specific website.\"\"\"\n",
    "    try:\n",
    "        if specific_site:\n",
    "            # Perform site-specific search using site: operator\n",
    "            specific_query = f\"site:{specific_site} {query}\"\n",
    "            print(f\"Searching for: {specific_query}\")\n",
    "            specific_results = search.run(specific_query)\n",
    "            print(f\"Specific search results: {specific_results}\")\n",
    "            specific_parsed = parse_search_results(specific_results)\n",
    "\n",
    "            # Complement with general search excluding the specific site - this provides broader context while avoiding duplicate content\n",
    "            general_query = f\"-site:{specific_site} {query}\"\n",
    "            print(f\"Searching for: {general_query}\")\n",
    "            general_results = search.run(general_query)\n",
    "            print(f\"General search results: {general_results}\")\n",
    "            general_parsed = parse_search_results(general_results)\n",
    "\n",
    "            # Combine results, prioritizing site-specific content - limit to top 3 results to maintain focus and processing efficiency\n",
    "            combined_results = (specific_parsed + general_parsed)[:3]\n",
    "        else:\n",
    "            # If no specific site is provided, perform a general open search\n",
    "            print(f\"Searching for: {query}\")\n",
    "            web_results = search.run(query)\n",
    "            print(f\"Web results: {web_results}\")\n",
    "            combined_results = parse_search_results(web_results)[:3]\n",
    "\n",
    "        # Extract only the textual snippets for summarization\n",
    "        web_knowledge = [result.get('snippet', '') for result in combined_results]\n",
    "\n",
    "        # Store the title and link of each result separately for attribution\n",
    "        sources = [(result.get('title', 'Untitled'), result.get('link', '')) for result in combined_results]\n",
    "\n",
    "        print(f\"Processed web_knowledge: {web_knowledge}\")\n",
    "        print(f\"Processed sources: {sources}\")\n",
    "        return web_knowledge, sources\n",
    "    except Exception as e:\n",
    "        print(f\"Error in perform_web_search: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el1MwefXmCqB"
   },
   "source": [
    "This section prepares search data for AI summarization in two steps:\n",
    "- `parse_search_results()` takes a raw search result string (typically formatted with `, snippet:`, `, title:`, and `, link:` delimiters) and splits it into clean dictionaries. Each dictionary contains the key elements needed: a snippet of text, the page title, and the URL.\n",
    "- `perform_web_search()` makes use of this parser. It uses the DuckDuckGo tool to issue a query. If a `specific_site` is given, it searches within that site and also outside it for comparison. It then combines the two sets of results and extracts the important parts: just the text for summarization and the source info (title + URL) for reference.\n",
    "\n",
    "By the end of this process, we are left with two clean lists: one with the text to summarize and one with corresponding sources. These are now ready to feed into the language model.\n",
    "\n",
    "### Text summarization function\n",
    "The summarization component leverages OpenAI's language models to distill lengthy web content into concise, actionable insights. This section implements the core AI functionality that transforms raw text snippets into structured summaries with proper source attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ittZ6YsimgQj"
   },
   "outputs": [],
   "source": [
    "# Generate AI-powered summary of web content with source attribution\n",
    "def summarize_text(text: str, source: Tuple[str, str]) -> str:\n",
    "    \"\"\"Summarize the given text using OpenAI's language model.\"\"\"\n",
    "    try:\n",
    "        # # Initialize OpenAI language model - temperature 0.7 provides good balance between consistency and creativity\n",
    "        llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini-2024-07-18\")\n",
    "\n",
    "        # Create structured prompt for consistent summarization output\n",
    "        prompt_template = \"Please summarize the following text in 1-2 bullet points:\\n\\n{text}\\n\\nSummary:\"\n",
    "\n",
    "        # Build prompt template with input variable definition\n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"text\"],\n",
    "        )\n",
    "\n",
    "        # Create processing chain combining prompt and language model\n",
    "        summary_chain = prompt | llm\n",
    "\n",
    "        # Prepare input data for the AI model\n",
    "        input_data = {\"text\": text}\n",
    "\n",
    "        # Execute the summarization process\n",
    "        summary = summary_chain.invoke(input_data)\n",
    "        # Extract content from the AI response object - handle both direct string responses and object responses\n",
    "        summary_content = summary.content if hasattr(summary, 'content') else str(summary)\n",
    "\n",
    "        # Format the final output with source attribution\n",
    "        formatted_summary = f\"Source: {source[0]} ({source[1]})\\n{summary_content.strip()}\\n\"\n",
    "        return formatted_summary\n",
    "    except Exception as e:\n",
    "        # Handle AI model errors\n",
    "        print(f\"Error in summarize_text: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86X8KL8JmhgJ"
   },
   "source": [
    "This function represents the intelligence core of our research assistant. It configures the AI model with appropriate parameters for summarization tasks, constructs effective prompts that guide the model toward concise outputs, and handles the response processing to ensure consistent formatting.\n",
    "\n",
    "\n",
    "### Main search and summarize function\n",
    "The final component brings together all previous elements into a cohesive research workflow. This function orchestrates the entire process from query input to formatted output, managing the complexity of coordinating web searches with AI summarization while maintaining source attribution throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-zUx3K88mmEl"
   },
   "outputs": [],
   "source": [
    "# Execute complete research workflow: search, extract, and summarize\n",
    "def search_summarize(query: str, specific_site: Optional[str] = None) -> str:\n",
    "    \"\"\"Perform a web search and summarize the results.\"\"\"\n",
    "    # Execute web search and extract structured information (content + sources)\n",
    "    web_knowledge, sources = perform_web_search(query, specific_site)\n",
    "\n",
    "    # Validate that search produced usable results\n",
    "    if not web_knowledge or not sources:\n",
    "        print(\"No web knowledge or sources found.\")\n",
    "        return \"\"\n",
    "\n",
    "    # Process each search result through AI summarization - list comprehension with filtering ensures only successful summaries are included\n",
    "    summaries = [\n",
    "        summarize_text(knowledge, source)\n",
    "        for knowledge, source in zip(web_knowledge, sources)\n",
    "        if summarize_text(knowledge, source)  # Filter out empty summaries\n",
    "    ]\n",
    "\n",
    "    # Combine all summaries into a single formatted output - this creates a comprehensive research report from multiple sources\n",
    "    combined_summary = \"\\n\".join(summaries)\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcxZLVC8mhaI"
   },
   "source": [
    "This function coordinates all previously defined subcomponents into a linear workflow. It starts by calling the search engine handler to retrieve snippets and metadata. If nothing is returned, it gracefully exits.\n",
    "\n",
    "Otherwise, it proceeds to run each snippet through the AI summarizer, ensuring that every result is paired with its source title and URL. The function uses a `zip()` to maintain alignment between the content and its source, and filters out any cases where the summarizer returns an empty string (which could happen due to malformed input or transient API issues).\n",
    "\n",
    "The result is a clean summary of key points sourced from across the webâ€”tailored to a single query. This output is ready to be printed, logged, displayed in a UI, or stored.\n",
    "\n",
    "\n",
    "### Example usage\n",
    "To demonstrate the research assistant's capabilities, we will execute a real research query. This example shows how users can leverage the tool for investigating current developments in artificial intelligence, with optional focus on academic sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4RmxjXa5mp7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: site:https://www.nature.com What are the latest advancements in artificial intelligence?\n",
      "Specific search results: \n",
      "Searching for: -site:https://www.nature.com What are the latest advancements in artificial intelligence?\n",
      "General search results: \n",
      "Processed web_knowledge: []\n",
      "Processed sources: []\n",
      "No web knowledge or sources found.\n",
      "Summary of latest advancements in AI (including information from https://www.nature.com):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define research parameters\n",
    "query = \"What are the latest advancements in artificial intelligence?\" # Choose a topic\n",
    "specific_site = \"https://www.nature.com\"  # Optional: specify a site or set to None\n",
    "\n",
    "# Execute the complete research workflow\n",
    "result = search_summarize(query, specific_site)\n",
    "\n",
    "# Display formatted results with clear context\n",
    "print(f\"Summary of latest advancements in AI (including information from {specific_site if specific_site else 'various sources'}):\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
