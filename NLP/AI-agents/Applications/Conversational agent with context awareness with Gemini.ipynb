{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhAT98eqRC0F"
   },
   "source": [
    "# Conversational agent with context awareness using Gemini\n",
    "\n",
    "This notebook demonstrates how to build a conversational agent that maintains memory across interactions within a session using Google Gemini. Traditional chatbots often suffer from a fundamental limitation: they treat each interaction as an isolated event, completely forgetting what was discussed moments before. By implementing conversation history management, we create an AI assistant that can engage in natural, flowing conversations where each response builds upon previous exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zrHkVsf5Q2R0"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Google API key for AI model access\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XydMYILLRLRY"
   },
   "source": [
    "- `RunnableWithMessageHistory` - LangChain provides a flexible abstraction for running \"chains\" of logic — like combining prompts with models. However, by default, these chains are stateless, meaning they don’t remember what happened in previous turns. That's where `RunnableWithMessageHistory` comes in - It wraps our chain and automatically manages inserting past conversation messages into the prompt and updating the memory with new messages after each interaction.\n",
    "- `ChatMessageHistory` is a class that holds a list of past messages (like `\"human\": \"Hello\"` and `\"ai\": \"Hi there!\"`). It’s used by RunnableWithMessageHistory as the actual place where history is stored and retrieved.\n",
    "\n",
    "### Initialize the language model\n",
    "The language model serves as the core intelligence of our conversational agent. Here we will configure the AI model with specific parameters that balance response quality, cost, and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SVrT6DttRLDQ"
   },
   "outputs": [],
   "source": [
    "# Initialize the Gemini language model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    max_output_tokens=1000,  # Limit response length for focused answers\n",
    "    temperature=0  # Set to 0 for deterministic responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRDhzOWqRKz-"
   },
   "source": [
    "This configuration creates our AI model instance using Google's Gemini model. Setting `max_output_tokens=1000` ensures responses remain concise and focused, while `temperature=0` eliminates randomness in responses, making the agent's behavior predictable and consistent across similar inputs. This deterministic approach is particularly valuable for applications requiring reliable and reproducible interactions.\n",
    "\n",
    "### Create a simple in-memory store for chat histories\n",
    "The heart of context awareness lies in effectively managing conversation history across multiple sessions. We need a system that can store, retrieve, and organize conversations for different users or conversation threads. This section implements an in-memory storage solution that maintains separate conversation histories for each unique session identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B9WaVLpuRKjh"
   },
   "outputs": [],
   "source": [
    "# Create a simple in-memory store for chat histories - this dictionary will hold separate conversation histories for each session\n",
    "store = {}\n",
    "\n",
    "def get_chat_history(session_id: str):\n",
    "    \"\"\"Retrieve or create a chat history for a given session ID.\"\"\"\n",
    "    if session_id not in store:\n",
    "        # Create new conversation history if session doesn't exist\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaYIAb8eRKU-"
   },
   "source": [
    "Here, we create a session management system using a simple dictionary-based approach. The `get_chat_history` function acts as a factory method that either retrieves an existing conversation history or creates a new one for first-time sessions. Each `ChatMessageHistory` object maintains a chronological record of messages within its session, enabling the AI to reference previous interactions. This architecture supports concurrent conversations by isolating each session's context, making it suitable for multi-user applications.\n",
    "\n",
    "### Create the prompt template\n",
    "Effective prompt engineering is crucial for guiding the AI's behavior and ensuring consistent, helpful responses. We will create a structured template that defines the AI's role, incorporates conversation history, and processes user input. The template serves as the blueprint for how our agent interprets and responds to interactions within the context of ongoing conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r-eBkXn0RKGI"
   },
   "outputs": [],
   "source": [
    "# Create the prompt template that defines our conversation structure\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # System message establishes the AI's role and behavior guidelines\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    # MessagesPlaceholder dynamically inserts conversation history - this is where previous messages will be injected to provide context\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    # Human message template for processing current user input\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN9p3TbfRJze"
   },
   "source": [
    "This prompt template establishes a three-part conversation structure that forms the foundation of contextual interactions. The system message sets behavioral expectations for the AI, creating consistency in tone and helpfulness. The `MessagesPlaceholder` is the key innovation here – it dynamically injects the entire conversation history into each prompt, ensuring the AI has access to all previous context when generating responses. The human input slot processes the current user query, allowing the template to be reused for any user input while maintaining the established structure.\n",
    "\n",
    "### Combine the prompt and model into a runnable chain\n",
    "Now we will combine our language model with the prompt template to create a processing pipeline. This chain represents the core logic flow: taking user input, combining it with conversation history through our template, and generating contextually aware responses. The goal is to create a seamless integration between prompt engineering and AI model processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AIaJjMSMRhCJ"
   },
   "outputs": [],
   "source": [
    "# Combine the prompt template and language model into a runnable chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9oGQ81PRhbj"
   },
   "source": [
    "This creates a processing pipeline using LangChain's pipe operator. The chain represents the flow from structured input (via our prompt template) to AI-generated output (via the language model). When invoked, this chain will take user input, inject it into our prompt template along with any conversation history, and pass the complete prompt to the language model for response generation.\n",
    "\n",
    "### Wrap the chain with message history\n",
    "Finally, we wrap our basic conversational chain with history management capabilities. This integration automatically handles the storage and retrieval of conversation messages, transforming our stateless chain into a stateful conversational agent. The system will now automatically maintain context across interactions without requiring manual history management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bqamF37FRnJj"
   },
   "outputs": [],
   "source": [
    "# Wrap the chain with message history management capabilities\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,  # The basic conversational chain we built\n",
    "    get_chat_history,  # Function to retrieve/create session history\n",
    "    input_messages_key=\"input\",  # Key name for user input in our prompt template\n",
    "    history_messages_key=\"history\"  # Key name for message history in our prompt template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJphbtzLRr2x"
   },
   "source": [
    "This wrapper transforms our basic chain into a fully context-aware conversational agent. The `RunnableWithMessageHistory` class automatically handles several critical functions: it retrieves the appropriate conversation history before each interaction, injects that history into our prompt template, processes the user's input through our AI model, and then stores both the user's message and the AI's response for future reference. The key mappings ensure that user input and conversation history are correctly placed within our prompt template structure.\n",
    "\n",
    "### Example usage\n",
    "With our context-aware system complete, we can now test its capabilities through a series of interactions. This demonstration will show how the agent maintains context across multiple exchanges, remembering previous interactions and building upon them naturally. We will use a consistent session ID to ensure all interactions belong to the same conversation thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PB4dp6ZlRrmR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm doing well, thank you for asking!  How are you today?\n",
      "AI: Your previous message was \"Hello! How are you?\"\n"
     ]
    }
   ],
   "source": [
    "# Unique identifier for this conversation session\n",
    "session_id = \"user_123\"\n",
    "\n",
    "# First interaction\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hello! How are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response1.content)\n",
    "\n",
    "# Second interaction\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What was my previous message?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"AI:\", response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUlHAxsLRrOw"
   },
   "source": [
    "Each `invoke` call includes a configuration specifying the session ID, ensuring that both interactions contribute to the same conversational context.\n",
    "\n",
    "### Print the conversation history\n",
    "We can examine the stored conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "We3-a_qWR3XR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation History:\n",
      "human: Hello! How are you?\n",
      "ai: I'm doing well, thank you for asking!  How are you today?\n",
      "human: What was my previous message?\n",
      "ai: Your previous message was \"Hello! How are you?\"\n"
     ]
    }
   ],
   "source": [
    "# Print the complete conversation history for inspection\n",
    "print(\"\\nConversation History:\")\n",
    "for message in store[session_id].messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uPUSM-xR28U"
   },
   "source": [
    "By iterating through the stored messages, we can see exactly what information the agent has access to during each interaction. The output will show both user messages (human) and AI responses (ai) in chronological order, demonstrating how the conversation history builds over time.\n",
    "\n",
    "This foundation provides a starting point for building more advanced conversational AI applications, from customer service assistants to educational tutors, where maintaining context significantly enhances the user experience and interaction quality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
