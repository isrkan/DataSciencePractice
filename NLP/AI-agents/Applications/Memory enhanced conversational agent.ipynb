{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv7iECC9vXjo"
   },
   "source": [
    "# Memory-enhanced conversational agent\n",
    "\n",
    "This notebook explores building a sophisticated conversational agent that implements dual-layer memory architecture. We distinguish between short-term memory (immediate conversation context) and long-term memory (persistent information that transcends individual sessions). This approach enables the agent to not only maintain conversational flow but also build relationships with users over time, remembering key details that enhance future interactions.\n",
    "\n",
    "While basic conversational agents can maintain context within a single conversation session, they face a significant limitation: they cannot retain important information beyond the immediate chat history.\n",
    "\n",
    "The challenge lies in intelligently deciding what information deserves long-term storage and how to effectively integrate both memory layers into the conversation generation process. We will implement a practical solution that balances memory efficiency with conversational quality, creating an agent that becomes more helpful and personalized with each interaction. We go further by implementing short-term memory (to retain recent message history) and long-term memory (to remember important facts across multiple messages or sessions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UMVEOFZkvW5r"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API key for AI model access\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G4ouiZbvi1t"
   },
   "source": [
    "### Initialize the language model\n",
    "The language model serves as the core intelligence of our conversational agent. Here we will configure the AI model with specific parameters that balance response quality, cost, and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TrpeLn6WvipS"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    max_tokens=1000,  # Limit response length for focused answers\n",
    "    temperature=0  # Set to 0 for deterministic responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvjga7Otvibc"
   },
   "source": [
    "This configuration creates our AI model instance. Setting `max_tokens=1000` ensures responses remain concise and focused, while `temperature=0` eliminates randomness in responses, making the agent's behavior predictable and consistent across similar inputs. This deterministic approach is particularly valuable for applications requiring reliable and reproducible interactions.\n",
    "\n",
    "### Creating memory storage systems\n",
    "The heart of our enhanced conversational agent lies in its dual-memory architecture. We need to create separate storage systems for short-term conversational context of the chat history and long-term persistent memories that survive across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J2CsYGxZviM9"
   },
   "outputs": [],
   "source": [
    "# Memory stores for managing different types of conversational context\n",
    "chat_store = {}  # Dictionary to store short-term conversation histories by session\n",
    "long_term_memory = {}  # Dictionary to store persistent memories across sessions\n",
    "\n",
    "# Function to retrieve or create a chat history for immediate conversation context - It ensures each user session has its own conversation context\n",
    "def get_chat_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        # Initialize new conversation history for first-time sessions\n",
    "        chat_store[session_id] = ChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# Function to update long-term memory with significant interactions - It uses simple heuristics to determine what information is worth remembering\n",
    "def update_long_term_memory(session_id: str, input: str, output: str):\n",
    "    if session_id not in long_term_memory:\n",
    "        long_term_memory[session_id] = []\n",
    "    # Store user inputs that contain substantial information (longer than 20 characters)\n",
    "    if len(input) > 20:\n",
    "        long_term_memory[session_id].append(f\"User said: {input}\")\n",
    "    # Maintain memory efficiency by keeping only the most recent 5 memories\n",
    "    if len(long_term_memory[session_id]) > 5:\n",
    "        long_term_memory[session_id] = long_term_memory[session_id][-5:]\n",
    "\n",
    "# Fumction to retrieve and format long-term memories for inclusion in prompts. It returns a concatenated string of relevant past interactions.\n",
    "def get_long_term_memory(session_id: str):\n",
    "    # Join all long-term memories into a coherent context string\n",
    "    return \". \".join(long_term_memory.get(session_id, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrS8hXLqviAH"
   },
   "source": [
    "Here, we create a memory management system with clear separation of concerns. The `chat_store` maintains immediate conversational context using LangChain's `ChatMessageHistory`, ensuring compatibility with the framework's conversation management tools. The `long_term_memory` system implements custom logic for determining what information deserves persistent storage, using message length as a simple but effective heuristic for importance.\n",
    "\n",
    "The memory management strategy prevents unbounded growth by limiting long-term storage to the five most recent important interactions. This approach balances memory utility with system performance, ensuring that the most relevant recent information remains accessible while preventing memory bloat that could degrade performance over time.\n",
    "\n",
    "\n",
    "### Create prompt template for memory integration\n",
    "The prompt template is where we orchestrate how different types of memory and current input come together to create context-aware responses. This structure determines how the language model interprets and uses both immediate and historical context. We will create a prompt template that includes both short-term and long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0sj8aOtuvh0K"
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that incorporates multiple memory layers\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the information from long-term memory if relevant.\"),\n",
    "    (\"system\", \"Long-term memory: {long_term_memory}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # Placeholder for conversation history\n",
    "    (\"human\", \"{input}\")  # Current user input\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FeeKzLrvhku"
   },
   "source": [
    "This prompt template creates a layered context structure that gives the language model access to different types of information. The system messages establish the AI's role and provide long-term memory context, while the `MessagesPlaceholder` dynamically inserts the current conversation history. This design allows the model to consider both immediate conversational context and persistent memories when generating responses, creating more coherent and personalized interactions.\n",
    "\n",
    "The separation of long-term memory into its own system message ensures that persistent information receives appropriate weight in the AI's response generation process. This architectural choice prevents long-term memories from being overshadowed by more recent conversation history while maintaining clear boundaries between different types of contextual information.\n",
    "\n",
    "### Building the conversational chain\n",
    "Now we need to combine our prompt template with the language model and wrap it with history management capabilities. This creates the core processing pipeline that handles user inputs and generates contextually appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rf9ZgKWPvxjh"
   },
   "outputs": [],
   "source": [
    "# Combine prompt template with language model to create the base conversation chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrap the chain with message history management for short-term memory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,  # The base conversation chain\n",
    "    get_chat_history,  # Function for short-term memory retrieval\n",
    "    input_messages_key=\"input\",  # Key for user input in the prompt template\n",
    "    history_messages_key=\"history\"  # Key for conversation history in the prompt template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2fkzfpJv0Fk"
   },
   "source": [
    "Here, we create our complete conversational pipeline by chaining the prompt template with the language model using LangChain's pipe operator. The `RunnableWithMessageHistory` wrapper then adds automatic conversation history management, ensuring that each interaction builds upon previous messages in the same session. The key parameters tell the system where to find the current input and where to inject the conversation history within our prompt template.\n",
    "\n",
    "### Creating the main chat interface\n",
    "The chat function serves as the primary interface for user interactions, orchestrating the flow of information between memory systems, the language model, and response generation while ensuring both types of memory are properly updated. We will create a function to handle chat interactions, including updating long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zyFXuCfxvz2A"
   },
   "outputs": [],
   "source": [
    "#Handle a conversation interaction with dual-memory integration.\n",
    "def chat(input_text: str, session_id: str):\n",
    "    # Retrieve relevant long-term memories for this session\n",
    "    long_term_mem = get_long_term_memory(session_id)\n",
    "\n",
    "    # Process the input through our conversational chain with both memory types\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"input\": input_text, \"long_term_memory\": long_term_mem},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # Update long-term memory based on this interaction\n",
    "    update_long_term_memory(session_id, input_text, response.content)\n",
    "\n",
    "    # Return the AI's response\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLp_Dc4Fvzhy"
   },
   "source": [
    "This function orchestrates the entire conversation flow by first retrieving any relevant long-term memories, then invoking our conversational chain with both the current input and memory context. The session configuration ensures that short-term memory (conversation history) is properly managed, while the explicit call to update long-term memory ensures that significant interactions are preserved for future conversations.\n",
    "\n",
    "### Testing the memory-enhanced agent\n",
    "Let's demonstrate how our agent maintains context within a conversation with a series of interactions and remembers information for future interactions. This example shows both short-term memory (immediate context) and long-term memory (persistent information) in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "poxl8PIXv7XB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello, Alice! How can I assist you today?\n",
      "AI: I don't have real-time weather data, but you can check a weather website or app for the most accurate and up-to-date information. If you tell me your location, I can suggest how to find the weather for your area!\n",
      "AI: Sunny days are wonderful! They can really lift your mood and are perfect for outdoor activities. Do you have any favorite things you like to do on sunny days?\n",
      "AI: Yes, your name is Alice! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Example conversation demonstrating memory capabilities\n",
    "session_id = \"user_123\"\n",
    "\n",
    "print(\"AI:\", chat(\"Hello! My name is Alice.\", session_id))\n",
    "print(\"AI:\", chat(\"What's the weather like today?\", session_id))\n",
    "print(\"AI:\", chat(\"I love sunny days.\", session_id))\n",
    "print(\"AI:\", chat(\"Do you remember my name?\", session_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf51Aq9xv9cC"
   },
   "source": [
    "This test sequence demonstrates the agent's memory capabilities in action. The first interaction introduces the user's name, which should be stored in long-term memory since it's longer than our 20-character threshold. The subsequent interactions build conversational context while the final question tests whether the agent can recall information from earlier in the conversation. When executed, we should see the AI successfully remember \"Alice\" and potentially reference the user's preference for sunny days, showcasing both the immediate conversational flow and the persistent memory functionality.\n",
    "\n",
    "### Review Memory\n",
    "Let's review the conversation history and long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z5y_QL76v_wV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation History:\n",
      "human: Hello! My name is Alice.\n",
      "ai: Hello, Alice! How can I assist you today?\n",
      "human: What's the weather like today?\n",
      "ai: I don't have real-time weather data, but you can check a weather website or app for the most accurate and up-to-date information. If you tell me your location, I can suggest how to find the weather for your area!\n",
      "human: I love sunny days.\n",
      "ai: Sunny days are wonderful! They can really lift your mood and are perfect for outdoor activities. Do you have any favorite things you like to do on sunny days?\n",
      "human: Do you remember my name?\n",
      "ai: Yes, your name is Alice! How can I help you today?\n",
      "\n",
      "Long-term Memory:\n",
      "User said: Hello! My name is Alice.. User said: What's the weather like today?. User said: Do you remember my name?\n"
     ]
    }
   ],
   "source": [
    "# Examine the complete conversation history (short-term memory)\n",
    "print(\"Conversation History:\")\n",
    "for message in chat_store[session_id].messages:\n",
    "    print(f\"{message.type}: {message.content}\")\n",
    "\n",
    "# Review what information was deemed important for long-term storage\n",
    "print(\"\\nLong-term Memory:\")\n",
    "print(get_long_term_memory(session_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B05lr2MFwAZZ"
   },
   "source": [
    "The conversation history shows the complete short-term memory, including both user inputs and AI responses in chronological order. The long-term memory output reveals which pieces of information our system has identified as significant enough for persistent storage, helping us evaluate the effectiveness of our memory selection criteria."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
