{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6GkIZtuE_1"
   },
   "source": [
    "# AI weekly news reporter\n",
    "\n",
    "The artificial intelligence landscape evolves at fast speed, with new breakthroughs, research findings, and industry developments emerging daily. For professionals, researchers, and enthusiasts trying to stay informed, this creates a significant challenge: how to efficiently filter through countless news sources, understand complex technical content, and synthesize information into actionable insights?\n",
    "\n",
    "This notebook presents a solution through an automated news aggregation and summarization system built on a multi-agent architecture. Rather than manually scouring multiple news sources and struggling with technical jargon, our system orchestrates three specialized AI agents that work together to collect, process, and present AI/ML news in an accessible format.\n",
    "\n",
    "The system demonstrates key concepts in modern AI development: multi-agent coordination, state management, and workflow orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h6T4qBAiuCmc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any, TypedDict, Optional\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpo8s9jKuT-n"
   },
   "source": [
    "### LLM & web search service initialization\n",
    "We initialize the external services that power our multi-agent system. This includes setting up our web search capabilities and language model interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZsCjUTU1uTxE"
   },
   "outputs": [],
   "source": [
    "# Initialize Tavily client for web search functionality\n",
    "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "# Configure language model with optimized parameters\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.1,  # Low temperature for consistent, focused outputs\n",
    "    max_tokens=600  # Reasonable limit for summary generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Tdtdu7BuTmk"
   },
   "source": [
    "Here we are creating our service clients that will be used throughout the workflow.\n",
    "- The Tavily client provides web search capabilities specifically designed for AI applications.\n",
    "- The ChatOpenAI client gives us access to GPT models. The configuration parameters are chosen to balance performance, cost, and output quality - low temperature ensures consistent summarization while the token limit prevents overly verbose responses.\n",
    "\n",
    "### Data models and state management\n",
    "Effective multi-agent systems require well-defined data structures to ensure type safety and clear communication between components. We will define our data models using Pydantic for validation and TypedDict for state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AHAA6zhquTb6"
   },
   "outputs": [],
   "source": [
    "class Article(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a single news article\n",
    "\n",
    "    Attributes:\n",
    "        title (str): Article headline\n",
    "        url (str): Source URL for reference\n",
    "        content (str): Full article content for summarization\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "class Summary(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents a processed article summary\n",
    "\n",
    "    Attributes:\n",
    "        title (str): Original article title\n",
    "        summary (str): AI-generated summary\n",
    "        url (str): Source URL for reference\n",
    "    \"\"\"\n",
    "    title: str\n",
    "    summary: str\n",
    "    url: str\n",
    "\n",
    "# This defines what information we can store and pass between nodes later\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Maintains workflow state between agents\n",
    "\n",
    "    Attributes:\n",
    "        articles (Optional[List[Article]]): Found articles from search phase\n",
    "        summaries (Optional[List[Summary]]): Generated summaries from processing phase\n",
    "        report (Optional[str]): Final compiled report\n",
    "    \"\"\"\n",
    "    articles: Optional[List[Article]]\n",
    "    summaries: Optional[List[Summary]]\n",
    "    report: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtf54_-auTRi"
   },
   "source": [
    "These data models serve as the backbone of our system's communication. The `Article` class uses Pydantic's validation to ensure data integrity, while the `Summary` and `GraphState` classes use TypedDict for lightweight type checking. The `GraphState` class is particularly important as it defines the \"memory\" of our workflow - each agent can access what previous agents have accomplished and add their own contributions.\n",
    "\n",
    "### Agent implementation\n",
    "Now we will implement our three specialized agents. Each agent encapsulates specific functionality and can be tested and maintained independently.\n",
    "\n",
    "#### 1. News searcher agent\n",
    "The NewsSearcher agent is responsible for discovering relevant AI/ML content from web sources. It interfaces with the Tavily API to perform intelligent search queries and retrieve high-quality articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0i0E3vbPuTGT"
   },
   "outputs": [],
   "source": [
    "class NewsSearcher:\n",
    "    \"\"\"\n",
    "    Agent responsible for finding relevant AI/ML news articles using the Tavily search API\n",
    "    \"\"\"\n",
    "\n",
    "    def search(self) -> List[Article]:\n",
    "        \"\"\"\n",
    "        Performs news search with configured parameters\n",
    "\n",
    "        Returns:\n",
    "            List[Article]: Collection of found articles\n",
    "        \"\"\"\n",
    "        # Perform advanced search with specific parameters\n",
    "        response = tavily.search(\n",
    "            query=\"artificial intelligence and machine learning news\",  # Targeted query\n",
    "            topic=\"news\",  # Focus on news content specifically\n",
    "            time_period=\"1w\",  # Recent articles only (last week)\n",
    "            search_depth=\"advanced\",  # Comprehensive search algorithm\n",
    "            max_results=5  # Manageable number for processing\n",
    "        )\n",
    "\n",
    "        # Transform search results into validated Article objects\n",
    "        articles = []\n",
    "        for result in response['results']:\n",
    "            articles.append(Article(\n",
    "                title=result['title'],\n",
    "                url=result['url'],\n",
    "                content=result['content']\n",
    "            ))\n",
    "\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHc2ME7kuS65"
   },
   "source": [
    "The `NewsSearcher` agent encapsulates all the complexity of web search behind a simple interface. It uses Tavily's advanced search capabilities to find recent, relevant AI/ML news. The search parameters are carefully chosen: we focus on news content from the past week using advanced search depth for comprehensive results. The agent transforms raw search results into validated `Article` objects, ensuring data consistency for downstream processing.\n",
    "\n",
    "#### 2. Summarizer agent\n",
    "The Summarizer agent transforms complex technical content into accessible summaries. It leverages language models to simplify jargon and present key insights in a format suitable for general audiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gq5nG5pOuSvE"
   },
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    \"\"\"\n",
    "    Agent that processes articles and specializes in content simplification and summarization using gpt-4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define the agent's core instruction for consistent behavior\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI expert who makes complex topics accessible\n",
    "        to general audiences. Summarize this article in 2-3 sentences, focusing on the key points\n",
    "        and explaining any technical terms simply.\n",
    "        \"\"\"\n",
    "\n",
    "    def summarize(self, article: Article) -> str:\n",
    "        \"\"\"\n",
    "        Generates an accessible summary of a single article\n",
    "\n",
    "        Args:\n",
    "            article (Article): Article to summarize\n",
    "\n",
    "        Returns:\n",
    "            str: Generated summary\n",
    "        \"\"\"\n",
    "        # Construct conversation with system prompt and article content\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Title: {article.title}\\n\\nContent: {article.content}\")\n",
    "        ])\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2thoYuiKuShw"
   },
   "source": [
    "The `Summarizer` agent addresses an aspect of technical content curation: making complex information accessible. The system prompt is crafted to ensure consistent behavior across all articles, emphasizing clarity and accessibility. The agent processes each article individually, allowing for focused attention on the specific content and technical concepts present in each piece.\n",
    "\n",
    "#### 3. Publisher agent\n",
    "The Publisher agent serves as the final step in our workflow, compiling individual summaries into a cohesive, professional report. It handles formatting, organization, and persistence of the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lMRqVvBBuSU4"
   },
   "outputs": [],
   "source": [
    "class Publisher:\n",
    "    \"\"\"\n",
    "    Agent that compiles summaries into a formatted report and saves it to disk\n",
    "    \"\"\"\n",
    "\n",
    "    def create_report(self, summaries: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Creates and saves a formatted markdown report\n",
    "\n",
    "        Args:\n",
    "            summaries (List[Dict]): Collection of article summaries\n",
    "\n",
    "        Returns:\n",
    "            str: Generated report content\n",
    "        \"\"\"\n",
    "        # Define report structure and formatting requirements\n",
    "        prompt = \"\"\"\n",
    "        Create a weekly AI/ML news report for the general public.\n",
    "        Format it with:\n",
    "        1. A brief introduction\n",
    "        2. The main news items with their summaries\n",
    "        3. Links for further reading\n",
    "\n",
    "        Make it engaging and accessible to non-technical readers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare summary data for the language model\n",
    "        summaries_text = \"\\n\\n\".join([\n",
    "            f\"Title: {item['title']}\\nSummary: {item['summary']}\\nSource: {item['url']}\"\n",
    "            for item in summaries\n",
    "        ])\n",
    "\n",
    "        # Generate the complete report using the language model\n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=prompt),\n",
    "            HumanMessage(content=summaries_text)\n",
    "        ])\n",
    "\n",
    "        # Add metadata and persist the report\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        markdown_content = f\"\"\"\n",
    "        Generated on: {current_date}\n",
    "\n",
    "        {response.content}\n",
    "        \"\"\"\n",
    "\n",
    "        # Save report to disk for future reference\n",
    "        filename = f\"ai_news_report_{current_date}.md\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(markdown_content)\n",
    "\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqf196dauSGW"
   },
   "source": [
    "The `Publisher` agent transforms individual summaries into a cohesive narrative. It uses the language model not just for summarization, but for editorial decision-making about structure, flow, and presentation. The agent handles both the creative aspects (report formatting and narrative flow) and the practical aspects (file persistence and metadata management). This demonstrates how agents can serve multiple roles within a single specialized function.\n",
    "\n",
    "### Workflow implementation\n",
    "With our agents implemented, we need to orchestrate their interaction through a workflow system. LangGraph provides the infrastructure for managing state flow and agent coordination.\n",
    "\n",
    "#### State management nodes\n",
    "In LangGraph, nodes represent individual processing steps that transform the workflow state and delegate their tasks to dedicated agents. Each node takes the current state, performs its specialized function, and returns an updated state. The `NewsSearcher`, `Summarizer`, and `Publisher` agents encapsulate the business logic, while nodes act as connectors between these agents and the LangGraph system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kg4Jqagcu-xJ"
   },
   "outputs": [],
   "source": [
    "# Node for searching news articles using the Tavily API\n",
    "def search_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for article search. This node initializes the workflow by finding relevant articles and updating the state with search results\n",
    "\n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with found articles\n",
    "    \"\"\"\n",
    "    searcher = NewsSearcher()  # Instantiate the news search agent\n",
    "    # Use the agent to fetch relevant AI/ML news articles from Tavily\n",
    "    state['articles'] = searcher.search()  # Add articles to workflow state\n",
    "    return state\n",
    "\n",
    "# Node for summarizing each retrieved article using the Summarizer agent\n",
    "def summarize_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for article summarization. This node processes articles from the search phase and generates summaries for each piece of content\n",
    "\n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with summaries\n",
    "    \"\"\"\n",
    "    summarizer = Summarizer() # Instantiate the summarizer agent\n",
    "    state['summaries'] = []  # Initialize summaries list in the state\n",
    "\n",
    "    # Process each article individually to generate summaries\n",
    "    for article in state['articles']: # Uses articles from previous node\n",
    "        summary = summarizer.summarize(article)\n",
    "\n",
    "        # Add the structured summary information back to the state\n",
    "        state['summaries'].append({\n",
    "            'title': article.title,\n",
    "            'summary': summary,\n",
    "            'url': article.url\n",
    "        })\n",
    "    return state\n",
    "\n",
    "# Node for compiling all summaries into a final markdown report\n",
    "def publish_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Node for report generation. This node compiles summaries into a final report and handles persistence and formatting\n",
    "\n",
    "    Args:\n",
    "        state (Dict[str, Any]): Current workflow state\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Updated state with final report\n",
    "    \"\"\"\n",
    "    publisher = Publisher()  # Instantiate the publisher agent\n",
    "\n",
    "    # Generate a full markdown report using the summaries\n",
    "    report_content = publisher.create_report(state['summaries'])\n",
    "\n",
    "    # Save the report to state for inspection or downstream use\n",
    "    state['report'] = report_content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgAxMdnYvFIw"
   },
   "source": [
    "These node functions serve as adapters between our agent classes and the LangGraph workflow system. Each node follows the same pattern: instantiate the appropriate agent, execute its primary function, update the state with results, and return the modified state. This approach maintains clear separation between business logic (in the agents) and workflow coordination (in the nodes).\n",
    "* The `search_node` is responsible for initiating the entire workflow. It uses the `NewsSearcher` agent to query the Tavily API for the latest AI/ML news, and stores the resulting articles in the state under the key `\"articles\"`.\n",
    "* The `summarize_node` consumes the articles from the previous node, then loops through them one-by-one, passing each to the `Summarizer` agent. Each result is appended as a new summary in a structured format (`title`, `summary`, `url`) under the `\"summaries\"` key.\n",
    "* Finally, the `publish_node` aggregates all the generated summaries into a well-formatted markdown report using the `Publisher` agent. It stores the result in the state under `\"report\"`, and also handles saving the report to disk.\n",
    "\n",
    "Each node adheres to the same contract:\n",
    "1. Accept the current state (`Dict[str, Any]`),\n",
    "2. Operate on relevant parts of it,\n",
    "3. Update the state with new information,\n",
    "4. Return the updated state to be passed to the next node.\n",
    "\n",
    "This consistent structure makes it easy to reason about and maintain the system, and also ensures it is extensible — adding more nodes (e.g., for fact-checking, trend analysis, or translation) would follow the same pattern.\n",
    "\n",
    "#### Workflow graph creation\n",
    "The final step is constructing the workflow graph that defines how our nodes interact and in what sequence they execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NCVlhqD3vEcx"
   },
   "outputs": [],
   "source": [
    "def create_workflow() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Constructs and configures the workflow graph\n",
    "    search -> summarize -> publish\n",
    "\n",
    "    Returns:\n",
    "        StateGraph: Compiled workflow ready for execution\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a workflow (graph) initialized with our state schema\n",
    "    workflow = StateGraph(state_schema=GraphState)\n",
    "\n",
    "    # Add processing nodes that we will flow between\n",
    "    workflow.add_node(\"search\", search_node)\n",
    "    workflow.add_node(\"summarize\", summarize_node)\n",
    "    workflow.add_node(\"publish\", publish_node)\n",
    "\n",
    "    # Define the flow with edges\n",
    "    workflow.add_edge(\"search\", \"summarize\") # search results flow to summarizer\n",
    "    workflow.add_edge(\"summarize\", \"publish\") # summaries flow to publisher\n",
    "\n",
    "    # Set where to start\n",
    "    workflow.set_entry_point(\"search\")\n",
    "\n",
    "    # Compile the workflow for execution\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5zs3P-fvLsc"
   },
   "source": [
    "The workflow graph construction creates a DAG that defines our processing pipeline. The linear flow (search → summarize → publish) reflects the dependencies between our processing steps. Each step builds upon the previous one, creating a clear, predictable execution path. The compiled workflow can be executed multiple times with different inputs, making it reusable and testable.\n",
    "\n",
    "### Usage example\n",
    "With all components implemented, we can now execute our complete news processing system. Here, we demonstrate how to run the workflow and access the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xrCD3-J3vPJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AI/ML Weekly News Report ===\n",
      "\n",
      "# Weekly AI/ML News Report\n",
      "\n",
      "**Introduction:**\n",
      "Welcome to this week's AI and Machine Learning news report! As technology continues to evolve, artificial intelligence (AI) and machine learning (ML) are making waves across various industries, from healthcare to cybersecurity. In this report, we’ll explore some of the latest developments that are shaping our world, making it more efficient, sustainable, and secure. Let’s dive in!\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Merck's Tailored Approach to AI in Drug Development\n",
      "**Summary:**  \n",
      "Merck is taking a unique approach to artificial intelligence and machine learning by moving away from a one-size-fits-all strategy in drug development. By leveraging AI, they have enhanced the safety and effectiveness of their medications, notably with the drug MK-1084, which was optimized using both historical and new data. This tailored approach allows Merck to create solutions that meet their specific needs and those of their customers, paving the way for more personalized medicine.\n",
      "\n",
      "**Read more:** [Genetic Engineering and Biotechnology News](https://www.genengnews.com/topics/artificial-intelligence/solution-building-merck-avoids-one-size-fits-all-approach-to-ai-and-ml)\n",
      "\n",
      "---\n",
      "\n",
      "### 2. AI's Role in Promoting Environmental Sustainability\n",
      "**Summary:**  \n",
      "Artificial intelligence is proving to be a game-changer for environmental sustainability. By quickly analyzing vast amounts of data, AI can uncover patterns that humans might overlook, leading to better decision-making in sustainability efforts. This capability is crucial as we face pressing environmental challenges, and machine learning is at the forefront of this initiative, helping organizations make impactful changes for a greener future.\n",
      "\n",
      "**Read more:** [Forbes](https://www.forbes.com/sites/lbsbusinessstrategyreview/2025/06/24/ai-and-the-future-of-sustainability-building-intelligence-for-impact/)\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Transforming Cybersecurity with AI Investments\n",
      "**Summary:**  \n",
      "The surge in investments in artificial intelligence is revolutionizing the cybersecurity landscape. AI technologies are designed to replicate and enhance human abilities in identifying and responding to cyber threats. With tools like machine learning and natural language processing, these systems can analyze security issues and respond more efficiently, often without needing explicit instructions. This advancement is crucial for keeping our digital environments safe and secure.\n",
      "\n",
      "**Read more:** [Forbes](https://www.forbes.com/sites/chuckbrooks/2025/06/27/surging-investments-in-ai-are-transforming-cybersecurity/)\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion:**\n",
      "This week’s highlights showcase the diverse applications of AI and ML, from improving healthcare to enhancing our environmental efforts and securing our digital spaces. As these technologies continue to evolve, they promise to bring about significant changes that can benefit society as a whole. Stay tuned for more updates in the world of AI and ML!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the workflow system\n",
    "    workflow = create_workflow()\n",
    "\n",
    "    # Execute the workflow with initial empty state\n",
    "    final_state = workflow.invoke({\n",
    "        \"articles\": None,  # Will be populated by search node\n",
    "        \"summaries\": None,  # Will be populated by summarize node\n",
    "        \"report\": None  # Will be populated by publish node\n",
    "    })\n",
    "\n",
    "    # Display the final generated report\n",
    "    print(\"\\n=== AI/ML Weekly News Report ===\\n\")\n",
    "    print(final_state['report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX1QUBZSLY4j"
   },
   "source": [
    "Here, we demonstrate the simplicity of running our complex multi-agent system. We start with an empty state and let each agent contribute its specialized functionality. The final state contains the complete processing history: the original articles, their summaries, and the final report. This provides full transparency into the system's operation and allows for debugging and quality assessment at each stage.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (langgraph-env)",
   "language": "python",
   "name": "langgraph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
