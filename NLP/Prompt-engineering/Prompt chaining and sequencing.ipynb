{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3MpljFzZsCb"
   },
   "source": [
    "# Prompt chaining and sequencing\n",
    "\n",
    "This notebook explores the concepts of prompt chaining and sequencing in the context of working with LLMs. As AI applications become more sophisticated, the ability to handle more complex, multi-step tasks becomes increasingly critical. In many scenarios, a single prompt or model response isn't sufficient to solve complex problems or generate sophisticated outputs. Instead, tasks often need to be broken down into smaller, more manageable parts, each requiring a specific response or analysis.\n",
    "\n",
    "Prompt chaining and sequencing are techniques designed to address this challenge. These approaches enable us to guide LLMs through a series of interconnected steps, where the output from one prompt becomes the input for the next. This structured flow is akin to a step-by-step process, where each prompt builds on the last, ensuring that the model's outputs remain coherent and aligned with the overall goal.\n",
    "\n",
    "This method is particularly valuable for tasks that require nuanced reasoning, multiple stages of processing, or decisions that need to be informed by prior steps. By using these techniques, we can construct more sophisticated AI-driven applications that are capable of handling complex problems, ensuring that the process is not only systematic but also more transparent and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jVhPZu3RZrDZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqZPVx-MZ0Dq"
   },
   "source": [
    "### Initialize the language model\n",
    "We instantiate a lightweight GPT model from OpenAI using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kzD7AZtDZzl1"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9NSC862ZzZf"
   },
   "source": [
    "## Basic Prompt Chaining\n",
    "In the first example, we will demonstrate simple prompt chaining. This involves creating two simple prompts: one to generate a short story based on a genre, and another to summarize that story. The output of the first prompt (the story) will be passed as input to the second prompt (the summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S8_tUyG-ZzKL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: In the year 2147, humanity discovered a hidden planet orbiting a dying sun, inhabited by sentient beings of pure light. These Lumarians shared a unique ability: they could merge their consciousness with any living creature, revealing the universe's secrets through shared memories. As an interstellar diplomat, Elara connected with a Lumarian named Zephyr, experiencing the birth of stars and the fall of civilizations. In that moment, she understood that knowledge was not just power, but a shared journey, and she vowed to protect the Lumarians from those who sought to exploit their gift.\n",
      "\n",
      "Summary: In 2147, interstellar diplomat Elara forms a profound connection with a Lumarian named Zephyr, discovering the power of shared knowledge and vowing to protect their unique abilities from exploitation.\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "story_prompt = PromptTemplate(\n",
    "    input_variables=[\"genre\"],  # Define 'genre' as an input variable for the prompt\n",
    "    template=\"Write a short {genre} story in 3-4 sentences.\"  # Template for generating the story\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"story\"],  # Define 'story' as an input variable for the prompt\n",
    "    template=\"Summarize the following story in one sentence:\\n{story}\"  # Template for summarizing the story\n",
    ")\n",
    "\n",
    "# Chain the prompts\n",
    "def story_chain(genre):\n",
    "    \"\"\"Generate a story and its summary based on a given genre.\n",
    "\n",
    "    Args:\n",
    "        genre (str): The genre of the story to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated story and its summary.\n",
    "    \"\"\"\n",
    "    # Generate the story based on the input genre using the first prompt\n",
    "    story = (story_prompt | llm).invoke({\"genre\": genre}).content\n",
    "\n",
    "    # Generate the summary based on the story generated in the previous step\n",
    "    summary = (summary_prompt | llm).invoke({\"story\": story}).content\n",
    "\n",
    "    return story, summary  # Return both the story and its summary\n",
    "\n",
    "# Test the chain\n",
    "genre = \"science fiction\"  # Define the genre for the story\n",
    "story, summary = story_chain(genre)  # Run the prompt chain\n",
    "print(f\"Story: {story}\\n\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQeYSkawhUuW"
   },
   "source": [
    "1. Prompt template definitions: The `PromptTemplate` class from LangChain is used to define templates for each prompt. These templates allow for dynamic insertion of variables into the prompt, enabling the model to generate content based on specific inputs.\n",
    "   - `story_prompt`: This prompt is designed to generate a short story based on a given genre. The genre is provided as an input variable and is used within the template to generate a unique story based on that genre.\n",
    "   - `summary_prompt`: This prompt takes a story and asks the model to summarize it in one sentence. The story is passed as the input variable, and the model is instructed to condense the content into a brief, one-sentence summary.\n",
    "2. The `story_chain` function:\n",
    "   - First step (story generation): The `story_prompt` is invoked with the provided `genre`, and the model generates a short story. This is done using the `invoke` method, where the genre is passed as input to the prompt. The `| llm` syntax is used to chain the prompt template to the language model. The model processes the prompt and returns a generated response, which is accessed via the `.content` attribute.\n",
    "   - Second Step (summary generation): The `story` generated in the first step is passed into the `summary_prompt` as input, and the model then produces a one-sentence summary of that story.\n",
    "\n",
    "By using prompt chaining in this manner, we can achieve more structured and logical AI outputs, where each step builds on the previous one, making the results more coherent and purposeful.\n",
    "\n",
    "By starting with a simple task (story generation and summarization), we can expand this technique to more complex workflows, where multiple steps are required to solve a problem or generate sophisticated content.\n",
    "\n",
    "\n",
    "## Sequential prompting\n",
    "In this section, we will explore sequential prompting, a more complex approach where multiple prompts are used in a series to analyze a given text in stages. We will break down the analysis into three specific tasks: identifying the main theme, determining the tone, and extracting the key takeaways. Each step builds upon the previous one, ensuring that the output is well-structured and coherent.\n",
    "\n",
    "By chaining these prompts, we guide the model through a multi-step process that enables deeper and more structured insights into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KPSb0HA6Zyxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: The main theme of the text is the dual nature of artificial intelligence advancements, highlighting both their potential benefits and associated ethical concerns. It emphasizes the need for careful and responsible development to maximize positive outcomes while mitigating risks.\n",
      "\n",
      "Tone: The overall tone of the text is balanced and thoughtful. It expresses a mixture of excitement and concern regarding the advancements in artificial intelligence. The language conveys optimism about the potential benefits of AI, while also highlighting the need for caution and ethical considerations. This duality reflects a responsible and measured perspective on a rapidly evolving technology.\n",
      "\n",
      "Takeaways: Here are the key takeaways from the provided text:\n",
      "\n",
      "1. **Dual Nature of AI Advancements**: The text emphasizes that artificial intelligence has both significant potential benefits and serious ethical concerns.\n",
      "\n",
      "2. **Potential Benefits**: AI is positioned as a transformative technology that can revolutionize various industries and enhance daily life.\n",
      "\n",
      "3. **Ethical Concerns**: The advancements in AI raise important ethical issues, including concerns about privacy, job displacement, and the risk of misuse.\n",
      "\n",
      "4. **Need for Caution**: It is crucial to approach the development of AI with careful consideration and foresight to ensure that positive outcomes are prioritized.\n",
      "\n",
      "5. **Balanced Perspective**: The tone reflects a mix of excitement about AI's possibilities and a responsibility to address its associated risks, advocating for a thoughtful and measured approach to its implementation. \n",
      "\n",
      "6. **Maximizing Benefits and Minimizing Risks**: The overarching message is the importance of maximizing the benefits of AI while actively working to mitigate its risks through responsible development practices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates for each step of the analysis\n",
    "theme_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # Takes the full text as input\n",
    "    template=\"Identify the main theme of the following text:\\n{text}\"  # Instructions to identify the theme\n",
    ")\n",
    "\n",
    "tone_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # Takes the full text as input\n",
    "    template=\"Describe the overall tone of the following text:\\n{text}\"  # Instructions to identify the tone\n",
    ")\n",
    "\n",
    "takeaway_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"theme\", \"tone\"],  # Takes the text, theme, and tone as input\n",
    "    template=\"Given the following text with the main theme '{theme}' and tone '{tone}', what are the key takeaways?\\n{text}\"  # Instructions to extract key takeaways\n",
    ")\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"Perform a multi-step analysis of a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the theme, tone, and key takeaways of the text.\n",
    "    \"\"\"\n",
    "    # Step 1: Identify the theme of the text\n",
    "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
    "\n",
    "    # Step 2: Identify the tone of the text\n",
    "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
    "\n",
    "    # Step 3: Extract the key takeaways based on the identified theme and tone\n",
    "    takeaways = (takeaway_prompt | llm).invoke({\"text\": text, \"theme\": theme, \"tone\": tone}).content\n",
    "\n",
    "    # Return a dictionary containing all three pieces of information\n",
    "    return {\"theme\": theme, \"tone\": tone, \"takeaways\": takeaways}\n",
    "\n",
    "# Test the sequential prompting\n",
    "sample_text = \"The rapid advancement of artificial intelligence has sparked both excitement and concern among experts. While AI promises to revolutionize industries and improve our daily lives, it also raises ethical questions about privacy, job displacement, and the potential for misuse. As we stand on the brink of this technological revolution, it's crucial that we approach AI development with caution and foresight, ensuring that its benefits are maximized while its risks are minimized.\"\n",
    "\n",
    "analysis = analyze_text(sample_text)  # Perform the analysis on the sample text\n",
    "# Print the analysis results\n",
    "for key, value in analysis.items():\n",
    "    print(f\"{key.capitalize()}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgT8yk94kHLX"
   },
   "source": [
    "1. Prompt templates:\n",
    "   - `theme_prompt`: This template asks the model to identify the main theme of a given text. The input is the entire text, and the output should be a summary of the central idea or theme.\n",
    "   - `tone_prompt`: This template requests the model to describe the tone of a text. The tone can be classified as positive, negative, neutral, formal, informal, etc.\n",
    "   - `takeaway_prompt`: This is the final prompt, which uses theme and tone along with a text to extract the key takeaways or conclusions.\n",
    "2. The `analyze_text` function:\n",
    "   - Step 1 (theme identification): The function first invokes the `theme_prompt` to extract the main theme of the text.\n",
    "   - Step 2 (tone analysis): Next, it invokes the `tone_prompt` to analyze the tone of the text.\n",
    "   - Step 3 (key takeaways): Using the theme and tone identified in the previous steps, the function invokes the `takeaway_prompt` to extract the key takeaways from the text.\n",
    "\n",
    "In this example, sequential prompting was used to break down a complex analysis task into a series of manageable steps. Each prompt builds on the information generated by the previous one, allowing for a detailed and structured analysis of the text. This technique is especially useful when dealing with tasks that require multiple stages of reasoning or decision-making.\n",
    "\n",
    "## Dynamic prompt generation\n",
    "In this section, we will build a dynamic question-answering system that generates follow-up questions based on previous answers. This approach allows the system to engage in a continuous dialogue where each new question depends on the previous response, making the interaction more natural and context-aware.\n",
    "- The system begins with an initial question.\n",
    "- For each subsequent step, it generates an answer to the current question, then uses that answer to generate a follow-up question.\n",
    "- The follow-up question is dynamically created based on the answer to the previous question, allowing the conversation to evolve in a coherent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O_ypp4OsZyYV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are the potential applications of quantum computing?\n",
      "A1: Quantum computing has potential applications in various fields, including:\n",
      "\n",
      "1. **Cryptography**: Enhancing security through quantum key distribution and breaking classical encryption methods.\n",
      "2. **Drug discovery**: Simulating molecular interactions to accelerate the development of new pharmaceuticals.\n",
      "3. **Optimization problems**: Solving complex optimization tasks in logistics, finance, and operations research more efficiently.\n",
      "4. **Machine learning**: Improving data analysis and pattern recognition through quantum-enhanced algorithms.\n",
      "5. **Material science**: Discovering new materials with desired properties by simulating atomic structures.\n",
      "6. **Climate modeling**: Analyzing large datasets for more accurate climate predictions and simulations.\n",
      "7. **Artificial intelligence**: Enhancing algorithms for faster training and improved decision-making.\n",
      "\n",
      "These applications leverage quantum mechanics principles, such as superposition and entanglement, to perform computations beyond the capabilities of classical computers.\n",
      "\n",
      "Q2: What are the challenges and limitations currently facing the development and implementation of quantum computing technologies?\n",
      "A2: The challenges and limitations currently facing quantum computing technologies include:\n",
      "\n",
      "1. **Technical Complexity**: Quantum systems are difficult to build and maintain, requiring precise control over qubits and their interactions.\n",
      "2. **Error Rates and Decoherence**: Qubits are prone to errors due to decoherence and noise, making error correction essential yet challenging.\n",
      "3. **Scalability**: Scaling quantum systems to a large number of qubits while maintaining performance and coherence is a significant hurdle.\n",
      "4. **Material Limitations**: Finding suitable materials and technologies for reliable qubit implementation remains an ongoing challenge.\n",
      "5. **Algorithm Development**: Many quantum algorithms are still in development, and practical applications for quantum superiority are limited.\n",
      "6. **Economic and Resource Barriers**: High costs associated with research, development, and infrastructure can hinder widespread adoption.\n",
      "7. **Integration with Classical Systems**: Creating effective interfaces between quantum and classical computing systems poses additional complexity.\n",
      "8. **Talent Shortage**: There is a shortage of skilled professionals with expertise in both quantum physics and computer science.\n",
      "\n",
      "These factors collectively slow the advancement and application of quantum computing technologies in practical scenarios.\n",
      "\n",
      "Q3: What strategies or approaches are being explored to overcome the challenges and limitations in quantum computing technologies?\n",
      "A3: Several strategies are being explored to overcome the challenges in quantum computing technologies, including:\n",
      "\n",
      "1. **Error Correction**: Developing quantum error correction codes to mitigate errors caused by decoherence and noise.\n",
      "\n",
      "2. **Improved Qubit Designs**: Researching more stable and scalable qubit implementations, such as topological qubits and superconducting qubits.\n",
      "\n",
      "3. **Hybrid Quantum-Classical Algorithms**: Combining classical computing techniques with quantum algorithms to optimize performance and solve practical problems.\n",
      "\n",
      "4. **Quantum Supremacy Demonstrations**: Conducting experiments to showcase quantum advantage in specific tasks, thereby driving interest and funding.\n",
      "\n",
      "5. **Scalable Architectures**: Designing architectures that allow for the integration of a large number of qubits and facilitating easier connectivity.\n",
      "\n",
      "6. **Materials Science Innovations**: Exploring new materials and fabrication techniques to enhance qubit performance and coherence times.\n",
      "\n",
      "7. **Interdisciplinary Collaboration**: Fostering partnerships among academia, industry, and government to accelerate research and development.\n",
      "\n",
      "8. **Software Development**: Advancing quantum programming languages and simulation tools to simplify the development of quantum applications.\n",
      "\n",
      "These approaches aim to address various technical challenges, enhance performance, and facilitate the practical deployment of quantum computing technologies.\n",
      "\n",
      "Q4: What are some of the specific challenges associated with quantum error correction, and how are researchers addressing these challenges?\n",
      "A4: Quantum error correction faces several specific challenges, including:\n",
      "\n",
      "1. **Fragility of Quantum States**: Quantum states are highly sensitive to decoherence and noise, making it difficult to maintain their integrity for error correction.\n",
      "\n",
      "2. **Resource Intensiveness**: Quantum error correction requires a significant overhead in qubits; for every logical qubit, many physical qubits may be needed to encode and protect it.\n",
      "\n",
      "3. **Complexity of Error Models**: Different types of errors (e.g., bit-flip, phase-flip) must be accurately modeled and corrected, complicating the design of error-correcting codes.\n",
      "\n",
      "4. **Threshold Theorem**: The effectiveness of error correction depends on maintaining a certain error rate below a threshold, which is challenging with current technology.\n",
      "\n",
      "Researchers are addressing these challenges by:\n",
      "\n",
      "- **Developing Advanced Codes**: New quantum error-correcting codes, such as surface codes and cat codes, are being designed to enhance fault tolerance and reduce resource requirements.\n",
      "\n",
      "- **Improving Qubit Quality**: Advances in qubit technology (e.g., superconducting qubits, topological qubits) aim to reduce error rates and improve coherence times.\n",
      "\n",
      "- **Hybrid Approaches**: Combining classical and quantum error correction techniques to leverage the strengths of both paradigms.\n",
      "\n",
      "- **Scalable Architectures**: Designing scalable quantum computing architectures that can manage the overhead of error correction more efficiently.\n",
      "\n",
      "- **Experimental Validation**: Conducting experiments to test and refine error correction methods in real quantum systems to ensure practical viability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "# This template generates an answer to the question\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],  # Takes a question as input\n",
    "    template=\"Answer the following question concisely:\\n{question}\"  # Instruction for generating a concise answer\n",
    ")\n",
    "\n",
    "# This template generates a follow-up question based on the previous question and its answer\n",
    "follow_up_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],  # Takes both the previous question and answer as input\n",
    "    template=\"Based on the question '{question}' and the answer '{answer}', generate a relevant follow-up question.\"  # Instruction to generate a relevant follow-up question\n",
    ")\n",
    "\n",
    "def dynamic_qa(initial_question, num_follow_ups=3):\n",
    "    \"\"\"Conduct a dynamic Q&A session with follow-up questions.\n",
    "\n",
    "    Args:\n",
    "        initial_question (str): The initial question to start the Q&A session.\n",
    "        num_follow_ups (int): The number of follow-up questions to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing questions and answers.\n",
    "    \"\"\"\n",
    "    qa_chain = []  # Initialize an empty list to store the question-answer pairs\n",
    "    current_question = initial_question  # Set the initial question\n",
    "\n",
    "    # Loop for generating the initial question and the required number of follow-up questions\n",
    "    for _ in range(num_follow_ups + 1):  # +1 for the initial question\n",
    "        # Get the answer to the current question\n",
    "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
    "        # Add the current question and its answer to the chain\n",
    "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
    "\n",
    "        if _ < num_follow_ups:  # Generate follow-up for all but the last iteration\n",
    "            # Generate the next question based on the current question and answer\n",
    "            current_question = (follow_up_prompt | llm).invoke({\"question\": current_question, \"answer\": answer}).content\n",
    "\n",
    "    return qa_chain  # Return the entire list of question-answer pairs\n",
    "\n",
    "# Test the dynamic Q&A system with an initial question\n",
    "initial_question = \"What are the potential applications of quantum computing?\"\n",
    "qa_session = dynamic_qa(initial_question)  # Conduct the dynamic Q&A session\n",
    "\n",
    "# Print the Q&A session\n",
    "for i, qa in enumerate(qa_session):\n",
    "    print(f\"Q{i+1}: {qa['question']}\")\n",
    "    print(f\"A{i+1}: {qa['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XvXlUotusj4"
   },
   "source": [
    "1. Prompt templates:\n",
    "   - `answer_prompt`: This template generates a concise answer to any question. It simply requires the input of a question, and the model responds with a relevant answer.\n",
    "   - `follow_up_prompt`: This template generates a follow-up question based on the current question and its associated answer. It takes both the question and the answer as input and generates a follow-up question to continue the conversation.\n",
    "2. The `dynamic_qa` function: The function takes an initial question to start the Q&A session, and an optional number of follow-up questions (default is 3).\n",
    "     - The function starts with the initial question and generates an answer using the `answer_prompt` template.\n",
    "     - It then stores the question-answer pair in a list (`qa_chain`).\n",
    "     - For each subsequent iteration, the function uses the previous question and answer to generate a follow-up question using the `follow_up_prompt` template.\n",
    "     - The process repeats until the desired number of follow-up questions has been generated.\n",
    "\n",
    "The system is capable of maintaining a conversation by generating follow-up questions based on previous answers. The ability to dynamically generate questions makes the system more interactive and capable of engaging in deeper dialogues. The process uses prompt chaining, where the output of one step (the answer) is passed as input to the next (the follow-up question). This ensures that the generated content is contextually relevant.\n",
    "\n",
    "\n",
    "## Error handling and validation\n",
    "In this section, we focus on error handling and validation techniques to make our prompt chains more robust and reliable. When working with AI models, it is essential to ensure that the generated outputs are both correct and relevant. We will incorporate multiple steps to handle potential errors, validate the model’s responses, and retry the generation process if necessary.\n",
    "- Error handling: Ensures that the system continues to function properly even if something goes wrong during the prompt generation process.\n",
    "- Validation: Confirms that the generated output meets the required conditions (e.g., relevance to the topic).\n",
    "\n",
    "The process involves generating a 4-digit number related to a specific topic, validating that the number is correct, and retrying if it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7nOFYKdRZx_z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result for topic 'World War II': 1945\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "# Template to generate a 4-digit number related to a specific topic\n",
    "generate_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],  # Takes the topic as input\n",
    "    template=\"Generate a 4-digit number related to the topic: {topic}. Respond with ONLY the number, no additional text.\"\n",
    ")\n",
    "\n",
    "# Template to validate if the generated number is relevant to the topic\n",
    "validate_prompt = PromptTemplate(\n",
    "    input_variables=[\"number\", \"topic\"],  # Takes the generated number and topic as input\n",
    "    template=\"Is the number {number} truly related to the topic '{topic}'? Answer with 'Yes' or 'No' and explain why.\"\n",
    ")\n",
    "\n",
    "# Function to extract a 4-digit number from a string\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract a 4-digit number from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to extract the number from.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted 4-digit number, or None if no valid number is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\b\\d{4}\\b', text)  # Search for a 4-digit number in the text\n",
    "    return match.group() if match else None  # Return the number if found, else return None\n",
    "\n",
    "# Function to generate a number and validate its relevance\n",
    "def robust_number_generation(topic, max_attempts=3):\n",
    "    \"\"\"Generate a topic-related number with validation and error handling.\n",
    "\n",
    "    Args:\n",
    "        topic (str): The topic to generate a number for.\n",
    "        max_attempts (int): Maximum number of generation attempts.\n",
    "\n",
    "    Returns:\n",
    "        str: A validated 4-digit number or an error message.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):  # Retry up to max_attempts if validation fails\n",
    "        try:\n",
    "            # Generate a 4-digit number related to the topic\n",
    "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
    "            # Extract the 4-digit number from the response\n",
    "            number = extract_number(response)\n",
    "\n",
    "            # If no valid number is found, raise an exception\n",
    "            if not number:\n",
    "                raise ValueError(f\"Failed to extract a 4-digit number from the response: {response}\")\n",
    "\n",
    "            # Validate the relevance of the number\n",
    "            validation = (validate_prompt | llm).invoke({\"number\": number, \"topic\": topic}).content\n",
    "            if validation.lower().startswith(\"yes\"):  # Check if the validation was successful\n",
    "                return number\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Number {number} was not validated. Reason: {validation}\")\n",
    "        except Exception as e:\n",
    "            # Print error message if an exception occurs\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "\n",
    "    return \"Failed to generate a valid number after multiple attempts.\"  # Return failure message if all attempts fail\n",
    "\n",
    "# Test the robust number generation\n",
    "topic = \"World War II\"\n",
    "result = robust_number_generation(topic)\n",
    "# Print the final result (either a valid number or an error message)\n",
    "print(f\"Final result for topic '{topic}': {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1AoVNB-ZxvA"
   },
   "source": [
    "1. Prompt templates:\n",
    "   - `generate_prompt`: This template prompts the model to generate a 4-digit number that is related to a given topic. The model is instructed to provide only the number, with no additional text.\n",
    "   - `validate_prompt`: This template asks the model to validate whether a number is truly related to the given topic. The model responds with either \"Yes\" or \"No,\" along with an explanation.\n",
    "2. `extract_number` function: This function uses a regular expression (`re.search`) to extract a 4-digit number from a string. If no number is found, it returns `None`. The function searches for a 4-digit number using the regular expression pattern `\\b\\d{4}\\b`, which matches exactly four digits surrounded by word boundaries.\n",
    "3. `robust_number_generation` function: Takes the topic for which a 4-digit number is to be generated, and the optional max_attempts parameter that specifies how many times the process will retry in case of failure.\n",
    "    1. The function attempts to generate the number for the given topic by invoking the `generate_prompt` template. The response is processed to extract the number.\n",
    "    2. If the number is invalid (i.e., not found or incorrect), an exception is raised.\n",
    "    3. If a valid number is generated, it is validated by invoking the `validate_prompt` template to check if the number is actually related to the topic.\n",
    "    4. If validation fails, the process retries up to the maximum number of attempts.\n",
    "    5. If a valid number is generated and validated, it is returned. Otherwise, after the maximum number of attempts, the function returns a failure message.\n",
    "\n",
    "\n",
    "The error handling and validation mechanism ensures that the system can gracefully handle failures and retry operations if necessary. By incorporating validation at each step and handling errors effectively, the system becomes more reliable and robust. This approach is essential when building AI applications that interact with unpredictable models, ensuring that the generated outputs are both correct and useful.\n",
    "- Error handling: The function is wrapped in a `try-except` block to catch any potential errors (e.g., network errors, invalid number extraction). If an error occurs during number extraction or validation, it retries the process for up to `max_attempts`.\n",
    "- Validation: After generating a number, the system validates its relevance to the given topic using a follow-up question (`validate_prompt`). If the validation fails, the process retries the generation and validation steps."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
