{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q2gjqg9lEuI"
   },
   "source": [
    "# Negative prompting and avoiding undesired outputs\n",
    "\n",
    "This notebook demonstrates how to guide and constrain model output using negative prompting, a technique that involves explicitly stating what the model should avoid doing. When working with LLMs, we often think about what should be included in a response—but it's just as important to think about what shouldn't. Unwanted content can include off-topic information, overly technical language, inappropriate analogies, or even subtle biases. Our goal is to influence the tone and content of LLM responses by excluding certain behaviors or topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8-Ij-zI2lDyp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WboBKw_PmKwJ"
   },
   "source": [
    "### Initialize the language model\n",
    "We instantiate a lightweight GPT model from OpenAI using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1GjSxG4EmKb-"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhUgZIHJmKPb"
   },
   "source": [
    "## Example 1: Negative prompting with instructional constraints\n",
    "We begin with a simple scenario: instructing the model to explain a topic without including certain types of information. This demonstrates how we can limit output style and content through structured language in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T0WPLLIVmKEM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which plants, algae, and some bacteria make their own food using sunlight. They take in carbon dioxide from the air and water from the soil. Using sunlight, they transform these ingredients into sugar, which serves as food, and oxygen, which they release into the air. This process is essential for life on Earth, as it provides energy for the plants and oxygen for other living beings.\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template with explicit exclusions (negative instructions)\n",
    "negative_example_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"\"\"Provide a brief explanation of {topic}.\n",
    "    Do NOT include any of the following in your explanation:\n",
    "    - Technical jargon or complex terminology\n",
    "    - Historical background or dates\n",
    "    - Comparisons to other related topics\n",
    "    Your explanation should be simple, direct, and focus only on the core concept.\"\"\"\n",
    ")\n",
    "\n",
    "# Invoke the prompt with a specific topic\n",
    "response = llm.invoke(negative_example_prompt.format(topic=\"photosynthesis\")).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9zY7Ai9mJ4X"
   },
   "source": [
    "We define a `PromptTemplate` that makes it clear what should be omitted from the model's output. Then we pass the formatted prompt to the model and retrieve the content. This showcases how natural language can be structured to act as a control mechanism for LLM outputs.\n",
    "\n",
    "Here, we are prompting the model to intentionally avoid certain response patterns. The instructions help prevent over-explaining or veering off-topic.\n",
    "\n",
    "## Example 2: Explicit dynamic exclusions\n",
    "Here, we introduce variable-driven exclusions — where the content to be excluded is passed dynamically. This is useful when the same structural prompt is reused across different tasks or user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2uFs6wIsmJsR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise offers a myriad of benefits that extend beyond physical appearance, enhancing overall well-being. Engaging in regular physical activity boosts cardiovascular health, strengthens muscles, and improves flexibility, contributing to greater functional ability in daily tasks. Additionally, exercise is known to elevate mood and reduce symptoms of anxiety and depression by releasing endorphins, the body's natural feel-good chemicals. It also promotes better sleep quality and cognitive function, fostering sharper focus and creativity. Overall, incorporating exercise into daily life cultivates a sense of vitality and resilience, enriching both mental and physical health.\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template that accepts a custom exclusion term\n",
    "exclusion_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"exclude\"],\n",
    "    template=\"\"\"Write a short paragraph about {topic}.\n",
    "    Important: Do not mention or reference anything related to {exclude}.\"\"\"\n",
    ")\n",
    "\n",
    "# Use the prompt to request a response about exercise while excluding body image\n",
    "response = llm.invoke(exclusion_prompt.format(\n",
    "    topic=\"the benefits of exercise\",\n",
    "    exclude=\"weight loss or body image\"\n",
    ")).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLLjUQcVmJgB"
   },
   "source": [
    "We are dynamically injecting the term to exclude (`exclude`) into the prompt structure. This allows us to reuse the same prompt logic across multiple applications while adapting the filtering criteria based on context or user input. In practice, this is useful when filtering outputs based on audience sensitivity, domain-specific restrictions, or platform policy (e.g., health disclaimers).\n",
    "\n",
    "## Example 3: Structured constraints on style and content\n",
    "In this section, we apply more rigorous and structured constraints using a formatted prompt. This includes avoiding specific words, limiting word count, preventing figurative language, and enforcing factual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y0mpAC-2mJTA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) refers to the development of algorithms and computational models that enable machines to perform tasks typically requiring cognitive functions. This includes data analysis, pattern recognition, decision-making, and learning from experience through techniques such as machine learning and deep learning. AI systems utilize large datasets to improve their accuracy and efficiency. They can operate in various domains, including natural language processing, computer vision, and autonomous systems, facilitating automation and enhancing problem-solving capabilities across industries. AI technologies are continuously evolving, driven by advancements in computational power and algorithmic research.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template with multiple constraints and stylistic rules\n",
    "constraint_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"style\", \"excluded_words\"],\n",
    "    template=\"\"\"Write a {style} description of {topic}.\n",
    "    Constraints:\n",
    "    1. Do not use any of these words: {excluded_words}\n",
    "    2. Keep the description under 100 words\n",
    "    3. Do not use analogies or metaphors\n",
    "    4. Focus only on factual information\"\"\"\n",
    ")\n",
    "\n",
    "# Provide topic and constraints for the LLM\n",
    "response = llm.invoke(constraint_prompt.format(\n",
    "    topic=\"artificial intelligence\",\n",
    "    style=\"technical\",\n",
    "    excluded_words=\"robot, human-like, science fiction\"\n",
    ")).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsCe7cvSmJFD"
   },
   "source": [
    "We are layering multiple prompt-level instructions to limit creativity and enforce factual tone. This approach ensures that generated content stays within desired constraints, useful for academic, technical, or policy-driven environments.\n",
    "\n",
    "\n",
    "## Evaluation: verifying adherence to constraints\n",
    "Now that we have defined rules for our outputs, we need a mechanism to evaluate whether the model actually followed them. This function allows us to define constraint checks and evaluate any given output against them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gN9y-9qy-VQh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'word_count': True, 'no_excluded_words': True, 'no_analogies': False}\n"
     ]
    }
   ],
   "source": [
    "# Define a function that applies evaluation checks to a model response\n",
    "def evaluate_output(output, constraints):\n",
    "    \"\"\"Evaluate if the output meets the given constraints.\"\"\"\n",
    "    results = {}\n",
    "    for constraint, check_func in constraints.items():\n",
    "        results[constraint] = check_func(output)\n",
    "    return results\n",
    "\n",
    "# Define some example constraints\n",
    "constraints = {\n",
    "    \"word_count\": lambda x: len(x.split()) <= 100,\n",
    "    \"no_excluded_words\": lambda x: all(word not in x.lower() for word in [\"robot\", \"human-like\", \"science fiction\"]),\n",
    "    \"no_analogies\": lambda x: not re.search(r\"\\b(as|like)\\b\", x, re.IGNORECASE)\n",
    "\n",
    "}\n",
    "\n",
    "# Apply the evaluation logic to the previous response\n",
    "evaluation_results = evaluate_output(response, constraints)\n",
    "print(\"Evaluation results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3Lzg8GI-ZOu"
   },
   "source": [
    "Each constraint is implemented as a Python lambda function and evaluated against the output. This enables a rule-based QA pipeline where we can validate and score LLM responses — particularly useful in production or compliance-sensitive workflows.\n",
    "\n",
    "### Refining the prompt based on evaluation results\n",
    "If any constraint fails, we can make the prompt more specific and reattempt generation. This feedback loop helps iteratively improve the quality and consistency of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1KQR9X8hmIwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refined response:\n",
      " Artificial intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require cognitive functions. These tasks include problem-solving, pattern recognition, natural language processing, and decision-making. AI employs algorithms, machine learning, and deep learning techniques to analyze data, adapt to new information, and improve performance over time. Applications span various domains, including healthcare, finance, transportation, and entertainment, facilitating automation and enhancing efficiency. AI systems can operate within defined parameters or learn from vast datasets to generate insights and predictions.\n",
      "\n",
      "Refined evaluation results: {'word_count': True, 'no_excluded_words': True, 'no_analogies': True}\n"
     ]
    }
   ],
   "source": [
    "# If the original output fails validation, refine the prompt with stronger exclusions\n",
    "if not all(evaluation_results.values()):\n",
    "    refined_prompt = constraint_prompt.format(\n",
    "        topic=\"artificial intelligence\",\n",
    "        style=\"technical and concise\",  # Added 'concise' to address word count\n",
    "        excluded_words=\"robot, human-like, science fiction, like, as\"  # Added 'like' and 'as' to avoid analogies and that led to failures\n",
    "    )\n",
    "    # Get a new response with refined prompt\n",
    "    refined_response = llm.invoke(refined_prompt).content\n",
    "    print(\"\\nRefined response:\\n\", refined_response)\n",
    "\n",
    "    # Evaluate the refined output\n",
    "    refined_evaluation = evaluate_output(refined_response, constraints)\n",
    "    print(\"\\nRefined evaluation results:\", refined_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV4UsxRemqM8"
   },
   "source": [
    "We close the feedback loop between model generation and evaluation. If the response violates rules (e.g., includes analogies or banned terms), we adapt the prompt accordingly—demonstrating how LLM interaction can be iterative and self-correcting."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
