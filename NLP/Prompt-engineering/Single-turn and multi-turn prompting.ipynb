{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02256d6-f918-4b68-acf5-538c42c72590",
   "metadata": {},
   "source": [
    "# Single-turn and multi-turn prompting\n",
    "\n",
    "In this notebook, we explore two foundational ways of interacting with LLMs:\n",
    "- Single-turn prompts — isolated interactions with the model.\n",
    "- Multi-turn prompts — ongoing conversations where the model maintains context between turns (remember previous inputs).\n",
    "\n",
    "Understanding and choosing the right prompt structure can significantly impact the quality and relevance of the model’s responses. Single-turn prompts are great for quick tasks, while multi-turn prompts unlock more natural, human-like interactions. We will use OpenAI’s GPT via LangChain, a framework that helps structure LLM workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aa6954-84ac-4249-be29-ee7c5391b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7bd9f-d059-4d53-b86e-ce7db136fe2f",
   "metadata": {},
   "source": [
    "#### Initialize the language model\n",
    "We initialize an instance of OpenAI's GPT-4o-mini model. This model is lightweight and fast, making it suitable for interactive use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd6a078-8ba8-40c8-b3a8-19acdc650fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ec55b-f13c-4dd5-aeac-cc9e13f0cff1",
   "metadata": {},
   "source": [
    "## Single-turn prompts\n",
    "Single-turn prompts are direct, one-time interactions with the model. The model receives an input, generates a response, and discards the context after responding. This type of prompt is ideal for simple questions, fact retrieval, or isolated commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0828a3c7-05a7-4242-a6fd-a8a86cedf98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three primary colors are red, blue, and yellow. These colors cannot be created by mixing other colors and can be combined in various ways to create a wide range of other colors. In additive color mixing (like in light), the primary colors are red, green, and blue (RGB).\n"
     ]
    }
   ],
   "source": [
    "# A basic one-shot prompt (no context from previous messages)\n",
    "single_turn_prompt = \"What are the three primary colors?\"\n",
    "\n",
    "# Ask the LLM and print the response\n",
    "print(llm.invoke(single_turn_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea6614-382b-4e5f-8a8b-e7873fffeb66",
   "metadata": {},
   "source": [
    "- We send the question directly to the language model (`llm.invoke(...)`).\n",
    "- The `.content` extracts the text part of the response.\n",
    "The model returns a response based only on this input — it has no memory of anything before or after.\n",
    "\n",
    "#### Using prompt templates\n",
    "To make our prompts more reusable and structured, we can define prompt templates. This lets us build prompts dynamically by filling in placeholders with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7c2ff0-af5a-42e1-bac8-d490bf64d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color theory is a conceptual framework used to understand how colors interact, how they can be combined, and the effects they can have on perception and emotions. It encompasses the principles of color mixing, the relationships between colors, and the psychological implications of color use in design and art.\n",
      "\n",
      "The three main components of color theory are:\n",
      "\n",
      "1. **Hue**: This refers to the actual color itself, such as red, blue, or yellow. Hue is the attribute of a color that allows it to be classified in the color spectrum.\n",
      "\n",
      "2. **Saturation**: Also known as intensity, saturation describes the purity or vividness of a color. A highly saturated color appears bright and rich, while a desaturated color appears more muted or grayish.\n",
      "\n",
      "3. **Value**: This component refers to the lightness or darkness of a color. Value is determined by the amount of light reflected off the surface of an object, affecting how we perceive its color.\n",
      "\n",
      "Together, these components help create a comprehensive understanding of how colors can be used and perceived in various contexts.\n"
     ]
    }
   ],
   "source": [
    "# Define a reusable prompt template with a variable {topic}\n",
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Provide a brief explanation of {topic} and list its three main components.\"\n",
    ")\n",
    "\n",
    "# Combine the template and the model into a prompt chain\n",
    "chain = structured_prompt | llm\n",
    "# Use the prompt chain by passing a topic\n",
    "print(chain.invoke({\"topic\": \"color theory\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500ecd5-3aad-4109-b715-0fc89808ef54",
   "metadata": {},
   "source": [
    "- The `PromptTemplate` is a LangChain class that lets us define a generic and reusable prompt.\n",
    "- The `{topic}` is a placeholder — a dynamic input we will fill at runtime.\n",
    "- We then \"pipe\" (`|`) the prompt template into the language model using LangChain's composability — this creates a prompt chain that takes input → fills the template → sends it to the model.\n",
    "- `chain.invoke({\"topic\": \"color theory\"})` fills in the {topic} with \"color theory\" at runtime and runs the full prompt.\n",
    "- The result is passed to the LLM, which generates a structured response.\n",
    "\n",
    "This is especially useful when we need to apply the same prompt logic across many topics.\n",
    "\n",
    "## Multi-turn prompts (Conversations)\n",
    "Multi-turn prompts simulate a natural conversation with the LLM, where memory is retained between turns. This allows the model to refer back to previous messages and maintain context. To implement this, we use LangChain's `RunnableWithMessageHistory` — which wraps a model and adds memory capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba57963-6e6e-433c-aa79-9e5c8f184f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Hi, I'm learning about space. Can you tell me about planets?\n",
      "A: Of course! Planets are celestial bodies that orbit stars, and in our solar system, they orbit the Sun. There are eight recognized planets in our solar system, which can be divided into two main categories: terrestrial planets and gas giants.\n",
      "\n",
      "### Terrestrial Planets:\n",
      "1. **Mercury**: The closest planet to the Sun, Mercury is a small, rocky planet with a thin atmosphere. It has extreme temperature variations and is heavily cratered, similar to our Moon.\n",
      "\n",
      "2. **Venus**: Often called Earth's \"sister planet\" due to its similar size and composition, Venus has a thick atmosphere composed mainly of carbon dioxide, leading to a strong greenhouse effect and very high surface temperatures.\n",
      "\n",
      "3. **Earth**: The only known planet to support life, Earth has a diverse environment with liquid water, a protective atmosphere, and a magnetic field. Its surface is 71% water and has a variety of ecosystems.\n",
      "\n",
      "4. **Mars**: Known as the \"Red Planet\" due to iron oxide (rust) on its surface, Mars has the largest volcano and canyon in the solar system. It has seasons, polar ice caps, and evidence of past water flow.\n",
      "\n",
      "### Gas Giants:\n",
      "5. **Jupiter**: The largest planet in our solar system, Jupiter is a gas giant primarily made of hydrogen and helium. It has a strong magnetic field, dozens of moons, and a prominent Great Red Spot, which is a giant storm.\n",
      "\n",
      "6. **Saturn**: Famous for its spectacular ring system, Saturn is primarily composed of hydrogen and helium. It has numerous moons, including Titan, which has a thick atmosphere and is of great interest for study.\n",
      "\n",
      "### Ice Giants:\n",
      "7. **Uranus**: An ice giant with a unique tilt that causes extreme seasonal variations, Uranus is mostly made of water, ammonia, and methane ice. It has a faint ring system and is known for its blue color due to methane in its atmosphere.\n",
      "\n",
      "8. **Neptune**: The farthest planet from the Sun, Neptune is similar in composition to Uranus and is known for its deep blue color and strong winds, which can reach speeds of over 1,500 miles per hour (2,400 kilometers per hour). It also has a faint ring system and several moons.\n",
      "\n",
      "### Dwarf Planets:\n",
      "In addition to these eight planets, there are also dwarf planets like Pluto, Eris, Haumea, and Makemake, which share similar characteristics with the planets but do not clear their orbital paths of other debris.\n",
      "\n",
      "### Exoplanets:\n",
      "Beyond our solar system, astronomers have discovered thousands of exoplanets—planets orbiting other stars. These exoplanets vary widely in size, composition, and distance from their stars, and they contribute to our understanding of planetary systems in the universe.\n",
      "\n",
      "If you have more specific questions about any of these planets or related topics, feel free to ask!\n",
      "\n",
      "Q: What's the largest planet in our solar system?\n",
      "A: The largest planet in our solar system is **Jupiter**. It is a gas giant with a diameter of about 86,881 miles (139,822 kilometers), making it more than 11 times wider than Earth. Jupiter is primarily composed of hydrogen and helium and has a very strong magnetic field, numerous moons (over 79 confirmed), and a prominent feature known as the Great Red Spot, which is a giant storm larger than Earth that has been raging for hundreds of years. Jupiter's immense size and mass (it is about 318 times more massive than Earth) significantly influence the dynamics of our solar system.\n",
      "\n",
      "Q: How does its size compare to Earth?\n",
      "A: Jupiter is significantly larger than Earth in several ways:\n",
      "\n",
      "1. **Diameter**: Jupiter has a diameter of about 86,881 miles (139,822 kilometers), while Earth has a diameter of about 7,917 miles (12,742 kilometers). This means Jupiter's diameter is more than 11 times that of Earth.\n",
      "\n",
      "2. **Volume**: The volume of Jupiter is immense. It can fit about 1,300 Earths inside it. In terms of volume, Jupiter is the largest planet in our solar system.\n",
      "\n",
      "3. **Mass**: Jupiter's mass is about 318 times that of Earth. This enormous mass contributes to its strong gravitational pull, which affects not only its own moons but also other bodies in the solar system.\n",
      "\n",
      "4. **Surface Area**: The surface area of Jupiter is about 121 times greater than that of Earth.\n",
      "\n",
      "In summary, Jupiter's size and mass are vastly greater than those of Earth, making it the giant of our solar system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Memory store to hold session histories (one for each unique session)\n",
    "store = {}\n",
    "\n",
    "# Function to retrieve or create a session-specific message history\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the LLM so it works with structured message history\n",
    "def message_chain(inputs):\n",
    "    # Combine memory (chat history) with current input\n",
    "    messages = inputs[\"chat_history\"] + [HumanMessage(content=inputs[\"input\"])]\n",
    "    return llm.invoke(messages)  # returns an AIMessage\n",
    "    \n",
    "# Convert our function into a runnable\n",
    "runnable = RunnableLambda(message_chain)\n",
    "\n",
    "# Create a memory-aware conversation chain\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    runnable=runnable,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",  # Where to find user input\n",
    "    history_messages_key=\"chat_history\"  # Where memory gets tracked\n",
    ")\n",
    "\n",
    "# Define a session ID to identify this conversation thread\n",
    "session_id = \"space_learner_001\"\n",
    "\n",
    "# Now we can send structured input\n",
    "# Define a helper function to interact with the model and print results cleanly\n",
    "def ask(question):\n",
    "    response = conversation.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    print(f\"Q: {question}\\nA: {response.content}\\n\")\n",
    "\n",
    "# Ask a series of related questions (multi-turn)\n",
    "ask(\"Hi, I'm learning about space. Can you tell me about planets?\")\n",
    "ask(\"What's the largest planet in our solar system?\")\n",
    "ask(\"How does its size compare to Earth?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00a7e0-9e55-482c-bb9c-8c0582d33b0c",
   "metadata": {},
   "source": [
    "- `store = {}` – Creating a memory store - This acts like a central storage for all session histories. Each unique conversation (like each user or chat session) gets its own memory. This setup makes it possible to keep multiple conversations separate and organized.\n",
    "- `get_session_history(session_id)` – Manage memory per session - This function ensures that every conversation session gets its own memory. If the session is new, it creates a fresh `InMemoryChatMessageHistory` object. If the session already exists, it retrieves the existing memory.\n",
    "- `message_chain(inputs)` – Combining input and memory - This function combines the conversation history (`chat_history`) with the current user input (`input`). It then sends both as a list to the language model (`llm.invoke(messages)`), which generates a response based on the full context of the conversation. This is the key part that makes the conversation flow naturally. By sending the entire conversation history along with the new input, the model can respond appropriately, understanding both what was said earlier and what’s being asked right now.\n",
    "    - Why? This ensures that each session has its own dedicated memory, allowing the model to recall past messages for that session. That is what lets the model “remember” what we have talked about.\n",
    "- `runnable = RunnableLambda(...)` – Wrap the LLM - The `RunnableWithMessageHistory` expects the model to take a dictionary-style input like `{\"input\": \"message\"}`. But the LLM we're using only expects list of `Message` objects. So, we wrap the LLM in a `RunnableLambda`, that ensures the model receives the input in the right format. This step adapts the LLM to work correctly with LangChain’s memory infrastructure by making sure the input format matches what LangChain expects.\n",
    "- `RunnableWithMessageHistory(...)` – Add memory to the model - This is where we combine everything: the language model, the memory system, and input/output formatting. This is the key step that enables multi-turn conversations. It makes the model remember what the user has already said and adapt its responses accordingly. We tell LangChain:\n",
    "    - How to run the model (`runnable`)\n",
    "    - How to manage memory (`get_session_history`)\n",
    "    - Where to find user input (`input_messages_key=\"input\"`)\n",
    "    - Where to store and retrieve conversation history (`history_messages_key=\"chat_history\"`)\n",
    "- `session_id = \"...\"` – Define conversation thread - Each user (or session) should have a unique ID. This is how the system knows which conversation history to use. Without a session ID, the model wouldn't know whether a message is part of a new conversation or a continuation of an old one.\n",
    "- `ask(...)` – Helper function to interact with the model - This is a utility function that sends input to the model in the correct format (`{\"input\": question}`), attaches the session ID in the config and prints the model's response nicely.\n",
    "\n",
    "Now we start asking questions. Each new message references previous ones thanks to the memory mechanism. For example, the third question, *\"How does its size compare to Earth?\"*, is understood in context — the model knows “its” refers to Jupiter, because that was discussed just before.\n",
    "\n",
    "## Comparing single-turn and multi-turn behavior\n",
    "Let’s run the same set of prompts in both modes to see how the responses differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838d1c6a-7461-442c-958e-877aae4d3d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-turn responses:\n",
      "Q: What is the capital of France?\n",
      "A: The capital of France is Paris.\n",
      "\n",
      "Q: What is its population?\n",
      "A: Could you please specify which location you are referring to when you ask about its population?\n",
      "\n",
      "Q: What is the city's most famous landmark?\n",
      "A: To provide an accurate answer, I would need to know which city you are referring to. Each city has its own unique landmarks that are considered famous. For example:\n",
      "\n",
      "- In Paris, the Eiffel Tower is a renowned landmark.\n",
      "- In New York City, the Statue of Liberty is iconic.\n",
      "- In London, the Big Ben and the Houses of Parliament are well-known.\n",
      "- In Sydney, the Sydney Opera House is a famous site.\n",
      "\n",
      "Please specify the city, and I can give you more information about its most famous landmark!\n",
      "\n",
      "\n",
      "Multi-turn responses:\n",
      "Q: What is the capital of France?\n",
      "A: The capital of France is Paris.\n",
      "\n",
      "Q: What is its population?\n",
      "A: As of my last update in October 2021, the population of Paris was estimated to be around 2.1 million people within the city limits. However, if you consider the wider metropolitan area, the population is approximately 12 million. For the most current population figures, it's best to check the latest statistics from a reliable source.\n",
      "\n",
      "Q: What is the city's most famous landmark?\n",
      "A: The most famous landmark in Paris is the Eiffel Tower. Completed in 1889 for the Exposition Universelle (World's Fair), it has since become a symbol of Paris and is one of the most recognizable structures in the world.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Single-turn prompts\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is its population?\",\n",
    "    \"What is the city's most famous landmark?\"\n",
    "]\n",
    "\n",
    "print(\"Single-turn responses:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {llm.invoke(prompt).content}\\n\")\n",
    "\n",
    "\n",
    "# Multi-turn prompts\n",
    "print(\"\\nMulti-turn responses:\")\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    runnable=runnable,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",  # Where to find user input\n",
    "    history_messages_key=\"chat_history\"  # Where memory gets tracked\n",
    ")  # Start a new conversation\n",
    "session_id = \"paris_learner_001\"  # Define the session ID for memory continuity\n",
    "# Send the same prompts, but with memory enabled\n",
    "for prompt in prompts:\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"A: {conversation.invoke({'input': prompt}, config={'configurable': {'session_id': session_id}}).content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713a73e-2498-41c7-bfc8-14ccbe9868e7",
   "metadata": {},
   "source": [
    "- In single-turn prompts, since there is no memory, the model can't understand references like \"its\" or \"the city\" unless you re-specify the subject in every prompt.\n",
    "- In multi-turn prompts, the model maintains context. It understands that \"its\" refers to Paris and provides more coherent answers across multiple queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
