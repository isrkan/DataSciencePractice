{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr3z-wLBXUlu"
   },
   "source": [
    "# Few-shot learning and in-context learning\n",
    "\n",
    "In this notebook, we explore few-shot learning and in-context learning, two powerful paradigms that allow LLMs to generalize and solve new tasks with minimal examples — often just a handful. These techniques are especially valuable in situations where labeled data is scarce, and traditional fine-tuning is impractical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SzDOHuzRXOdB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvh3Y9qCc8RV"
   },
   "source": [
    "### Initialize the language model\n",
    "We instantiate a lightweight GPT model from OpenAI using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-UVJOutdc8d5"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jg0Lsu3XWUf"
   },
   "source": [
    "## Basic few-shot learning\n",
    "Few-shot learning is a powerful prompting technique in which a small number of labeled examples are included directly in the input to guide the language model in learning the desired task behavior. Unlike traditional supervised learning, the model is not fine-tuned — instead, it leverages its general knowledge and the in-prompt examples to make inferences.\n",
    "\n",
    "In this section, we demonstrate few-shot learning for sentiment classification, where the model is given just three labeled examples (positive, negative, neutral) and is then asked to classify a new, unseen input.\n",
    "\n",
    "Few-shot learning is highly effective when:\n",
    "- We want to bootstrap new tasks quickly.\n",
    "- We don’t have enough labeled data for fine-tuning.\n",
    "- We are dealing with dynamic or domain-specific input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gzefxo2nXW8y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I can't believe how great this new restaurant is!\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "def few_shot_sentiment_classification(input_text):\n",
    "    # Define a few-shot prompt with 3 labeled examples\n",
    "    few_shot_prompt = PromptTemplate(\n",
    "        input_variables=[\"input_text\"],\n",
    "        template=\"\"\"\n",
    "        Classify the sentiment as Positive, Negative, or Neutral.\n",
    "\n",
    "        Examples:\n",
    "        Text: I love this product! It's amazing.\n",
    "        Sentiment: Positive\n",
    "\n",
    "        Text: This movie was terrible. I hated it.\n",
    "        Sentiment: Negative\n",
    "\n",
    "        Text: The weather today is okay.\n",
    "        Sentiment: Neutral\n",
    "\n",
    "        Now, classify the following:\n",
    "        Text: {input_text}\n",
    "        Sentiment:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create a chain by combining the prompt with the language model\n",
    "    chain = few_shot_prompt | llm\n",
    "    # Invoke the chain with input text\n",
    "    result = chain.invoke(input_text).content\n",
    "\n",
    "    # Remove extra whitespace from the result\n",
    "    result = result.strip()\n",
    "    # Extract only the sentiment label\n",
    "    if ':' in result:\n",
    "        result = result.split(':')[1].strip()\n",
    "\n",
    "    return result  # This will now return just \"Positive\", \"Negative\", or \"Neutral\"\n",
    "\n",
    "test_text = \"I can't believe how great this new restaurant is!\"\n",
    "result = few_shot_sentiment_classification(test_text)\n",
    "\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fc3ChORDgq0"
   },
   "source": [
    "1. Few-shot prompt construction:\n",
    "   - We define a `PromptTemplate` that includes three labeled examples for sentiment classification. Each example contains a short sentence followed by its correct sentiment label.\n",
    "   - The prompt ends with a new sentence (`{input_text}`) and a blank sentiment label, prompting the model to infer the label using the same pattern.\n",
    "2. Prompt execution: The prompt is passed to the `llm` using LangChain’s `|` (pipe) syntax, which creates a `Runnable` chain — in this case, one that formats the prompt and sends it to the model.\n",
    "3. Inference: The `invoke()` method runs the chain with the user's input text. The model analyzes the few-shot prompt and completes the missing sentiment label.\n",
    "4. Post-processing: We clean up the model’s response by trimming whitespace. If the model includes a colon (e.g., `\"Sentiment: Positive\"`), we extract only the label (\"Positive\") for cleaner output.\n",
    "5. Output: The final result is a simple string representing the sentiment category: `\"Positive\"`, `\"Negative\"`, or `\"Neutral\"`.\n",
    "\n",
    "This approach works well because:\n",
    "- The model is shown the pattern through examples. There is no need for training loops or labeled datasets — it generalizes from examples in the prompt.\n",
    "- It uses its pretrained knowledge of sentiment cues and linguistic structure.\n",
    "\n",
    "\n",
    "## Advanced few-shot techniques\n",
    "We now expand our few-shot setup to support multi-task learning. While basic few-shot learning is effective for single-task setups, many real-world applications require language models to handle multiple distinct tasks — such as sentiment classification, language detection, text summarization, etc.\n",
    "\n",
    "This is where multi-task learning comes into play. The idea is to use a single prompt format that supports multiple task types, allowing the model to dynamically adjust its behavior based on the task description provided in the prompt.\n",
    "\n",
    "This setup forms the foundation of prompt-based multi-task learners, which are highly valuable in real-world systems like customer support agents, multilingual chatbots, and analytics pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EKJdt8WpXa0v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "German\n"
     ]
    }
   ],
   "source": [
    "def multi_task_few_shot(input_text, task):\n",
    "    # Define a multi-task prompt with examples for sentiment and language detection\n",
    "    few_shot_prompt = PromptTemplate(\n",
    "        input_variables=[\"input_text\", \"task\"],\n",
    "        template=\"\"\"\n",
    "        Perform the specified task on the given text.\n",
    "\n",
    "        Examples:\n",
    "        Text: I love this product! It's amazing.\n",
    "        Task: sentiment\n",
    "        Result: Positive\n",
    "\n",
    "        Text: Bonjour, comment allez-vous?\n",
    "        Task: language\n",
    "        Result: French\n",
    "\n",
    "        Now, perform the following task:\n",
    "        Text: {input_text}\n",
    "        Task: {task}\n",
    "        Result:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create a chain that links the prompt to the language model\n",
    "    chain = few_shot_prompt | llm\n",
    "    # Invoke the chain with both the text and the task type\n",
    "    return chain.invoke({\"input_text\": input_text, \"task\": task}).content\n",
    "\n",
    "# Test case 1: Classify sentiment\n",
    "print(multi_task_few_shot(\"I can't believe how great this is!\", \"sentiment\"))\n",
    "# Test case 2: Detect language\n",
    "print(multi_task_few_shot(\"Guten Tag, wie geht es Ihnen?\", \"language\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64DWyaMVGVi-"
   },
   "source": [
    "1. Prompt design:\n",
    "   - We create a prompt that contains two task-specific examples:\n",
    "     - One for sentiment classification.\n",
    "     - One for language detection.\n",
    "   - Each example includes a `Text`, a `Task`, and a `Result`. This shows the model how the task parameter influences the expected output.\n",
    "   - The prompt ends with a new instruction: a fresh `Text` and a `Task`, for which the model must produce a `Result`.\n",
    "2. Dynamic task switching:\n",
    "   - The model is able to switch behavior depending on the task provided (e.g., \"sentiment\" vs \"language\").\n",
    "   - This enables the reuse of a single model and prompt structure across multiple NLP tasks.\n",
    "3. Execution pipeline:\n",
    "   - We use LangChain’s `PromptTemplate` to format the prompt.\n",
    "   - We pipe it into the `llm` object using `|` to create a chain.\n",
    "   - The `invoke()` method sends the input variables to the model and returns the model’s prediction.\n",
    "4. Task testing:\n",
    "   - We test the function on two inputs:\n",
    "     - A positive sentence for sentiment analysis.\n",
    "     - A German phrase for language identification.\n",
    "\n",
    "\n",
    "This approach works well because:\n",
    "- Efficiency: We don’t need to fine-tune separate models for each task.\n",
    "- Flexibility: New tasks can be added simply by extending the prompt with more examples.\n",
    "- Generalization: The model can infer the rules for different tasks by analogy.\n",
    "\n",
    "\n",
    "## In-context learning (ICL)\n",
    "In-context learning is a form of few-shot prompting where the LLMs learn how to perform a task from examples and instructions embedded directly in the prompt. Unlike traditional learning paradigms, ICL does not require any parameter updates or fine-tuning — the model adapts its behavior dynamically based on the given context.\n",
    "\n",
    "In this section, we demonstrate in-context learning through a simple custom task: converting English words into Pig Latin. We will give the model a description of the task, a few input–output examples, and then ask it to perform the same transformation on a new input.\n",
    "\n",
    "Use cases for in-context learning:\n",
    "- Text transformations (e.g., translation, reformatting)\n",
    "- Code generation with syntax patterns\n",
    "- Structured data extraction\n",
    "- Logical or arithmetic pattern recognition\n",
    "- Any domain where patterns can be communicated through examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eLOB9zm3Xbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: python\n",
      "Output: Output: ythonpay\n"
     ]
    }
   ],
   "source": [
    "def in_context_learning(task_description, examples, input_text):\n",
    "    # Format the examples into a single string block\n",
    "    example_text = \"\".join([f\"Input: {e['input']}\\nOutput: {e['output']}\\n\\n\" for e in examples])\n",
    "\n",
    "    # Construct the prompt using the task description, examples, and test input\n",
    "    in_context_prompt = PromptTemplate(\n",
    "        input_variables=[\"task_description\", \"examples\", \"input_text\"],\n",
    "        template=\"\"\"\n",
    "        Task: {task_description}\n",
    "\n",
    "        Examples:\n",
    "        {examples}\n",
    "\n",
    "        Now, perform the task on the following input:\n",
    "        Input: {input_text}\n",
    "        Output:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Pipe the prompt to the language model to create a chain\n",
    "    chain = in_context_prompt | llm\n",
    "    # Invoke the chain with the specified inputs\n",
    "    return chain.invoke({\"task_description\": task_description, \"examples\": example_text, \"input_text\": input_text}).content\n",
    "\n",
    "# Task description for the model\n",
    "task_desc = \"Convert the given text to pig latin.\"\n",
    "# Few-shot examples demonstrating the task pattern\n",
    "examples = [\n",
    "    {\"input\": \"hello\", \"output\": \"ellohay\"},\n",
    "    {\"input\": \"apple\", \"output\": \"appleay\"}\n",
    "]\n",
    "# New input to test\n",
    "test_input = \"python\"\n",
    "\n",
    "# Run the model\n",
    "result = in_context_learning(task_desc, examples, test_input)\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80PebCk3Kdl9"
   },
   "source": [
    "1. Task setup:\n",
    "   - We define a *task description* (\"Convert the given text to pig latin.\") to inform the model what it’s expected to do.\n",
    "   - We provide *few-shot examples* that illustrate the desired input-output behavior.\n",
    "2. Prompt construction:\n",
    "   - All examples are formatted into a single string and inserted into a structured prompt template along with the task and the new input.\n",
    "   - The prompt clearly separates examples from the new task using phrases like: `\"Now, perform the task on the following input:\"`.\n",
    "3. Model invocation:\n",
    "   - The `PromptTemplate` is piped (`|`) to the language model (`llm`) to form a processing chain.\n",
    "   - The `invoke()` method executes the chain with our variables and retrieves the model’s output.\n",
    "4. Inference: The model reads the pattern from the examples and generalizes it to the new input — `\"python\"` — to generate the correct Pig Latin output.\n",
    "\n",
    "\n",
    "This approach works well because:\n",
    "- No fine-tuning required: The model learns the task in real time from the examples — no retraining needed.\n",
    "- Rapid prototyping: We can build and test new task behaviors in seconds by just changing the prompt.\n",
    "- Model generalization: LLMs can accurately replicate complex logic from only a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypQPIEpwXXc1"
   },
   "source": [
    "## Best practices and evaluation\n",
    "Designing effective few-shot or in-context prompts requires more than just feeding the model a few examples. The quality, clarity, and structure of our prompts can significantly impact performance.\n",
    "\n",
    "### Prompting best practices\n",
    "To ensure few-shot and in-context learning yield reliable results:\n",
    "- **Example selection**:\n",
    "  - **Clarity** – Use clear, unambiguous examples that are easy for the model (and humans) to interpret. Avoid linguistic ambiguity or unclear task definitions.\n",
    "  - **Diversity** – Cover different aspects and expressions of the task (e.g., positive, negative, and neutral sentiment).\n",
    "  - **Relevance** – Choose examples that closely resemble the types of inputs your model will receive during inference.\n",
    "  - **Balance** – Ensure that all relevant categories or classes are fairly represented to avoid model bias.\n",
    "  - **Edge cases** – Include examples that are slightly unusual or potentially tricky. This helps the model generalize better.\n",
    "- **Prompt Engineering**:\n",
    "  - **Explicit instructions** – Clearly state what task the model is being asked to perform. Ambiguity in task wording can lead to poor completions.\n",
    "  - **Consistent format** – Maintain a uniform input-output structure across all examples. Repetition helps the model learn the pattern more effectively.\n",
    "  - **Conciseness** – Avoid unnecessary context or verbose descriptions. Only include information essential to the task.\n",
    "  - **Structural alignment** – Match the format and tone of your examples to the inputs the model will see during deployment.\n",
    "\n",
    "\n",
    "#### Evaluate model performance\n",
    "Evaluating a language model’s performance is a critical step in ensuring it behaves reliably and accurately in real-world scenarios. In the context of few-shot and in-context learning, evaluation allows us to validate the effectiveness of our prompt design and understand how well the model generalizes to new, unseen inputs based on limited examples.\n",
    "\n",
    "Evaluation helps us:\n",
    "- **Validate prompt effectiveness**: It tells us whether the examples and structure used in our prompt are actually helping the model understand the task.\n",
    "- **Identify weaknesses**: Repeated errors or consistent misclassifications (e.g., mistaking neutral statements for positive) highlight areas where the prompt might need refinement.\n",
    "- **Benchmark progress**: As we iterate on our few-shot design, evaluation metrics help track whether changes lead to meaningful improvements.\n",
    "- **Support decision-making**: Accurate evaluation gives us confidence before deploying models into production environments, where mistakes may have higher consequences.\n",
    "\n",
    "\n",
    "To assess how well our few-shot sentiment classification model performs, we define an evaluation function. This function takes a model and a set of labeled test cases and computes its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b__eSRjkXcCE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This product exceeded my expectations!\n",
      "Predicted: Positive\n",
      "Actual: Positive\n",
      "Correct: True\n",
      "\n",
      "Input: I'm utterly disappointed with the service.\n",
      "Predicted: Negative\n",
      "Actual: Negative\n",
      "Correct: True\n",
      "\n",
      "Input: The temperature today is 72 degrees.\n",
      "Predicted: Neutral\n",
      "Actual: Neutral\n",
      "Correct: True\n",
      "\n",
      "Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_func, test_cases):\n",
    "    '''\n",
    "    Evaluate the model on a set of test cases.\n",
    "\n",
    "    Args:\n",
    "    model_func: The function that makes predictions.\n",
    "    test_cases: A list of dictionaries, where each dictionary contains an \"input\" text and a \"label\" for the input.\n",
    "\n",
    "    Returns:\n",
    "    The accuracy of the model on the test cases.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = len(test_cases)\n",
    "\n",
    "    for case in test_cases:\n",
    "        input_text = case['input']\n",
    "        true_label = case['label']\n",
    "        # Call the prediction function with the input\n",
    "        prediction = model_func(input_text).strip()\n",
    "\n",
    "        # Compare prediction to ground truth (case-insensitive)\n",
    "        is_correct = prediction.lower() == true_label.lower()\n",
    "        correct += int(is_correct)\n",
    "\n",
    "        # Print detailed result\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Predicted: {prediction}\")\n",
    "        print(f\"Actual: {true_label}\")\n",
    "        print(f\"Correct: {is_correct}\\n\")\n",
    "\n",
    "    # Compute accuracy as correct predictions / total predictions\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Define a small test set of labeled examples\n",
    "test_cases = [\n",
    "    {\"input\": \"This product exceeded my expectations!\", \"label\": \"Positive\"},\n",
    "    {\"input\": \"I'm utterly disappointed with the service.\", \"label\": \"Negative\"},\n",
    "    {\"input\": \"The temperature today is 72 degrees.\", \"label\": \"Neutral\"}\n",
    "]\n",
    "\n",
    "# Evaluate the few-shot sentiment classifier\n",
    "accuracy = evaluate_model(few_shot_sentiment_classification, test_cases)\n",
    "# Report model performance\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlJEVWlfOTD0"
   },
   "source": [
    "1. We pass a list of test cases, where each case contains:\n",
    "   - An input text (e.g., a customer review).\n",
    "   - A correct label (e.g., `\"Positive\"`).\n",
    "2. The evaluation function loops through each test case:\n",
    "   - It calls the model function to generate a prediction.\n",
    "   - It compares the predicted sentiment to the actual label.\n",
    "   - It logs the results and counts correct predictions.\n",
    "3. Accuracy is computed as the number of correct predictions divided by the total number of test cases.\n",
    "\n",
    "We can extend this approach with:\n",
    "- Larger and more diverse test sets. More data allows us to better estimate generalization performance across varied sentence styles and topics.\n",
    "- F1 score or precision/recall metrics, especially for imbalanced classes.\n",
    "- Error analysis to understand failure patterns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
