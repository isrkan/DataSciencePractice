{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tp2YA7M_7AW"
   },
   "source": [
    "# Prompt optimization techniques\n",
    "\n",
    "This notebook explores practical methods for improving prompt effectiveness when working with LLMs. We focus on two main techniques: A/B testing and iterative refinement. These strategies help us fine-tune prompt phrasing to yield better, more relevant responses from the model.\n",
    "\n",
    "Crafting the right prompt becomes important. It is not just about asking a question anymore—how we ask it can dramatically change the quality and usefulness of the output. So the goal here is to learn how to evaluate and enhance prompts systematically—just like we would optimize hyperparameters in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ze_k8m16_6PX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni_qJzpD__J5"
   },
   "source": [
    "### Initialize the language model\n",
    "We instantiate a lightweight GPT model from OpenAI using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bLW7hj-j_-1c"
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQQu8tW5_-nz"
   },
   "source": [
    "## A/B testing prompts\n",
    "A/B testing allows us to compare different versions of a prompt to evaluate which performs better for a specific task. The idea is to isolate variations in phrasing and measure how those variations affect the response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QUrg4K2XDxdp"
   },
   "outputs": [],
   "source": [
    "# Define prompt variation A\n",
    "prompt_a = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms.\"\n",
    ")\n",
    "\n",
    "# Define prompt variation B\n",
    "prompt_b = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Provide a beginner-friendly explanation of {topic}, including key concepts and an example.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQZcANx9DzLx"
   },
   "source": [
    "We define two alternative prompts that aim to explain the same topic. Prompt A is very general, while Prompt B adds a request for structure and an example, which may help guide the model toward more informative answers. We use LangChain’s `PromptTemplate` to define reusable prompt formats. Both prompts accept a `topic` variable and ask the model to explain it.\n",
    "\n",
    "#### Evaluation function\n",
    "We need a way to evaluate the responses generated by each prompt. We will do this by scoring them based on several criteria: clarity, informativeness, and engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R3_nxR5VEE5e"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate response quality\n",
    "def evaluate_response(response, criteria):\n",
    "    \"\"\"Evaluate the quality of a response based on given criteria.\n",
    "\n",
    "    Args:\n",
    "        response (str): The generated response.\n",
    "        criteria (list): List of criteria to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average score across all criteria.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for criterion in criteria:\n",
    "        print(f\"Evaluating response based on {criterion}...\")\n",
    "        # Ask the model to rate the response\n",
    "        prompt = f\"On a scale of 1-10, rate the following response on {criterion}. Start your response with the numeric score:\\n\\n{response}\"\n",
    "        response = llm.invoke(prompt).content\n",
    "        # show 50 characters of the response\n",
    "        # Use regex to find the first number in the response\n",
    "        score_match = re.search(r'\\d+', response)\n",
    "        if score_match:\n",
    "            score = int(score_match.group())\n",
    "            scores.append(min(score, 10))  # Ensure score is not greater than 10\n",
    "        else:\n",
    "            print(f\"Warning: Could not extract numeric score for {criterion}. Using default score of 5.\")\n",
    "            scores.append(5)  # Default score if no number is found\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF6k_HeCEX7u"
   },
   "source": [
    "For each response, we generate follow-up prompts asking the model to self-evaluate its output against a given criterion. This approach gives us a semi-automated evaluation loop using the model itself for subjective scoring. We extract the numerical score using regex and calculate the average across all criteria.\n",
    "\n",
    "#### Running the A/B test on an example\n",
    "Now we can plug in a topic and compare both prompt versions side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mGRrCKVD_-Yv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Prompt A score: 9.00\n",
      "Prompt B score: 9.00\n",
      "Winning prompt: B\n"
     ]
    }
   ],
   "source": [
    "# Define the topic to explain\n",
    "topic = \"machine learning\"\n",
    "\n",
    "# Generate responses for both prompt versions\n",
    "response_a = llm.invoke(prompt_a.format(topic=topic)).content\n",
    "response_b = llm.invoke(prompt_b.format(topic=topic)).content\n",
    "\n",
    "# Define evaluation criteria\n",
    "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
    "\n",
    "# Evaluate both responses\n",
    "score_a = evaluate_response(response_a, criteria)\n",
    "score_b = evaluate_response(response_b, criteria)\n",
    "\n",
    "# Print results\n",
    "print(f\"Prompt A score: {score_a:.2f}\")\n",
    "print(f\"Prompt B score: {score_b:.2f}\")\n",
    "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLwfAg0w_-Kt"
   },
   "source": [
    "We run both prompts with the same topic and generate responses. These are then scored based on our criteria, and the scores are compared. This helps us determine which prompt leads to better overall output.\n",
    "\n",
    "## Iterative refinement of prompts\n",
    "Now that we have tested which prompt performs better, let’s say we want to improve it further. Iterative refinement lets us gradually improve a prompt by analyzing its outputs and generating suggestions for enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZT5dBZ5lQxrE"
   },
   "outputs": [],
   "source": [
    "def refine_prompt(initial_prompt, topic, iterations=3):\n",
    "    \"\"\"Refine a prompt through multiple iterations.\n",
    "\n",
    "    Args:\n",
    "        initial_prompt (PromptTemplate): The starting prompt template.\n",
    "        topic (str): The topic to explain.\n",
    "        iterations (int): Number of refinement iterations.\n",
    "\n",
    "    Returns:\n",
    "        PromptTemplate: The final refined prompt template.\n",
    "    \"\"\"\n",
    "    current_prompt = initial_prompt\n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            # Generate response using current prompt\n",
    "            response = llm.invoke(current_prompt.format(topic=topic)).content\n",
    "        except KeyError as e:\n",
    "            print(f\"Error in iteration {i+1}: Missing key {e}. Adjusting prompt...\")\n",
    "            # Remove the problematic placeholder\n",
    "            current_prompt.template = current_prompt.template.replace(f\"{{{e.args[0]}}}\", \"relevant example\")\n",
    "            response = llm.invoke(current_prompt.format(topic=topic)).content\n",
    "\n",
    "        # Ask the model how to improve the prompt\n",
    "        feedback_prompt = f\"Analyze the following explanation of {topic} and suggest improvements to the prompt that generated it:\\n\\n{response}\"\n",
    "        feedback = llm.invoke(feedback_prompt).content\n",
    "\n",
    "        # Use model's feedback to refine the prompt\n",
    "        refine_prompt = f\"Based on this feedback: '{feedback}', improve the following prompt template. Ensure to only use the variable {{topic}} in your template:\\n\\n{current_prompt.template}\"\n",
    "        refined_template = llm.invoke(refine_prompt).content\n",
    "\n",
    "        # Create a new prompt template from refined result\n",
    "        current_prompt = PromptTemplate(\n",
    "            input_variables=[\"topic\"],\n",
    "            template=refined_template\n",
    "        )\n",
    "\n",
    "        print(f\"Iteration {i+1} prompt: {current_prompt.template}\")\n",
    "\n",
    "    return current_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDf54mwWQyBN"
   },
   "source": [
    "This function takes a prompt and iteratively improves it. After each round, it asks the model for feedback on how to improve the prompt based on the generated response. Then it uses that feedback to create a refined version of the prompt, repeating this loop for a specified number of iterations. This loop mimics how we would improve a prompt manually, but automates it with the LLM's help.\n",
    "\n",
    "#### Applying iterative refinement to the best prompt on an example\n",
    "We use the prompt that performed better in the A/B test and apply our refinement process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8AKwU-dN_9-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Prompt A score: 9.00\n",
      "Prompt B score: 8.00\n",
      "Winning prompt: A\n",
      "Iteration 1 prompt: Here’s an improved prompt template that incorporates the suggestions provided while maintaining the variable {topic}:\n",
      "\n",
      "---\n",
      "\n",
      "\"Explain {topic} in simple terms, including its different aspects such as key categories (e.g., supervised, unsupervised, reinforcement learning), real-world applications (like healthcare, finance, and autonomous vehicles), and the significance of data quality. Describe the fundamental learning process of algorithms, touching on how they adjust based on feedback. Additionally, highlight some limitations or challenges associated with {topic}. Use a relatable analogy to clarify the concept, and consider incorporating relevant technical terms for a more comprehensive understanding.\"\n",
      "\n",
      "--- \n",
      "\n",
      "This revised prompt encourages a more thorough and engaging explanation of {topic}, aligning with the feedback provided.\n",
      "Iteration 2 prompt: Here’s a further refined prompt template that incorporates the suggestions while ensuring clarity and engagement:\n",
      "\n",
      "---\n",
      "\n",
      "\"Provide a comprehensive overview of {topic} in simple terms for beginners. Break down the key categories (such as supervised, unsupervised, and reinforcement learning) and illustrate each with specific real-world applications across various industries like healthcare, finance, and autonomous vehicles. Explain the fundamental learning process of algorithms, including how they adapt based on feedback, and discuss the significance of data quality and ethical considerations, such as bias and data privacy. Use relatable analogies to clarify complex concepts, and consider incorporating relevant technical terms to enhance understanding. Finally, touch on the limitations and challenges associated with {topic}, and include insights on future trends or emerging technologies in the field.\"\n",
      "\n",
      "--- \n",
      "\n",
      "This prompt ensures a well-rounded exploration of {topic}, making it suitable for the intended audience while emphasizing clarity, depth, and engagement.\n",
      "Iteration 3 prompt: Here’s a refined prompt template that incorporates your feedback while ensuring clarity and engagement:\n",
      "\n",
      "---\n",
      "\n",
      "\"Provide a beginner-friendly overview of {topic}, tailored for individuals with no prior knowledge of the subject. Break down the key categories (such as supervised, unsupervised, and reinforcement learning) and illustrate each with relatable real-world applications across various industries, including healthcare, finance, and autonomous vehicles. Explain the fundamental learning process of algorithms, detailing how they adapt based on feedback, and emphasize the importance of data quality and ethical considerations, such as bias and data privacy. Use analogies or everyday examples to clarify complex concepts, and minimize technical jargon, providing definitions for any necessary terms to ensure accessibility. Include engaging elements, such as reflective questions or a short quiz at the end of sections to reinforce learning. Discuss the limitations and challenges associated with {topic}, as well as insights on future trends or emerging technologies in the field. Conclude with recommendations for further reading or resources for beginners who wish to explore {topic} in more depth.\"\n",
      "\n",
      "---\n",
      "\n",
      "This prompt aims to produce an engaging and informative overview of {topic}, making it suitable for a beginner audience while prioritizing clarity and educational value.\n",
      "\n",
      "Final refined prompt:\n",
      "Here’s a refined prompt template that incorporates your feedback while ensuring clarity and engagement:\n",
      "\n",
      "---\n",
      "\n",
      "\"Provide a beginner-friendly overview of {topic}, tailored for individuals with no prior knowledge of the subject. Break down the key categories (such as supervised, unsupervised, and reinforcement learning) and illustrate each with relatable real-world applications across various industries, including healthcare, finance, and autonomous vehicles. Explain the fundamental learning process of algorithms, detailing how they adapt based on feedback, and emphasize the importance of data quality and ethical considerations, such as bias and data privacy. Use analogies or everyday examples to clarify complex concepts, and minimize technical jargon, providing definitions for any necessary terms to ensure accessibility. Include engaging elements, such as reflective questions or a short quiz at the end of sections to reinforce learning. Discuss the limitations and challenges associated with {topic}, as well as insights on future trends or emerging technologies in the field. Conclude with recommendations for further reading or resources for beginners who wish to explore {topic} in more depth.\"\n",
      "\n",
      "---\n",
      "\n",
      "This prompt aims to produce an engaging and informative overview of {topic}, making it suitable for a beginner audience while prioritizing clarity and educational value.\n"
     ]
    }
   ],
   "source": [
    "# Perform A/B test\n",
    "topic = \"machine learning\"\n",
    "# Generate and score responses for original and refined prompts\n",
    "response_a = llm.invoke(prompt_a.format(topic=topic)).content\n",
    "response_b = llm.invoke(prompt_b.format(topic=topic)).content\n",
    "\n",
    "# Define evaluation criteria\n",
    "criteria = [\"clarity\", \"informativeness\", \"engagement\"]\n",
    "# Evaluate both responses\n",
    "score_a = evaluate_response(response_a, criteria)\n",
    "score_b = evaluate_response(response_b, criteria)\n",
    "\n",
    "print(f\"Prompt A score: {score_a:.2f}\")\n",
    "print(f\"Prompt B score: {score_b:.2f}\")\n",
    "print(f\"Winning prompt: {'A' if score_a > score_b else 'B'}\")\n",
    "\n",
    "# Start with the winning prompt from A/B testing\n",
    "initial_prompt = prompt_b if score_b > score_a else prompt_a\n",
    "\n",
    "# Refine the prompt through multiple iterations\n",
    "refined_prompt = refine_prompt(initial_prompt, \"machine learning\")\n",
    "\n",
    "print(\"\\nFinal refined prompt:\")\n",
    "print(refined_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7cEHNhk_9yC"
   },
   "source": [
    "We generate one more response for both the original and refined prompts and score them. Then, We select the winning prompt based on A/B test results and feed it through the refinement loop to improve it further. We can apply this same loop to any kind of prompt: summarization, translation, extraction, etc.\n",
    "\n",
    "#### Comparing original and refined prompts\n",
    "Finally, let’s compare the actual outputs generated by the original and refined prompts side by side using the same evaluation method as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qhEst7HK_9mD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Evaluating response based on clarity...\n",
      "Evaluating response based on informativeness...\n",
      "Evaluating response based on engagement...\n",
      "Original prompt score: 9.00\n",
      "Refined prompt score: 9.00\n",
      "Improvement: 0.00 points\n"
     ]
    }
   ],
   "source": [
    "# Generate responses using both prompts\n",
    "original_response = llm.invoke(initial_prompt.format(topic=\"machine learning\")).content\n",
    "refined_response = llm.invoke(refined_prompt.format(topic=\"machine learning\")).content\n",
    "\n",
    "# Score both responses\n",
    "original_score = evaluate_response(original_response, criteria)\n",
    "refined_score = evaluate_response(refined_response, criteria)\n",
    "\n",
    "# Show final comparison\n",
    "print(f\"Original prompt score: {original_score:.2f}\")\n",
    "print(f\"Refined prompt score: {refined_score:.2f}\")\n",
    "print(f\"Improvement: {(refined_score - original_score):.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92NVrUjr_9ZH"
   },
   "source": [
    "We are measuring the performance uplift achieved by our refinement strategy, based on the same scoring criteria. If the refined score is higher, we know our iteration loop is working as intended."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
