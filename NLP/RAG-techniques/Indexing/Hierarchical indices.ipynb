{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDG6z6b482dW"
   },
   "source": [
    "# Hierarchical indices in document retrieval\n",
    "\n",
    "In this notebook, we implement a hierarchical indexing system to improve the relevance and efficiency of document retrieval. Unlike flat vector-based approaches that treat all content chunks equally, this method adds structure by first summarizing documents and then indexing those summaries alongside detailed chunks.\n",
    "\n",
    "We use OpenAI’s GPT-4o-mini for summarization, FAISS for vector storage, and OpenAI Embeddings for representing text. The idea is to search summaries first to narrow down relevant sections, and then search within those sections for fine-grained detail. This is ideal when working with large documents or long-form content, where flat retrieval can miss context or feel too noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "D20dVoX280Ol"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import RateLimitError\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md-LR6oH9XDk"
   },
   "source": [
    "### Handling API rate limiting\n",
    "When working with OpenAI's API (or any external service), there is a chance our code might hit a rate limit — which means we are sending too many requests too quickly. If this happens, the API won't return our result, and our app could break unless we handle it.\n",
    "\n",
    "To fix this, we implement a retry mechanism that waits a bit and tries again, using something called exponential backoff with jitter. That just means:\n",
    "- Exponential backoff means that the time between retries gets longer after each failure: first wait 2 seconds if it fails the first time. If it fails again, wait 4 seconds. Then 8, and so on — giving the server a chance to breathe.\n",
    "- Jitter adds a bit of randomness to that wait time, to avoid everyone retrying at the exact same time (which could overwhelm the server again).\n",
    "\n",
    "This function will wrap any async request we make (like summarizing with GPT or embedding text) and make sure that if we hit a temporary limit, the code won't crash — it'll just wait and try again.\n",
    "\n",
    "#### Define the retry logic with exponential backoff + jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uEOJDkKHGg0U"
   },
   "outputs": [],
   "source": [
    "async def exponential_backoff(attempt):\n",
    "    \"\"\"\n",
    "    Implements exponential backoff with a jitter.\n",
    "\n",
    "    Args:\n",
    "        attempt: The current retry attempt number.\n",
    "\n",
    "    Waits for a period of time before retrying the operation.\n",
    "    The wait time is calculated as (2^attempt) + a random fraction of a second.\n",
    "    \"\"\"\n",
    "    # Calculate the wait time with exponential backoff and jitter\n",
    "    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "    print(f\"Rate limit hit. Retrying in {wait_time:.2f} seconds...\")\n",
    "\n",
    "    # Asynchronously sleep for the calculated wait time\n",
    "    await asyncio.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkVWq1W9GhDH"
   },
   "source": [
    "This function handles the actual delay logic. We will use it inside our retry wrapper below.\n",
    "\n",
    "#### Retry any async operation with backoff logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OKtc3_zX_Ji1"
   },
   "outputs": [],
   "source": [
    "async def retry_with_exponential_backoff(coroutine, max_retries=5):\n",
    "    \"\"\"\n",
    "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
    "\n",
    "    Args:\n",
    "        coroutine: The coroutine to be executed.\n",
    "        max_retries: The maximum number of retry attempts.\n",
    "\n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "\n",
    "    Raises:\n",
    "        The last encountered exception if all retry attempts fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt to execute the coroutine\n",
    "            return await coroutine\n",
    "        except RateLimitError as e:\n",
    "            # If the last attempt also fails, raise the exception\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "\n",
    "            # Wait for an exponential backoff period before retrying\n",
    "            await exponential_backoff(attempt)\n",
    "\n",
    "    # If max retries are reached without success, raise an exception\n",
    "    raise Exception(\"Max retries reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJvjXxlaDFyv"
   },
   "source": [
    "What this is doing (step-by-step):\n",
    "1. It defines an async function that wraps any other coroutine (an async task like sending a request). We pass in any async task — like a call to GPT or an embedding request.\n",
    "2. If it runs fine, we get the result immediately.\n",
    "3. If it fails due to a rate limit, the retry function:\n",
    "   - Calculates how long to wait using `exponential_backoff`.\n",
    "   - Waits asynchronously without blocking other tasks.\n",
    "   - Tries again (up to `max_retries` times).\n",
    "4. If it still fails after all retries, the error is raised so we can handle it (or crash gracefully).\n",
    "\n",
    "### Load the PDF\n",
    "We will use `PyPDFLoader` to extract text from the PDF. It reads the PDF page by page and stores the extracted text in a list of document objects, where each document contains the content of a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pT2zR3u4DF_8"
   },
   "outputs": [],
   "source": [
    "# Path to the PDF document\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Define a coroutine to load the document\n",
    "async def load_pdf(path):\n",
    "    # Use the default event loop to run the load operation in a separate thread\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loader = PyPDFLoader(path)\n",
    "    \n",
    "    # Run the loading operation in a separate thread\n",
    "    documents = await loop.run_in_executor(None, loader.load)\n",
    "    return documents\n",
    "\n",
    "# Use the load_pdf coroutine to load the document\n",
    "documents = await load_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOEu8p4QJblr"
   },
   "source": [
    "### Summarize each page\n",
    "So now that we have the full document loaded and split into separate pages (or logical chunks), the next smart move is to create a summary of each one. Why? Because it's a lot easier — and more efficient — to start searching through summaries than to jump straight into hundreds of raw text blocks. Think of it like scanning a table of contents before reading a whole book. We will create a high-level summary of each document using the GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "slVhlRMWJbyU"
   },
   "outputs": [],
   "source": [
    "# Create document-level summaries\n",
    "summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000) # Load the summarization LLM\n",
    "# Load a built-in summarization chain using LangChain's map_reduce approach\n",
    "summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5JFaaMpJcB4"
   },
   "source": [
    "- We are setting up the language model we'll use to generate the summaries. `ChatOpenAI` loads the GPT-4o-mini model and with a `temperature` of 0 it gives us more deterministic, reliable results (not too creative, just focused). The `max_tokens` lets it write longer summaries when needed.\n",
    "- Then we load what LangChain calls a \"summarization chain\" — it is like a pre-built recipe that takes in a document and gives us back a nicely structured summary. We use the `\"map_reduce\"` type here, which is helpful when summarizing longer inputs: it processes parts individually and then combines the results.\n",
    "\n",
    "Now let’s actually loop through our pages, summarize each one, and collect the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "08PN1bCnJcQQ"
   },
   "outputs": [],
   "source": [
    "# Process documents in smaller batches to avoid rate limits\n",
    "batch_size = 5  # Adjust this based on your rate limits\n",
    "# Collect summaries here\n",
    "summaries = []\n",
    "\n",
    "# Summarize each page asynchronously\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i+batch_size]\n",
    "\n",
    "    # For each document in the batch, generate a summary\n",
    "    batch_tasks = []\n",
    "    for doc in batch:\n",
    "        # Use our retry helper to avoid crashing on rate limits\n",
    "        task = retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        batch_tasks.append(task)\n",
    "\n",
    "    # Run the batch and gather summaries\n",
    "    batch_summaries = await asyncio.gather(*batch_tasks)\n",
    "\n",
    "    # Store summaries as Document objects\n",
    "    for original_doc, summary_result in zip(batch, batch_summaries):\n",
    "        summary = summary_result['output_text']\n",
    "        summaries.append(Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"source\": path,\n",
    "                \"page\": original_doc.metadata[\"page\"],\n",
    "                \"summary\": True\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # short pause to avoid hitting rate limits\n",
    "    await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6aoWdnTDGLy"
   },
   "source": [
    "- We go through the pages (or document chunks) in small groups of 5. That’s mostly to avoid triggering OpenAI’s rate limits — because asking it to summarize 50 pages at once is probably going to get us blocked or throttled.\n",
    "- For each group, we create tasks — one per document — and call our `retry_with_exponential_backoff()` function, just in case the API complains. These tasks are executed with `asyncio.gather()` so we can run them in parallel and speed things up.\n",
    "- Once we have the summaries, we store each one as a new `Document`. We also add some metadata so we can track which summary belongs to which page later. That `\"summary\": True` tag will be important in the next steps when we build our hierarchical retrieval logic.\n",
    "- We also add a little `await asyncio.sleep(1)` just to be nice to the API and avoid hammering it too fast.\n",
    "\n",
    "### Create detailed chunks\n",
    "Now that we have our document summarized into smaller, more digestible pieces (summaries), we can move on to the next part: splitting the original text into detailed chunks.\n",
    "\n",
    "So why would we need this? Well, summaries give us a high-level view, but if we need more context or want to dive into specific sections, we need to break the document into smaller chunks — chunks that still contain enough detail for the user to understand without needing the entire page of text. These chunks will give us much more flexibility when we are performing searches later.\n",
    "\n",
    "Now, instead of splitting the document at random, we want to split it based on the number of characters and overlap between chunks, ensuring no meaningful context is lost when we move from one chunk to another. For that, we will use LangChain's `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0IIUNyN_PJlF"
   },
   "outputs": [],
   "source": [
    "# Define chunk size and overlap\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "# Define the coroutine to split the documents\n",
    "async def split_documents(docs, chunk_size, chunk_overlap):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    \n",
    "    # Run the split operation in a separate thread using the event loop's executor\n",
    "    detailed_chunks = await loop.run_in_executor(None, text_splitter.split_documents, docs)\n",
    "    return detailed_chunks\n",
    "\n",
    "# Split the documents into detailed chunks\n",
    "detailed_chunks = await split_documents(documents, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW48udoGQ8K8"
   },
   "source": [
    "We start by setting up the `RecursiveCharacterTextSplitter`. Here, we define a couple of important things:\n",
    "1. **`chunk_size`** — This tells the splitter how big we want each chunk of text to be. We set this based on how much content we think is manageable and still meaningful (in terms of information retention).\n",
    "2. **`chunk_overlap`** — This is like padding between chunks. It ensures that the end of one chunk and the beginning of the next overlap just a little, so we don’t lose important context when moving between sections.\n",
    "3. **`length_function`** — This just defines how we count the length of the text. We use the standard `len()` function, which counts characters.\n",
    "\n",
    "By calling `split_documents`, we split the entire document into smaller, more manageable chunks. The output here will be a list of documents where each document is a chunk of the original text.\n",
    "\n",
    "Next, we want to update the metadata for each of these detailed chunks. This is important because later on, when we are searching through these chunks, we want to know from which page or section each chunk came from. We will also tag these chunks as not being summaries so we can keep track of which documents are summaries and which are more detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qAP7jNyEQ8ah"
   },
   "outputs": [],
   "source": [
    "# Update metadata for detailed chunks\n",
    "for i, chunk in enumerate(detailed_chunks):\n",
    "    chunk.metadata.update({\n",
    "        \"chunk_id\": i,\n",
    "        \"summary\": False,\n",
    "        \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miP-4xIMPKNd"
   },
   "source": [
    "We loop through each of the `detailed_chunks` we just created. For each chunk, we update the metadata to:\n",
    "- **Assign a unique `chunk_id`** — This is just a unique identifier for each chunk (it is an index in this case). It helps us keep track of which chunk is which.\n",
    "- **Set `summary: False`** — This tells us that the chunk is not a summary but an actual piece of detailed content.\n",
    "- **Record the `page` number** — The original document had page numbers, and we want to retain this information to reference the chunk’s location in the original text.\n",
    "\n",
    "### Embed and create vector stores\n",
    "Now that we have our summaries and detailed chunks ready, the next step is to transform these chunks into embeddings and store them in vector stores. We will be using OpenAI's embedding model to generate embeddings for both the summaries and the detailed chunks. After that, we will store the embeddings in vector stores for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b9p0mCOsSFSO"
   },
   "outputs": [],
   "source": [
    "# Initializes the OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGC-GjwzUDvE"
   },
   "source": [
    "Next, we will create the vector store. We will use FAISS, a library for nearest neighbor search, to create the vector stores. This will allow us to find the most semantically similar chunks of text when performing a query. We need to embed both the summaries and detailed chunks, so we will create two separate vector stores — one for each type of chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RHinWCxZUC9k"
   },
   "outputs": [],
   "source": [
    "# Create vector stores asynchronously with rate limit handling\n",
    "async def create_vectorstore(docs):\n",
    "    \"\"\"\n",
    "    Creates a vector store from a list of documents with rate limit handling.\n",
    "\n",
    "    Args:\n",
    "        docs: The list of documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    # Use run_in_executor to run the FAISS embedding process in a separate thread\n",
    "    return await retry_with_exponential_backoff(\n",
    "        loop.run_in_executor(None, FAISS.from_documents, docs, embeddings)\n",
    "    )\n",
    "\n",
    "# Generate vector stores for summaries and detailed chunks concurrently\n",
    "summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "    create_vectorstore(summaries),\n",
    "    create_vectorstore(detailed_chunks)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA5XF_n1SFiU"
   },
   "source": [
    "Here we define an asynchronous helper function `create_vectorstore` that will:\n",
    "1. Convert documents to embeddings: This is done by passing a batch of documents (summaries or detailed chunks) to the `FAISS.from_documents()` method along with the `embeddings` model.\n",
    "2. Handle rate limits: Since embedding large batches of documents may hit API rate limits, we use our previously defined `retry_with_exponential_backoff` function to ensure that the embedding process retries automatically with an exponential delay in case it encounters a rate limit error.\n",
    "\n",
    "After this, we are ready to create the vector stores for both the summaries and the detailed chunks. We will do this concurrently for efficiency. In this part of the code, we use `asyncio.gather` to execute the `create_vectorstore` function concurrently for both the summaries and the detailed chunks. This ensures that both vector stores are created at the same time, rather than sequentially, making the process more efficient.\n",
    "\n",
    "#### Save vector stores for reuse\n",
    "In this step, we will save the vector stores for future use. Instead of recomputing the embeddings and vector stores every time we run the process, we can persist them on disk, so we can reuse them without the need for recalculation. This saves us time, computational resources, and makes our retrieval system more efficient in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Mzw6-hum9XPw"
   },
   "outputs": [],
   "source": [
    "summary_vectorstore.save_local(\"../vector_stores/summary_store\")\n",
    "detailed_vectorstore.save_local(\"../vector_stores/detailed_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Tqo62-oDDZ2"
   },
   "source": [
    "We use the FAISS library’s `.save_local()` method to store the vector stores on disk. This method serializes the vector stores and writes them to a specified directory.\n",
    "\n",
    "Or, if they already exist, load them instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py05qaMfDCY1"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"../vector_stores/summary_store\") and os.path.exists(\"../vector_stores/detailed_store\"):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    summary_vectorstore = FAISS.load_local(\"../vector_stores/summary_store\", embeddings, allow_dangerous_deserialization=True)\n",
    "    detailed_vectorstore = FAISS.load_local(\"../vector_stores/detailed_store\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAC2skA19XbS"
   },
   "source": [
    "### Perform hierarchical retrieval\n",
    "Now that we have created and saved the vector stores (both for summaries and detailed chunks), the next step is performing retrieval. The goal is to retrieve relevant information based on a query, but in a way that is both efficient and contextually relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "45e_qjNMDCmc"
   },
   "outputs": [],
   "source": [
    "query = \"What is the greenhouse effect?\"\n",
    "k_summaries = 3\n",
    "k_chunks = 5\n",
    "\n",
    "# Step 1: Search summaries\n",
    "top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "\n",
    "# Step 2: Drill down into relevant pages\n",
    "relevant_chunks = []\n",
    "\n",
    "for summary in top_summaries:\n",
    "    # For each summary, retrieve relevant detailed chunks\n",
    "    page_number = summary.metadata[\"page\"]\n",
    "    page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "    page_chunks = detailed_vectorstore.similarity_search(\n",
    "        query,\n",
    "        k=k_chunks,\n",
    "        filter=page_filter\n",
    "    )\n",
    "    relevant_chunks.extend(page_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9QSwYmLZ0cc"
   },
   "source": [
    "1. Set the query and parameters: We define the query we are searching for. We also set `k_summaries = 3`, meaning we want to retrieve the top 3 relevant summaries, and `k_chunks = 5`, meaning we want to get 5 detailed chunks per relevant summary.\n",
    "2. Search for relevant summaries: The first step in hierarchical retrieval is to search through the summary vector store (`summary_vectorstore`) to find the most relevant summaries to the query.\n",
    "   - We use the `similarity_search()` method of the `summary_vectorstore`, which compares the query with each summary and returns the top `k_summaries` most similar to the query.\n",
    "   - The result, `top_summaries`, is a list of the top 3 summaries that are most relevant to the query.\n",
    "3. Filter by page and retrieve detailed chunks: For each of the top summaries retrieved, we extract the page number of that summary using `summary.metadata[\"page\"]`. This is important because we want to ensure that we only retrieve detailed chunks from the same page as the relevant summary.\n",
    "4. Search for detailed chunks: Using the `similarity_search()` method on the detailed vector store (`detailed_vectorstore`), we search for detailed chunks that are similar to the query.\n",
    "   - We apply the `page_filter` to restrict the search to the chunks on the same page as the current summary.\n",
    "   - We retrieve the top `k_chunks` detailed chunks for each relevant summary.\n",
    "5. Combine all relevant chunks: For each summary, we retrieve detailed chunks and extend the list of relevant chunks (`relevant_chunks`) with these results. After looping through all the top summaries, `relevant_chunks` will contain all the relevant detailed chunks that are closely tied to the query.\n",
    "\n",
    "### Display the results\n",
    "In this final step, we will present the relevant information we have retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5k1XSErI9XoO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0\n",
      "Content: Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is...\n",
      "---\n",
      "Page: 0\n",
      "Content: Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change ...\n",
      "---\n",
      "Page: 0\n",
      "Content: Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to si...\n",
      "---\n",
      "Page: 5\n",
      "Content: Energy-efficient buildings use less energy for heating, cooling, and lighting. This can be \n",
      "achieved...\n",
      "---\n",
      "Page: 5\n",
      "Content: a long time. These projects can help sequester carbon and provide new habitats for wildlife. \n",
      "Strate...\n",
      "---\n",
      "Page: 2\n",
      "Content: development of eco-friendly fertilizers and farming techniques is essential for reducing the \n",
      "agricu...\n",
      "---\n",
      "Page: 2\n",
      "Content: Heatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \n",
      "Changing Se...\n",
      "---\n",
      "Page: 2\n",
      "Content: Ruminant animals, such as cows and sheep, produce methane during digestion. Manure \n",
      "management pract...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for chunk in relevant_chunks:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content[:100]}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
