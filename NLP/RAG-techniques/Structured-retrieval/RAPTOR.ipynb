{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhe96c8G0GhK"
   },
   "source": [
    "# RAPTOR: Recursive abstractive processing and thematic organization for retrieval\n",
    "\n",
    "In many real-world scenarios, documents are long and information-dense, yet users often ask very specific or high-level questions. Flat retrieval systems can either miss the big picture or drown the model in irrelevant details. RAPTOR (Recursive Abstractive Processing and Thematic Organization for Retrieval) addresses this by building a multi-level abstraction of the input documents, clustering semantically similar content, summarizing each cluster, and repeating this process hierarchically. This results in a tree of document abstractions — where high-level summaries can guide retrieval towards relevant details when answering a query.\n",
    "\n",
    "This notebook demonstrates how to build a RAPTOR tree over long documents, organize that tree into a searchable vectorstore, retrieve relevant information hierarchically using both semantics and context and generate concise, accurate answers using only the most useful content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yOiYNTv90AmH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# MKL library and thread handling on Windows\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZsY3PbP0Wqx"
   },
   "source": [
    "### Initialize logging, embeddings, and language model\n",
    "\n",
    "We initialize our logging, language model, and embedding model below.\n",
    "- Logging: This helps us monitor the process, especially when building hierarchical trees or running long summarization tasks. It is also useful for debugging and understanding how the system behaves.\n",
    "- Embeddings: Text embeddings are dense vector representations of text that capture semantic meaning. We will use them for clustering and similarity search.\n",
    "- LLM: This will be the core engine for summarization, contextual compression, and final answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CTz85G_00WaB"
   },
   "outputs": [],
   "source": [
    "# Set up logging configuration to monitor pipeline stages and status messages\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize embeddings for document and query representation\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize chat model for summarization and generation\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThT0idXO0WN9"
   },
   "source": [
    "- The logging setup ensures that every major step—like embedding, clustering, summarizing—gets logged with a timestamp and severity level (info, warning, etc.).\n",
    "- The `OpenAIEmbeddings` instance will later be used to embed both original documents and their summaries, so that we can organize and retrieve them efficiently using similarity search.\n",
    "- The `ChatOpenAI` model will power all natural language tasks, such as generating summaries at each hierarchical level and eventually answering the user's query using compressed, relevant content.\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "At the heart of RAPTOR is the repeated cycle of embedding, clustering, and summarizing at multiple hierarchical levels. To support this, we define a set of modular utility functions that each handle a core building block of the process.\n",
    "\n",
    "These utilities will be reused throughout the tree-building pipeline, allowing us to abstract away the lower-level steps of transforming raw documents into structured, semantically meaningful summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rq7TQKvZ0WB1"
   },
   "outputs": [],
   "source": [
    "def extract_text(item):\n",
    "    \"\"\"\n",
    "    Extracts the raw text content from either a plain string or a LangChain AIMessage object.\n",
    "    This allows flexibility when working with both user input and LLM responses.\n",
    "    \"\"\"\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content # Pull the actual text from the AI response\n",
    "    return item  # Assume the input is already a plain string\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embed texts using OpenAIEmbeddings. Embeddings are later used for clustering and similarity-based retrieval.\"\"\"\n",
    "    # Log how many documents we are processing\n",
    "    logging.info(f\"Embedding {len(texts)} texts\")\n",
    "    # Converts a list of texts into their corresponding embedding vectors\n",
    "    return embeddings.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform clustering on embeddings using Gaussian Mixture Model (GMM).\n",
    "    Each cluster ideally represents a thematic grouping of semantically related texts.\n",
    "    \"\"\"\n",
    "    # Log how many clusters we are trying to create\n",
    "    logging.info(f\"Performing clustering with {n_clusters} clusters\")\n",
    "    # Initialize the Gaussian Mixture model with a fixed random seed for reproducibility\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    # Fit the GMM to the data and return the predicted cluster labels for each embedding\n",
    "    return gm.fit_predict(embeddings)\n",
    "\n",
    "def summarize_texts(texts: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generates a concise summary of a list of texts using the LLM.\n",
    "    This is used to create the higher-level nodes in the RAPTOR tree.\n",
    "    \"\"\"\n",
    "    # Log the number of text chunks being summarized at this step\n",
    "    logging.info(f\"Summarizing {len(texts)} texts\")\n",
    "    # Define the prompt template for the LLM to summarize the input\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    # Combine the prompt template and the LLM into a runnable LangChain chain\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"text\": texts} # Pass all texts to be summarized\n",
    "    # Run the chain and return the LLM's response\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, level: int):\n",
    "    \"\"\"\n",
    "    Uses PCA to reduce embedding dimensionality and plots a scatterplot of the clusters.\n",
    "    Helpful for visually inspecting how well the semantic clustering performed at each level.\n",
    "    \"\"\"\n",
    "    # Use PCA to reduce high-dimensional embeddings down to 2D for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Create a scatter plot of points colored by cluster assignment\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)  # Add a color legend for clusters\n",
    "    plt.title(f'Cluster Visualization - Level {level}')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vAJNUi_0V1x"
   },
   "source": [
    "Here, we define the modular operations that make RAPTOR's recursive structure work:\n",
    "- The `extract_text` function gives us a unified way to handle inputs from different stages (e.g., strings or AIMessage outputs).\n",
    "- `embed_texts` transforms raw or summarized text into dense vector representations, which are the backbone of clustering and retrieval in RAPTOR.\n",
    "- `perform_clustering` applies a Gaussian Mixture Model to the embeddings to detect thematic groupings. These clusters will serve as the basis for summarizing and compressing information at each level.\n",
    "- `summarize_texts` generates one summary per cluster, forming the input for the next level of the hierarchy.\n",
    "- `visualize_clusters` helps us inspect whether the clusters formed are meaningful, which can be useful during development or troubleshooting.\n",
    "\n",
    "### Building the RAPTOR tree\n",
    "\n",
    "This recursive process compresses large collections of documents into semantically meaningful summaries across multiple levels. It takes the original texts, embeds them, clusters them into thematic groups, summarizes each group, and repeats this process on the summaries. This builds a tree from raw content (level 0) up to abstracted summaries (level N).\n",
    "\n",
    "At every iteration, we track metadata like cluster origin, parent-child relations, and summary lineage. This hierarchy is crucial for later tasks like drill-down exploration, retrieval, or visualizing abstraction paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5pF9F6WX0Vou"
   },
   "outputs": [],
   "source": [
    "def build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n",
    "    # Dictionary to store each level's DataFrame (text, embeddings, clusters, metadata)\n",
    "    results = {}\n",
    "    # Start by extracting raw text (in case some are AIMessage objects)\n",
    "    current_texts = [extract_text(text) for text in texts]\n",
    "    # Initialize metadata for original texts — level 0, no parents, labeled as \"original\"\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "\n",
    "    # Loop over levels to progressively build the tree from specific (level 0) to abstract (level N)\n",
    "    for level in range(1, max_levels + 1):\n",
    "        logging.info(f\"Processing level {level}\")\n",
    "\n",
    "        # Step 1: Convert texts into embedding vectors\n",
    "        embeddings = embed_texts(current_texts)\n",
    "\n",
    "        # Step 2: Define number of clusters — max 10 or half the input size\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "\n",
    "        # Step 3: Cluster the embeddings to group them into thematic units\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "\n",
    "        # Step 4: Save this level's data into a structured DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "\n",
    "        results[level-1] = df  # Store this level's data\n",
    "\n",
    "        summaries = []  # To hold new summaries (higher-level concepts)\n",
    "        new_metadata = []  # Metadata for the next level of summaries\n",
    "\n",
    "        # Step 5: For each cluster, summarize its content and record lineage\n",
    "        for cluster in df['cluster'].unique():\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "\n",
    "            # Summarize the grouped texts into a single higher-level abstraction\n",
    "            summary = summarize_texts(cluster_texts)\n",
    "            summaries.append(summary)\n",
    "\n",
    "            # Build metadata that connects this summary to its child documents\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n",
    "                \"id\": f\"summary_{level}_{cluster}\"\n",
    "            })\n",
    "\n",
    "        # Prepare for next level: summaries become new input texts\n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata\n",
    "\n",
    "        # Stop if only one summary is left — no more meaningful abstraction possible\n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            logging.info(f\"Stopping at level {level} as we have only one summary\")\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcx_7bKH0VcG"
   },
   "source": [
    "This is the core logic that builds the RAPTOR architecture layer by layer — it constructs a hierarchical tree where each level is a progressively more abstract representation of the data below it.\n",
    "\n",
    "At level 0, we just have the original raw text chunks. Each subsequent level embeds those texts into vectors, clusters them into topic groups, summarizes each group into a single node, and continues upward. Each summary node tracks which inputs it represents via metadata. This metadata is essential for backtracking or navigating the tree later — e.g., reconstructing all children from a parent, or tracing the lineage of a summary.\n",
    "\n",
    "- It tracks each document and summary across levels using metadata, enabling easy traversal up and down the hierarchy.\n",
    "- Embeddings are created only for the current working set of texts, allowing each clustering step to reflect only local semantic similarity.\n",
    "- The Gaussian Mixture Model determines how documents cluster together, dynamically adapting to how many texts are in play.\n",
    "- The LLM summarization step distills each cluster into a higher-level concept, which then serves as input for the next iteration.\n",
    "- The tree terminates early if only one summary remains—indicating that the abstraction has converged to a single high-level idea.\n",
    "- The `results` dictionary effectively stores each \"layer\" of the tree, with DataFrames that hold the actual content, its representation, and metadata.\n",
    "\n",
    "This modular and recursive structure is what allows RAPTOR to build layered, explainable summaries from complex document collections.\n",
    "\n",
    "### Vectorstore construction\n",
    "Now that the full RAPTOR tree is built — with hierarchical summaries and metadata — we can turn the entire structure into a searchable knowledge base. This next step embeds every node in the tree (original texts and all summaries) into vector space and stores them inside a FAISS vector index.\n",
    "\n",
    "This enables efficient semantic retrieval: we can search for content by meaning, not just by keyword. And because each node still carries metadata like its abstraction level or parent lineage, we can filter our searches by depth, origin, or other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "65OlV8_y0VOs"
   },
   "outputs": [],
   "source": [
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree.\"\"\"\n",
    "    all_texts = []  # To collect all node texts (raw and summarized)\n",
    "    all_embeddings = []  # Corresponding embeddings\n",
    "    all_metadatas = []  # Metadata for lineage, level, etc.\n",
    "\n",
    "    # Loop over each level in the tree and extract its contents\n",
    "    for level, df in tree_results.items():\n",
    "        # Collect the actual text content (converted to string to be safe)\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])\n",
    "\n",
    "        # Handle embeddings in both list and ndarray formats\n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n",
    "        # Append the associated metadata\n",
    "        all_metadatas.extend(df['metadata'].tolist())\n",
    "\n",
    "    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n",
    "\n",
    "    # Create LangChain Document objects manually to ensure correct types\n",
    "    documents = [Document(page_content=str(text), metadata=metadata)\n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "\n",
    "    # Build and return the FAISS index from documents and embeddings\n",
    "    return FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zOJiDg20VCT"
   },
   "source": [
    "Now, the RAPTOR hierarchy is searchable. Each node is tied to its level and origin, enabling level-specific or lineage-based filtering.\n",
    "- Every node in the tree, regardless of level, gets embedded into a vector space.\n",
    "- Texts and their metadata are wrapped into standardized Document objects.\n",
    "- FAISS — a high-performance similarity search library — indexes all vectors efficiently.\n",
    "- The result is a powerful vectorstore where we can:\n",
    "  - Search across the whole tree for semantically similar ideas\n",
    "  - Filter results by level, origin cluster, or summary ID\n",
    "  - Retrieve abstract summaries or drill down into specifics\n",
    "\n",
    "### Hierarchical tree traversal retrieval\n",
    "The RAPTOR tree is not just for summarization — it’s also designed for multi-level semantic retrieval. Instead of flattening the tree and searching all documents equally, this method queries the tree from the top down, traversing through its summary nodes. This way, we start with high-level concepts and drill down only into the most relevant clusters.\n",
    "\n",
    "This approach has a number of benefits:\n",
    "- Improves precision by narrowing the search to semantically aligned paths.\n",
    "- Leverages the tree’s metadata (like levels and parent-child relationships).\n",
    "- Allows retrieval to dynamically mix abstract summaries and detailed documents.\n",
    "\n",
    "In the function below, we:\n",
    "- Embed the user query once.\n",
    "- Begin searching at the highest abstraction level.\n",
    "- Recursively follow `child_ids` of top-level matches into deeper levels.\n",
    "- Collect documents across levels and return a relevance-ranked list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Uyze9eZj0Uvr"
   },
   "outputs": [],
   "source": [
    "def tree_traversal_retrieval(query: str, vectorstore: FAISS, k: int = 3) -> List[Document]:\n",
    "    \"\"\"Perform tree traversal retrieval.\"\"\"\n",
    "    # Convert the query into its embedding form once\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Recursively retrieve documents by traversing the RAPTOR tree downward.\n",
    "        If parent_ids are specified, limits the search to children of those nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        if parent_ids:\n",
    "            # Filter search to only nodes at this level and within the parent lineage\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n",
    "            )\n",
    "        else:\n",
    "            # First level search — no filtering by parent\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level\n",
    "            )\n",
    "\n",
    "        # Base case: if we have reached level 0 or found no matches, return what we have\n",
    "        if not docs or level == 0:\n",
    "            return docs\n",
    "\n",
    "        # Otherwise, go one level deeper by collecting child document IDs\n",
    "        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n",
    "        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n",
    "\n",
    "        # Recursively collect results from lower level nodes\n",
    "        child_docs = retrieve_level(level - 1, child_ids)\n",
    "        # Combine current level docs with those retrieved below\n",
    "        return docs + child_docs\n",
    "\n",
    "    # Start from the highest available level in the tree\n",
    "    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n",
    "    return retrieve_level(max_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WYZYXiW0UQF"
   },
   "source": [
    "Instead of just searching over a flat document list, this retrieval strategy respects the hierarchy of our RAPTOR tree:\n",
    "- The top-level summaries act as conceptual filters. They catch broad semantic matches.\n",
    "- Once top-level nodes are selected, the algorithm descends into their child summaries or documents, limiting the search space to relevant subtrees.\n",
    "- Each recursive pass refines the relevance by tracing semantic lineage, from abstract → specific.\n",
    "- This method returns a blend of high-level and low-level nodes, all connected through the tree’s structure.\n",
    "\n",
    "This hierarchical approach is especially useful when dealing with long documents, large collections, or any scenario where abstract themes and detailed evidence both matter. It is a key part of what makes RAPTOR not just a summarizer, but a retrieval-first architecture.\n",
    "\n",
    "\n",
    "### Create contextual retriever\n",
    "Retrieval alone can yield overly broad results. To tighten this up, RAPTOR introduces a `ContextualCompressionRetriever`, which enhances relevance and conciseness by post-processing the retrieved documents with an LLM.\n",
    "This component works like a semantic sieve:\n",
    "- It takes in raw retrieved content.\n",
    "- Runs it through a compression chain powered by a prompt + LLM.\n",
    "- Outputs only the most contextually relevant snippets, filtered through the lens of the user’s question.\n",
    "\n",
    "This is especially useful when:\n",
    "- We are retrieving from large, verbose documents.\n",
    "- We want faster downstream inference (e.g., in a QA chain).\n",
    "- We need higher precision in multi-level chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5INS6RYD02Lq"
   },
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    \"\"\"\n",
    "    Create a retriever with contextual compression.\n",
    "    It wraps the FAISS-based retriever in a ContextualCompressionRetriever, which uses an LLM to extract only the parts of each document that are relevant to the current query.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "\n",
    "    # Step 1: Create a basic retriever from the vectorstore\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Step 2: Define a prompt that instructs the LLM to extract only what's relevant\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Wrap the LLM with a chain that knows how to use the prompt\n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "\n",
    "    # Step 4: Create the contextual compression retriever\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,  # LLM-based filter\n",
    "        base_retriever=base_retriever  # Initial document retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWNXy5MK0UKC"
   },
   "source": [
    "This function upgrades a basic vector retriever into a smarter one that thinks before returning documents:\n",
    "- We first wrap the FAISS vectorstore into a standard retriever object. This handles the initial similarity-based filtering based on the user’s query.\n",
    "- Then we define a prompt template that tells the LLM what to do: from the retrieved context, extract only the parts that help answer the query.\n",
    "- This prompt is wrapped into a LangChain `LLMChainExtractor`, which creates a callable component that compresses documents into shorter, more relevant chunks.\n",
    "- Finally, we glue the extractor and retriever together into a `ContextualCompressionRetriever`.\n",
    "\n",
    "At runtime, the retriever:\n",
    "1. Performs a similarity search to get top-k document chunks.\n",
    "2. Forwards those chunks along with the query to the LLM extractor.\n",
    "3. Returns only the filtered, compressed pieces for downstream use (e.g., summarization, QA, synthesis).\n",
    "\n",
    "This LLM-powered compression is essential when working with noisy or layered data (like RAPTOR trees), helping our agent stay focused and efficient.\n",
    "\n",
    "### Define hierarchical retrieval\n",
    "RAPTOR doesn’t just flatten a knowledge tree into embeddings—it respects and leverages hierarchy. So when it comes to retrieval, we don’t just search from the bottom-up or top-down arbitrarily—we do both strategically.\n",
    "\n",
    "This retrieval strategy starts at the highest level of abstraction (summaries), working down to the original source texts through lineage metadata. At each level, it:\n",
    "- Retrieves documents matching the query.\n",
    "- Traces the metadata of retrieved summaries to their child documents in lower levels.\n",
    "- Dynamically updates the query to reflect the children of relevant results.\n",
    "- Collects documents across levels to build a layered, comprehensive context set.\n",
    "\n",
    "This design is powerful for semantic search where themes span different depths—from broad overviews to specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zZ5n39Rm07Kc"
   },
   "outputs": [],
   "source": [
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []  # Store results from all levels\n",
    "\n",
    "    # Traverse levels from top (abstract summaries) to bottom (original docs)\n",
    "    for level in range(max_level, -1, -1):\n",
    "        # Retrieve documents at this level with the current query\n",
    "        level_docs = retriever.invoke(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level  # Ensure level matches\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "\n",
    "        # If documents found and more levels exist below, retrieve their children from the next level down\n",
    "        if level_docs and level > 0:\n",
    "            # Gather all child IDs from retrieved documents' metadata\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "\n",
    "            # Refine the query to guide the next retrieval level\n",
    "            if child_ids:  # Only modify query if there are valid child IDs\n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query  # Append ID-based constraint to the query\n",
    "\n",
    "    return all_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynRYELxT09B7"
   },
   "source": [
    "This function traverses the RAPTOR tree from abstract to concrete, collecting a blend of documents relevant to the query at multiple layers of semantic depth.\n",
    "- It begins at the highest level (i.e., summaries), filtering documents based on their metadata level.\n",
    "- Retrieved summaries are inspected for `child_ids`, which are metadata pointers to lower-level documents they represent.\n",
    "- If child IDs are found, they are embedded into the query string for the next level down—effectively scoping retrieval to parts of the tree already considered semantically relevant.\n",
    "- This cycle repeats until it hits level 0 (raw text chunks), gathering documents at each step.\n",
    "\n",
    "The end result is a multi-level result set that combines high-level abstractions with supporting details from the lower levels of the tree. This helps maintain both semantic alignment and contextual grounding in our final outputs. This pattern is especially useful when:\n",
    "- We want to answer questions that require high-level synthesis and supporting evidence.\n",
    "- We want to visualize or explain how an abstract conclusion was derived from concrete inputs.\n",
    "- We are building chains that depend on hierarchical depth (e.g., summarization + QA).\n",
    "\n",
    "By traversing intelligently rather than exhaustively, RAPTOR avoids noisy results and enables meaningful semantic zooming during retrieval.\n",
    "\n",
    "\n",
    "### RAPTOR query pipeline (Online pnference phase)\n",
    "This is where everything comes together. Once the tree is built and the retriever is set up, the `raptor_query` function acts as the end-to-end orchestration point for answering user questions. It coordinates all the moving parts:\n",
    "- Starts by running hierarchical retrieval to get semantically and structurally relevant documents.\n",
    "- Formats these results with metadata for transparency and debugging.\n",
    "- Uses a context-aware prompt to query the LLM for a grounded answer.\n",
    "- Returns a structured payload including the answer, the exact context used, and metadata-rich document lineage.\n",
    "\n",
    "This pattern allows for not just answer generation, but also deep introspection of how and why the model answered the way it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Svl29dzh09cn"
   },
   "outputs": [],
   "source": [
    "def raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval and LLM-based answer generation.\"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")\n",
    "\n",
    "    # Step 1: Retrieve relevant documents across the RAPTOR hierarchy\n",
    "    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n",
    "\n",
    "    # Step 2: Format the retrieved docs with metadata for inspection\n",
    "    doc_details = []\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        doc_details.append({\n",
    "            \"index\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"level\": doc.metadata.get('level', 'Unknown'),\n",
    "            \"similarity_score\": doc.metadata.get('score', 'N/A')  # May be unavailable\n",
    "        })\n",
    "\n",
    "    # Step 3: Combine the content into a single context string\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Step 4: Use the LLM to generate an answer based on the context\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context, please answer the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    answer = chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "    logging.info(\"Query processing completed\")\n",
    "\n",
    "    # Step 5: Return structured results\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": doc_details,\n",
    "        \"num_docs_retrieved\": len(relevant_docs),\n",
    "        \"context_used\": context,\n",
    "        \"answer\": answer,\n",
    "        \"model_used\": llm.model_name,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # Show first 100 characters\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")\n",
    "        print()\n",
    "\n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['context_used'])\n",
    "\n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'].content)\n",
    "\n",
    "    print(f\"\\nModel Used: {result['model_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTm0_EqH1IkW"
   },
   "source": [
    "This is the final pipeline that connects all parts of RAPTOR into a working system that can handle real-world queries. Here is what it is doing behind the scenes:\n",
    "- It begins by invoking the hierarchical retriever, which explores the RAPTOR tree from top to bottom, surfacing documents whose lineage is aligned with the query.\n",
    "- Each document's metadata—such as its abstraction level, origin cluster, or similarity score—is preserved. This allows us to trace answers back to specific sources or thematic clusters.\n",
    "- The content of all retrieved docs is stitched together into a single context block.\n",
    "- This context is passed into a prompt template and executed via a LangChain chain, producing an answer that is grounded in the retrieved material.\n",
    "- All outputs are returned in a structured dictionary, making the results easy to inspect, display, or log.\n",
    "- A helper function is included to pretty-print everything.\n",
    "\n",
    "This method is a demonstration of RAPTOR’s hybrid strengths: structured semantics and LLM reasoning, working together.\n",
    "\n",
    "### Example usage\n",
    "\n",
    "#### Load and preprocess the source document\n",
    "Before RAPTOR can summarize or retrieve anything, we first need to load and split the input content. In this example, we use a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "k0HCDSdy1IU0"
   },
   "outputs": [],
   "source": [
    "# Load raw data from a PDF file\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Use a PDF loader to extract document chunks\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Extract plain text from the document objects\n",
    "texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mlQs1XP1HqG"
   },
   "source": [
    "Here, we read a PDF document into memory and parse it into a list of raw text chunks. These chunks serve as the input to the RAPTOR tree builder. `PyPDFLoader` from LangChain handles PDF parsing under the hood, splitting by pages or layout regions depending on the configuration.\n",
    "\n",
    "\n",
    "#### Build the RAPTOR tree and components\n",
    "Now we build the RAPTOR components step-by-step:\n",
    "- First, we recursively summarize and cluster the document collection into a hierarchical tree.\n",
    "- Then we store all nodes (texts and summaries) into a FAISS vectorstore for fast retrieval.\n",
    "- Finally, we create a contextual retriever that uses LLM-based compression to extract only the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GLpwD_x-1c6E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 23:08:45,926 - INFO - Processing level 1\n",
      "2025-06-05 23:08:45,930 - INFO - Embedding 10 texts\n",
      "2025-06-05 23:08:46,907 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:08:47,070 - INFO - Performing clustering with 5 clusters\n",
      "2025-06-05 23:08:55,935 - INFO - Summarizing 1 texts\n",
      "2025-06-05 23:08:59,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:08:59,527 - INFO - Summarizing 2 texts\n",
      "2025-06-05 23:09:02,689 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:02,694 - INFO - Summarizing 2 texts\n",
      "2025-06-05 23:09:05,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:05,038 - INFO - Summarizing 3 texts\n",
      "2025-06-05 23:09:09,741 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:09,749 - INFO - Summarizing 2 texts\n",
      "2025-06-05 23:09:11,689 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:11,694 - INFO - Processing level 2\n",
      "2025-06-05 23:09:11,698 - INFO - Embedding 5 texts\n",
      "2025-06-05 23:09:11,995 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:12,161 - INFO - Performing clustering with 2 clusters\n",
      "2025-06-05 23:09:15,186 - INFO - Summarizing 3 texts\n",
      "2025-06-05 23:09:17,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:17,527 - INFO - Summarizing 2 texts\n",
      "2025-06-05 23:09:19,469 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:19,475 - INFO - Processing level 3\n",
      "2025-06-05 23:09:19,477 - INFO - Embedding 2 texts\n",
      "2025-06-05 23:09:19,778 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:19,800 - INFO - Performing clustering with 1 clusters\n",
      "2025-06-05 23:09:21,268 - INFO - Summarizing 2 texts\n",
      "2025-06-05 23:09:23,771 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:23,779 - INFO - Embedding 1 texts\n",
      "2025-06-05 23:09:24,082 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:24,089 - INFO - Stopping at level 3 as we have only one summary\n",
      "2025-06-05 23:09:24,097 - INFO - Building vectorstore with 18 texts\n",
      "2025-06-05 23:09:24,590 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:24,823 - INFO - Loading faiss with AVX2 support.\n",
      "2025-06-05 23:09:24,847 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-06-05 23:09:24,863 - INFO - Creating contextual compression retriever\n"
     ]
    }
   ],
   "source": [
    "# Build the RAPTOR tree from the extracted texts\n",
    "tree_results = build_raptor_tree(texts)\n",
    "\n",
    "# Create a vectorstore to support similarity search across all tree levels\n",
    "vectorstore = build_vectorstore(tree_results)\n",
    "\n",
    "# Create retriever that compresses the retrieved context using the LLM\n",
    "retriever = create_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUAn7JGc1dUF"
   },
   "source": [
    "Under the hood, this step recursively abstracts the input into thematic layers. Each node contains metadata about its origin, allowing precise traversal and retrieval. The FAISS index ensures similarity search is fast, even across many levels. The contextual retriever will refine search results using the language model’s reasoning.\n",
    "\n",
    "\n",
    "#### Run a query through the full RAPTOR pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UEn3EMcQ1jpk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 23:09:24,892 - INFO - Processing query: What is the greenhouse effect?\n",
      "2025-06-05 23:09:25,076 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:26,681 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:27,973 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:28,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:29,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:30,017 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:31,045 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:31,917 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:33,091 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:34,014 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:34,213 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:35,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:36,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:37,594 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:39,130 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:39,439 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:40,974 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:42,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:43,022 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:43,635 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:45,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 23:09:45,386 - INFO - Query processing completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the greenhouse effect?\n",
      "\n",
      "Number of documents retrieved: 16\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methan...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 2:\n",
      "    Content: The provided context does not include a definition or explanation of the greenhouse effect. Therefor...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 3:\n",
      "    Content: The provided context does not contain any information specifically addressing the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 4:\n",
      "    Content: The context provided does not contain any information about the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 5:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), met...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 6:\n",
      "    Content: The provided context does not include a definition or explanation of the greenhouse effect. Therefor...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 7:\n",
      "    Content: The context provided does not include information specifically about the greenhouse effect. Therefor...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 8:\n",
      "    Content: The provided context does not contain any information about the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 9:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), tra...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 10:\n",
      "    Content: The provided context does not contain any information specifically about the greenhouse effect. Ther...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 11:\n",
      "    Content: The provided context does not contain information specifically explaining the greenhouse effect. The...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 12:\n",
      "    Content: The context provided does not contain any information related to the greenhouse effect. Therefore, t...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 13:\n",
      "    Content: The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), met...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 14:\n",
      "    Content: The provided context does not contain specific information about the greenhouse effect. Therefore, n...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 15:\n",
      "    Content: The provided context does not contain any information specifically describing the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 16:\n",
      "    Content: The provided context does not contain relevant information about the greenhouse effect....\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "The greenhouse effect is the process by which greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun in the atmosphere. This effect is essential for life on Earth, as it keeps the planet warm enough to support life. However, human activities have intensified this natural process, leading to a warmer climate.\n",
      "\n",
      "The provided context does not include a definition or explanation of the greenhouse effect. Therefore, there is no relevant information available to answer the question about what the greenhouse effect is.\n",
      "\n",
      "The provided context does not contain any information specifically addressing the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The context provided does not contain any information about the greenhouse effect.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "The provided context does not include a definition or explanation of the greenhouse effect. Therefore, there is no relevant information extracted for answering the question.\n",
      "\n",
      "The context provided does not include information specifically about the greenhouse effect. Therefore, no relevant information can be extracted to answer the question.\n",
      "\n",
      "The provided context does not contain any information about the greenhouse effect.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "The provided context does not contain any information specifically about the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The provided context does not contain information specifically explaining the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The context provided does not contain any information related to the greenhouse effect. Therefore, there is no relevant information to extract for answering the question.\n",
      "\n",
      "The greenhouse effect is a natural process where greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, keeping the planet warm enough to support life. However, human activities have intensified this process, leading to a warmer climate.\n",
      "\n",
      "The provided context does not contain specific information about the greenhouse effect. Therefore, no relevant information can be extracted to answer the question.\n",
      "\n",
      "The provided context does not contain any information specifically describing the greenhouse effect.\n",
      "\n",
      "The provided context does not contain relevant information about the greenhouse effect.\n",
      "\n",
      "Generated Answer:\n",
      "The greenhouse effect is a natural process in which greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun in the atmosphere. This process is essential for maintaining a temperature that supports life on Earth. However, human activities have intensified this effect, leading to an increase in global temperatures and climate change.\n",
      "\n",
      "Model Used: gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "# Run an actual query through the full RAPTOR pipeline\n",
    "max_level = 3  # The deepest level available in the tree\n",
    "query = \"What is the greenhouse effect?\"\n",
    "\n",
    "# Get results from RAPTOR\n",
    "result = raptor_query(query, retriever, max_level)\n",
    "\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mOL0m2l1jLS"
   },
   "source": [
    "Here, we perform a real query using the system. It:\n",
    "- Starts retrieval from the highest summary level and walks down the tree.\n",
    "- Uses metadata lineage to trace back the documents that informed the answer.\n",
    "- Shows us which texts were considered relevant, where they came from in the hierarchy, and what the LLM ultimately answered based on that evidence.\n",
    "\n",
    "This gives us not just an answer, but also visibility into which parts of the hierarchy contributed to it. We can trace what documents were selected, at what level, how they were scored, and how they map to the original document."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
