{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDfr8L_qG9uv"
   },
   "source": [
    "# Evaluating RAG systems with DeepEval\n",
    "\n",
    "In RAG systems, the output quality depends not only on the language model but also on the relevance and quality of retrieved documents. To build trustworthy and robust RAG applications, we need ways to evaluate them beyond simple accuracy.\n",
    "\n",
    "This notebook walks through a practical setup for evaluating RAG outputs using the `deepeval` library. It focuses on measuring correctness, faithfulness to retrieved context, and contextual relevance of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lCULW9jnGzCD"
   },
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAAGmKJaHGVV"
   },
   "source": [
    "### Test correctness\n",
    "Let’s begin by checking if the model’s answer is actually factually correct compared to the expected answer. This is a basic and essential evaluation step — making sure the model tells the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LFTh9S4LHGJ8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-3.5-turbo-0125, strict=False, async_mod…</span></pre>\n"
      ],
      "text/plain": [
       "\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-3.5-turbo-0125, strict=False, async_mod…\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r",
       "\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17888971300910023\n"
     ]
    }
   ],
   "source": [
    "# Define a GEval metric for checking correctness\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",  # Name for logging or reports\n",
    "    model=\"gpt-3.5-turbo-0125\",  # Evaluation model\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,  # What the model should have said\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT  # What the model actually said\n",
    "    ],\n",
    "    evaluation_steps=[\n",
    "        # The prompt used internally to guide the LLM's judgment\n",
    "        \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define a test case: Ground truth vs prediction\n",
    "gt_answer = \"Madrid is the capital of Spain.\"  # This is the expected, factually correct output\n",
    "pred_answer = \"MadriD.\"  # This is what our RAG model produced - Incorrect casing and slightly vague\n",
    "\n",
    "# Construct the test case to evaluate\n",
    "test_case_correctness = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",  # The original question\n",
    "    expected_output=gt_answer,  # Ground truth answer\n",
    "    actual_output=pred_answer,  # Model-generated answer\n",
    ")\n",
    "\n",
    "# Evaluate the test case using the correctness metric\n",
    "correctness_metric.measure(test_case_correctness)\n",
    "print(correctness_metric.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_w7qnZtHFzI"
   },
   "source": [
    "Here, we define and run a factual correctness test on a single QA pair, comparing a predicted answer with a known correct answer. We sre using `GEval`, which wraps an LLM to judge correctness not just for exact matches, but for semantic and factual alignment. It's particularly powerful because it allows for flexibility in wording, while still penalizing factual errors. Even though \"MadriD\" is close to the correct answer, it might be rated lower due to issues with capitalization or ambiguous formatting — the scoring reflects how confidently the evaluator model thinks the answer is valid.\n",
    "\n",
    "This kind of evaluation is useful when we are building systems where correctness matters more than style or structure — like education, factual assistants, or research tools.\n",
    "\n",
    "\n",
    "### Test faithfulness\n",
    "Even when an answer is factually correct, it might still be unfaithful — meaning it includes information that wasn't supported by the retrieved context. For RAG systems, we want to ensure that the model doesn't hallucinate facts or \"make things up\" beyond what was retrieved. This step checks whether the answer remains grounded in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o7R9sbIsHFoP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-3.5-turbo-0125, strict=False, async_mode=True)…</span></pre>\n"
      ],
      "text/plain": [
       "\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-3.5-turbo-0125, strict=False, async_mode=True)…\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r",
       "\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Define a simple test case: question and its retrieved context\n",
    "question = \"what is 3+3?\"\n",
    "context = [\"6\"]  # Simulated retrieved documents — in this case, a simple answer\n",
    "generated_answer = \"6\"  # Model output (correct and consistent)\n",
    "\n",
    "# Instantiate the faithfulness metric to check if the answer aligns with context\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    threshold=0.7,  # Score below this would flag the answer as unfaithful\n",
    "    model=\"gpt-3.5-turbo-0125\",  # LLM that performs the evaluation\n",
    "    include_reason=False  # Skip explanations for now\n",
    ")\n",
    "\n",
    "# Package everything into a test case\n",
    "test_case = LLMTestCase(\n",
    "    input = question,\n",
    "    actual_output=generated_answer,\n",
    "    retrieval_context=context\n",
    "\n",
    ")\n",
    "\n",
    "# Evaluate the model's answer for alignment with the context\n",
    "faithfulness_metric.measure(test_case)\n",
    "# Print the faithfulness score (ranges from 0 to 1)\n",
    "print(faithfulness_metric.score)\n",
    "# Optional: print reason if include_reason=True was set\n",
    "#print(faithfulness_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TUVmgnYHFdJ"
   },
   "source": [
    "This code sets up a small test to validate whether a model's output is faithful to the retrieved context. The answer is `\"6\"`, which aligns directly with the context (also just `\"6\"`), so the score is expected to be high.\n",
    "\n",
    "Technically, the `FaithfulnessMetric` uses LLM to compare the answer against the supporting documents. It's not checking if the answer is correct in the abstract — it checks whether that answer is justified based on the retrieval. This is a critical distinction in RAG systems, where correctness alone isn't enough — faithfulness ensures the model is not freelancing.\n",
    "\n",
    "This metric is especially useful when retrieved content includes partial or ambiguous information and we want to catch hallucinations or unsupported reasoning in the generated answer.\n",
    "\n",
    "\n",
    "### Test contextual relevancy\n",
    "Even if a model produces a fluent and factually correct answer, it doesn’t necessarily mean the retrieved context played a role in shaping that output. In a RAG setup, we want the retrieval to be not only accurate but useful. This part tests whether the retrieved documents were genuinely helpful for generating the final answer — or whether the model essentially ignored them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uIzgveCdHFRv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-3.5-turbo-0125, strict=False, async_mo…</span></pre>\n"
      ],
      "text/plain": [
       "\r",
       "\u001b[2KEvent loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-3.5-turbo-0125, strict=False, async_mo…\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r",
       "\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "The score is 0.33 because the majority of the statements in the retrieval context are not relevant to the input about shoe fitting, but there is one statement that directly addresses the issue.\n"
     ]
    }
   ],
   "source": [
    "# Define a sample generated answer\n",
    "actual_output = \"then go somewhere else.\"\n",
    "# Define the context that was retrieved by the RAG system\n",
    "retrieval_context = [\"this is a test context\",\"mike is a cat\",\"if the shoes don't fit, then go somewhere else.\"]\n",
    "# Define the expected answer based on relevant content\n",
    "gt_answer = \"if the shoes don't fit, then go somewhere else.\"\n",
    "\n",
    "# Create the contextual relevancy metric\n",
    "relevance_metric = ContextualRelevancyMetric(\n",
    "    threshold=1,  # Maximum score threshold — very strict in this example\n",
    "    model=\"gpt-3.5-turbo-0125\",  # Evaluation model used to judge relevance\n",
    "    include_reason=True  # Ask the model to explain the score it gives\n",
    ")\n",
    "\n",
    "# Package everything into a DeepEval test case\n",
    "relevance_test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,  # What the model said\n",
    "    retrieval_context=retrieval_context,  # Retrieved context the model had access to\n",
    "    expected_output=gt_answer,  # What we expected the model to say\n",
    "\n",
    ")\n",
    "\n",
    "# Evaluate how relevant the retrieved context was to the output\n",
    "relevance_metric.measure(relevance_test_case)\n",
    "# Print the relevancy score (0–1, higher is better)\n",
    "print(relevance_metric.score)\n",
    "# Print the reason from the evaluator (optional)\n",
    "print(relevance_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eokYxHCWTMzV"
   },
   "source": [
    "This test checks if the generated answer is grounded in the actual retrieved passages. The key here is not just whether the answer is “good” — it is whether the answer was influenced by the retrieval.\n",
    "\n",
    "In this case, only one of the three context chunks is actually relevant to the answer. The rest are noise. The metric relies on an LLM evaluator to assess whether the useful context was present and sufficiently aligned with the model’s output.\n",
    "\n",
    "This helps evaluate whether retrieval is pulling useful content or just filler — a critical distinction in high-stakes or noisy RAG deployments where irrelevant documents might drown out the real signal. We are not testing the generation here — we are testing the retrieval pipeline’s contribution to the final output.\n",
    "\n",
    "\n",
    "### Evaluating multiple test cases with multiple metrics\n",
    "Once we have defined individual metrics like correctness, faithfulness, and contextual relevance, it becomes useful to scale up our evaluation. Instead of testing cases one by one, deepeval allows us to run multiple test cases through multiple metrics in a single evaluation step.\n",
    "\n",
    "This part showcases how to do that — an essential feature when we want to benchmark our RAG system across several dimensions at once.\n",
    "\n",
    "#### Add another test case\n",
    "Before running a batch evaluation, we create another reusable test case — this time a basic factual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qZ8Hfxb0HaDg"
   },
   "outputs": [],
   "source": [
    "# Define a second test case to evaluate\n",
    "new_test_case = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",  # Original question\n",
    "    expected_output=\"Madrid is the capital of Spain.\",  # Ground truth\n",
    "    actual_output=\"MadriD.\",  # Model's generated output (formatting is slightly off)\n",
    "    retrieval_context=[\"Madrid is the capital of Spain.\"]  # What the model had access to\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2bPbm7vVhHj"
   },
   "source": [
    "This test checks how well the model answers a factual question and whether it stays grounded in the provided context. Although this case seems simple, it helps validate multiple evaluation layers simultaneously: is the answer accurate? Is it grounded in the context? Is the context even relevant?\n",
    "\n",
    "#### Run multi-metric evaluation\n",
    "Now that we have two test cases and three metrics defined (correctness, faithfulness, relevance), we evaluate all combinations at once. This is where `deepeval` really shines — we can get structured, consistent evaluation across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAEn4TAzHE73"
   },
   "outputs": [],
   "source": [
    "# Run all three metrics on both test cases in one batch\n",
    "evaluate(\n",
    "    test_cases=[relevance_test_case, new_test_case],  # The cases we defined\n",
    "    metrics=[correctness_metric, faithfulness_metric, relevance_metric]  # Metrics we want to apply\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vum8J1-aHEwr"
   },
   "source": [
    "This code runs all provided test cases against the listed metrics, using the appropriate LLM behind each metric. Internally, `deepeval` will execute the evaluation logic for each metric, generate a score, and optionally include an explanation or rationale.\n",
    "\n",
    "The real strength here is consistency — every test case is measured against the same standards, enabling meaningful comparison and deeper insight into where our RAG system may be strong or weak.\n",
    "\n",
    "This also scales beautifully: if we are testing dozens or hundreds of cases, we only need to define them once and plug them into this batch call.\n",
    "\n",
    "### Utility: Batch creation of test cases from lists\n",
    "When we are evaluating many inputs at once — especially during QA pipelines or automated benchmarks — creating `LLMTestCase` objects manually becomes tedious and error-prone. This helper function solves that by turning raw data into a clean list of test cases, ready for evaluation.\n",
    "\n",
    "It accepts four parallel lists:\n",
    "- Questions (inputs)\n",
    "- Ground truth answers (expected outputs)\n",
    "- Model outputs (generated answers)\n",
    "- Retrieved documents (context retrieved for each question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EfQoaGwcHk7f"
   },
   "outputs": [],
   "source": [
    "# Utility function to generate a list of LLMTestCase objects for batch evaluation\n",
    "def create_deep_eval_test_cases(questions, gt_answers, generated_answers, retrieved_documents):\n",
    "    return [\n",
    "        LLMTestCase(\n",
    "            input=question,  # Original user query\n",
    "            expected_output=gt_answer,  # Ground truth answer\n",
    "            actual_output=generated_answer,  # Output generated by the model\n",
    "            retrieval_context=retrieved_document  # List of retrieved document snippets\n",
    "        )\n",
    "        for question, gt_answer, generated_answer, retrieved_document in zip(\n",
    "            questions, gt_answers, generated_answers, retrieved_documents\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Hm9K2a3HmaS"
   },
   "source": [
    "This function loops over four input sequences simultaneously using `zip()` — one for each element of a test case — and constructs a list of `LLMTestCase` objects. Each test case encapsulates everything needed to evaluate a single example under correctness, faithfulness, or contextual relevance metrics.\n",
    "\n",
    "This is especially useful when:\n",
    "- We are running evaluation across a large dataset.\n",
    "- We already have model outputs stored from a previous run.\n",
    "- We are integrating deep evaluation into automated pipelines or scheduled batch jobs.\n",
    "\n",
    "It turns messy raw data into a structured format ready for `deepeval`'s `evaluate()` function — helping ensure our RAG evaluation remains scalable, testable, and consistent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
