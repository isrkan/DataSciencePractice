{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7mpNZwWSUi5"
   },
   "source": [
    "# Fusion retrieval in document search\n",
    "\n",
    "This notebook demonstrates the implementation of fusion retrieval in a document search system. Traditional retrieval systems typically use either semantic search (vector-based) or keyword-based search (BM25). However, each approach has its limitations:\n",
    "- Vector search (semantic search) captures the meanings and concepts of documents, but can be less precise when exact matches are required.\n",
    "- BM25 search (keyword-based) is great for finding exact keyword matches, but it doesn't account for semantic meaning or context.\n",
    "\n",
    "The goal is to combine vector-based similarity search with keyword-based BM25 retrieval to improve the overall accuracy and relevance of document retrieval. By leveraging both semantic understanding (from the vector search) and exact keyword matching (from BM25), the system provides a more robust and comprehensive retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jjCe4ZgLSdFW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_OKdmY1Srud"
   },
   "source": [
    "### Loading the PDF\n",
    "We will use `PyPDFLoader` to extract text from the PDF. It reads the PDF page by page and stores the extracted text in a list of document objects, where each document contains the content of a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qmD4kJxgSpuG"
   },
   "outputs": [],
   "source": [
    "# Path to the PDF document\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Load PDF documents\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIrkkn-0TgPK"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Splitting documents into chunks\n",
    "We use the `RecursiveCharacterTextSplitter` to split the document into smaller chunks. This is ideal when working with large documents to make them manageable for embedding generation and for easier retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H8I82alLTgev"
   },
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnYo-6ByTgue"
   },
   "source": [
    "we are splitting the document into chunks of size 1000 characters with 200 characters of overlap between chunks.\n",
    "- `chunk_size` makes it more manageable for indexing and retrieval.\n",
    "- chunk_overlap` ensures that the context is preserved when the text is split. This helps maintain the flow of information between chunks.\n",
    "- `length_function` tells the splitter to calculate the length of the chunks based on the number of characters, ensuring that the chunks are exactly the specified size.\n",
    "\n",
    "#### Replace tabs with spaces\n",
    "In many cases, PDFs may contain tab characters (`\\t`) that were used for indentation but aren't necessary for the final processed text. We will replace these with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2bi679NaTg8b"
   },
   "outputs": [],
   "source": [
    "# Replace tab characters ('\\t') with spaces in the document chunks\n",
    "for doc in texts:\n",
    "    doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr9q7iFgVc3m"
   },
   "source": [
    "Now, the document is ready for both semantic and keyword-based search.\n",
    "\n",
    "### Creating the vector store\n",
    "Once the text is cleaned and processed, we can create embeddings for each of the chunks using OpenAI API. These embeddings represent the meaning of the text in a high-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6mI5fB1kVdED"
   },
   "outputs": [],
   "source": [
    "# Initializes the OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_1rVfWCVdPZ"
   },
   "source": [
    "We will also use FAISS to efficiently store and index the embeddings, which allows us to perform similarity search and query efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JmGK3RY7Vdch"
   },
   "outputs": [],
   "source": [
    "# Create vector store using FAISS\n",
    "vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9srr_eXWGZ2"
   },
   "source": [
    "Here, we create a FAISS vector store by:\n",
    "- Generating embeddings for each chunk of text.\n",
    "- Storing the embeddings in FAISS, which allows fast similarity search later.\n",
    "\n",
    "This function automatically creates a flat (brute-force) index by default.\n",
    "\n",
    "### Creating the BM25 index for keyword-based retrieval\n",
    "Next, we will create a BM25 index using the BM25Okapi class from the `rank_bm25` module. BM25 is a algorithm for ranking documents based on keyword matching. The main idea behind BM25 is to score documents based on how well they match the query words, adjusting for factors like term frequency and document length.\n",
    "\n",
    "Before we can create the BM25 index, we need to tokenize the text of each document by splitting it into words. This will allow BM25 to process the documents efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jQNASgXsWHH5"
   },
   "outputs": [],
   "source": [
    "# Tokenize each document by splitting on whitespace\n",
    "tokenized_docs = [doc.page_content.split() for doc in texts]\n",
    "\n",
    "# Create BM25 index from the tokenized documents\n",
    "bm25 = BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uRhWi0-WG5X"
   },
   "source": [
    "When we create a BM25 index, it pre-processes the documents in our collection (corpus) and creates a structure that allows fast retrieval based on the frequency of terms in the documents.\n",
    "\n",
    "Once the BM25 index is created, the BM25 object can be used to score documents based on a query. Specifically, you can use the `get_scores()` method to calculate the BM25 scores for all documents in the index given a query.\n",
    "\n",
    "### Fusion retrieval: Combining vector and BM25 retrieval\n",
    "The key part of this system is the fusion retrieval method, which combines the results from the vector-based search and the BM25-based search. This method performs both searches and ranks documents based on a weighted combination of the scores from each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "W4aAkHU5mJny"
   },
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = \"What are the impacts of climate change on the environment?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypFMsSEMmKpH"
   },
   "source": [
    "#### Vector-based search\n",
    "We begin by performing a vector-based search using the FAISS vector store.\n",
    "\n",
    "- First, we retrieve all documents from the vector store.\n",
    "- Then, we proceed to perform a vector-based search for the query.This search finds the documents that are semantically most similar to the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mQjaVwI_XHFs"
   },
   "outputs": [],
   "source": [
    "# 1. Get all documents from the vectorstore\n",
    "all_docs = vector_store.similarity_search(\"\", k=vector_store.index.ntotal)\n",
    "\n",
    "# Perform vector-based search\n",
    "vector_results = vector_store.similarity_search_with_score(query, k=len(all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O2_7NpGXHUP"
   },
   "source": [
    "1. Here, we are calling the `similarity_search()` method on the vectorstore with an empty query (`\"\"`), effectively retrieving all the documents in the vector store. We specify `k=vectorstore.index.ntotal` to fetch all the documents stored in the FAISS index. This gives us the complete set of documents available for retrieval.\n",
    "2. Here, we use the `similarity_search_with_score()` method to search for the most relevant documents based on their vector embeddings and return the top documents along with their similarity scores. These scores represent how semantically similar each document is to the query. The argument `k=len(all_docs)` is used to retrieve the same number of documents as there are in the entire vector store.\n",
    "  - In the context of fusion retrieval, we need to consider every document in the vector store to make sure we can later combine the results from both vector-based search (using FAISS) and BM25 search. By retrieving all documents, we ensure that both search methods (vector-based and BM25) can process the same set of documents, allowing for proper fusion of their results.\n",
    "\n",
    "#### BM25-based search\n",
    "Alongside the vector search, we also perform a BM25-based search using the previously created BM25 index. The BM25 algorithm works by matching the query terms with the terms in the documents, using statistical measures like term frequency (TF) and inverse document frequency (IDF) to rank the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vl-8otTyXHhl"
   },
   "outputs": [],
   "source": [
    "# Perform BM25 search\n",
    "bm25_scores = bm25.get_scores(query.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szq2JWFwXHvX"
   },
   "source": [
    "The query is first split into individual terms using the `split()` method. Then, the `get_scores()` method from the BM25 index is called to compute the BM25 scores for each document. These scores are based on how well the query terms match the words in each document, adjusted for factors like term frequency and document length.\n",
    "\n",
    "#### Normalizing the scores\n",
    "To combine the results from both the vector-based search and the BM25-based search, we first normalize the scores from both methods. This ensures that the scores are on the same scale, making them comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sj1doC2FYbzy"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "\n",
    "# Normalize vector scores\n",
    "vector_scores = np.array([score for _, score in vector_results])\n",
    "vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "\n",
    "# Normalize BM25 scores\n",
    "bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcKyL4tgYcBP"
   },
   "source": [
    "This is done by subtracting the minimum score and dividing by the range (max - min). Normalization ensures that the scores are comparable even though they might come from different scoring systems. `epsilon` is used to prevent a division by zero error in case the maximum and minimum values of the vector scores are identical (i.e., all the scores are the same).\n",
    "\n",
    "#### Combining the scores\n",
    "Now, we combine the normalized scores from both methods using a weighted sum. The weight alpha controls the balance between the two methods. A value of `alpha=0.5` means both methods contribute equally to the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RI7H-pjcYcOX"
   },
   "outputs": [],
   "source": [
    "# Combine vector and BM25 scores using a weighted sum (alpha controls the balance)\n",
    "alpha = 0.5  # 0.5 means equal weight for both methods\n",
    "combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zLp69a4Ycbq"
   },
   "source": [
    "The `alpha` parameter controls the weight given to the vector-based search, and `(1 - alpha)` controls the weight for the BM25 search. By adjusting alpha, we can fine-tune how much influence each method has on the final ranking.\n",
    "\n",
    "#### Ranking and retrieving the top documents\n",
    "Finally, we rank the documents based on the combined scores and retrieve the top `k` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P9TqTfvhY_7v"
   },
   "outputs": [],
   "source": [
    "# Rank documents based on combined scores\n",
    "sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "# Retrieve the top k documents based on the combined scores\n",
    "top_docs = [all_docs[i] for i in sorted_indices[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHALtpbxZAK_"
   },
   "source": [
    "- The `np.argsort()` function sorts the combined scores in descending order. The result is the indices of the documents, sorted by their combined relevance scores.\n",
    "- Using the sorted indices, we select the top 5 documents from the original set of documents (`all_docs`) that were retrieved earlier. These are the documents that are deemed most relevant to the query based on the combined scores.\n",
    "\n",
    "#### Displaying the results\n",
    "Now that we have the top `k` documents based on the fusion of vector and BM25 retrieval, we can display the content of these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BuL9ZAffZA3u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a long time. These projects can help sequester carbon and provide new habitats for wildlife. \\nStrategic planning and ecological considerations are essential for maximizing benefits. \\nClimate Policy \\nEffective climate policy is essential for driving large-scale change. International agreements, \\nsuch as the Paris Agreement, aim to limit global warming to well below 2 degrees Celsius \\nabove pre-industrial levels. National and local policies also play a critical role in \\nimplementing mitigation and adaptation strategies. \\nInternational Agreements \\nInternational climate agreements, such as the Kyoto Protocol and the Paris Agreement, set \\ntargets and frameworks for reducing greenhouse gas emissions globally. Cooperation and \\ncommitment from all countries are necessary for achieving climate goals. \\nNational Policies',\n",
       " 'arid and semi-arid regions. Droughts can lead to food and water shortages and exacerbate \\nconflicts. \\nFlooding \\nHeavy rainfall events are becoming more common, leading to increased flooding. Urban \\nareas with poor drainage and infrastructure are particularly at risk. Flood management \\nstrategies include improved drainage systems, green infrastructure, and floodplain restoration. \\nOcean Acidification \\nIncreased CO2 levels in the atmosphere lead to higher concentrations of CO2 in the oceans. \\nThis causes the water to become more acidic, which can harm marine life, particularly \\norganisms with calcium carbonate shells or skeletons, such as corals and some shellfish. \\nCoral Reefs',\n",
       " 'and storage, and sustainable agriculture. Collaboration between governments, industries, and \\nacademia is essential for fostering innovation. \\nRenewable Energy Technology \\nInvesting in research and development of renewable energy technologies can lead to more \\nefficient and cost-effective solutions. Emerging technologies, such as advanced solar cells \\nand wind turbine designs, hold promise for the future. \\nCarbon Capture and Storage \\nCarbon capture and storage (CCS) technologies aim to capture CO2 emissions from industrial \\nsources and store them underground. These technologies are critical for reducing emissions \\nfrom hard-to-abate sectors and achieving net-zero targets. \\nSustainable Agriculture \\nInnovations in sustainable agriculture can help reduce emissions, enhance food security, and \\nprotect ecosystems. Practices such as agroforestry, precision farming, and regenerative \\nagriculture offer pathways to a more sustainable and resilient food system.',\n",
       " 'managed retreats. \\nExtreme Weather Events \\nClimate change is linked to an increase in the frequency and severity of extreme weather \\nevents, such as hurricanes, heatwaves, droughts, and heavy rainfall. These events can have \\ndevastating impacts on communities, economies, and ecosystems. \\nHurricanes and Typhoons \\nWarmer ocean temperatures can intensify hurricanes and typhoons, leading to more \\ndestructive storms. Coastal regions are at heightened risk of storm surge and flooding. Early \\nwarning systems and resilient infrastructure are critical for mitigating these risks. \\nDroughts \\nIncreased temperatures and changing precipitation patterns are contributing to more frequent \\nand severe droughts. This affects agriculture, water supply, and ecosystems, particularly in \\narid and semi-arid regions. Droughts can lead to food and water shortages and exacerbate \\nconflicts. \\nFlooding \\nHeavy rainfall events are becoming more common, leading to increased flooding. Urban',\n",
       " 'protect ecosystems. Practices such as agroforestry, precision farming, and regenerative \\nagriculture offer pathways to a more sustainable and resilient food system. \\nBy understanding the causes, effects, and potential solutions to climate change, we can take \\ninformed actions to protect our planet for future generations. Global cooperation, innovation, \\nand commitment are key to addressing this pressing challenge. \\n \\nChapter 5: The Role of Technology in Climate Change \\nMitigation \\nAdvanced Renewable Energy Solutions \\nNext-Generation Solar Technologies']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the content of the top documents\n",
    "docs_content = [doc.page_content for doc in top_docs]\n",
    "docs_content"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
