{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q3tBQ8WKy_4"
   },
   "source": [
    "# Reranking methods in RAG systems\n",
    "\n",
    "Reranking is a crucial step in RAG systems that aims to improve the relevance and quality of retrieved documents. It involves reassessing and reordering initially retrieved documents to ensure that the most relevant information is prioritized for subsequent processing or presentation. This notebook demonstrates two powerful reranking strategies: one using a LLMs, and another using a cross-encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DKGc56YpKwrq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHVDkM-TLSr3"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load and encode the PDF into a vector store\n",
    "In this step, we process a PDF document and convert its textual content into a vector database using FAISS and OpenAI embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q6knIHJXKzlg"
   },
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Replace tab characters ('\\t') with spaces in the document chunks\n",
    "    for doc in texts:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpyLi_TFKzyV"
   },
   "source": [
    "- `PyPDFLoader` reads the PDF and extracts its text content.\n",
    "- `loader.load()` returns a list of `Document` objects, each usually representing one page of the PDF.\n",
    "- `RecursiveCharacterTextSplitter` tries to split the text at natural breakpoints (paragraphs, then sentences, etc.). we are splitting the document into chunks of size 1000 characters with 200 characters of overlap between chunks.\n",
    "  - `chunk_size` makes it more manageable for indexing and retrieval.\n",
    "  - `chunk_overlap` ensures that the context is preserved when the text is split. This helps maintain the flow of information between chunks.\n",
    "  - `length_function` tells the splitter to calculate the length of the chunks based on the number of characters, ensuring that the chunks are exactly the specified size.\n",
    "- We then initialize `OpenAIEmbeddings()`, which provides access to OpenAI’s text embedding model. This model will convert each chunk of text into a high-dimensional vector that captures its semantic meaning.\n",
    "- With the cleaned chunks and the embedding model ready, we use `FAISS.from_documents()` to create a vector store. It enables us to efficiently query the document for relevant content based on a user’s input.\n",
    "- Finally, the function returns the completed `vectorstore`.\n",
    "\n",
    "## Method 1: LLM based function to rerank the retrieved documents\n",
    "In this step, we define a reranking function that uses a LLM to evaluate and reorder documents retrieved from the vector store. This allows us to go beyond simple similarity scores and instead use natural language reasoning to assess which chunks are most relevant to a given query.\n",
    "\n",
    "Embedding-based retrieval (like FAISS) is fast and powerful, but it relies purely on vector similarity, which may not always capture deeper nuances in the query-document relationship. By using an LLM to re-score the documents, we can apply a much more context-aware and task-specific measure of relevance — one that mimics human reasoning. This often leads to better answer quality and fewer irrelevant results in downstream tasks.\n",
    "\n",
    "#### Define a structured output class\n",
    "We start by defining a small `RatingScore` class using `pydantic` that describes the structure of the LLM’s response. It simply expects a single numerical score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eH7HYtLfQCz8"
   },
   "outputs": [],
   "source": [
    "# Class to define the expected output format from the LLM — a single float value.\n",
    "class RatingScore(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp_8t7XOQGPA"
   },
   "source": [
    "This class specifies the structure we expect when the LLM outputs a response — in this case, just a single field: `relevance_score`, a float from 1 to 10 indicating how relevant the document is to the query.\n",
    "\n",
    "#### Function to prompt the LLM to evaluate relevance\n",
    "We now define the `rerank_documents` function, which prompts the LLM to score each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4lGmVkZyKz-x"
   },
   "outputs": [],
   "source": [
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    # Define the prompt template that will be used to query the LLM\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "\n",
    "    # Instantiate the LLM with deterministic settings (temperature = 0)\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "    # Combine the prompt with the model and request structured output (RatingScore)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "\n",
    "    scored_docs = []\n",
    "    # Evaluate each document by prompting the LLM\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
    "        # Invoke the chain to get a relevance score from the LLM\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        # Ensure the score is a float; fallback to 0 if parsing fails\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score)) # Store doc along with its score\n",
    "\n",
    "    # Sort all documents by score in descending order\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    # Return only the top_n most relevant documents\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ_UktgtK0LL"
   },
   "source": [
    "- Inside the `rerank_documents()` function, we define a natural language prompt using `PromptTemplate`. This prompt asks the LLM to rate how relevant a given document is to a query on a scale from 1 to 10. The instructions encourage the model to focus on semantic relevance, not just keyword overlap.\n",
    "- We instantiate a `ChatOpenAI` model with `temperature=0` to ensure deterministic outputs (more consistent scoring). We also use the `with_structured_output()` method to enforce that the LLM returns a well-structured JSON output matching our `RatingScore` schema.\n",
    "- For each document in the list, we pass the query and the document content to the prompt pipeline. The LLM returns a `relevance_score`, which we attempt to convert to a float. If something goes wrong (e.g. a malformed output), we assign a default score of `0` to prevent the process from failing.\n",
    "- Once all scores are collected, we sort the documents in descending order of their relevance score using `sorted()`. This produces a reranked list, with the most relevant documents at the top.\n",
    "- Finally, we return only the top `n` documents (default: 3) from the reranked list. These will be the most semantically relevant chunks chosen by the LLM, and they can now be used as input to a generation model.\n",
    "\n",
    "#### Example usage: Run retrieval and reranking on a real query\n",
    "Now that we have defined the reranking function, we can try it out on a real example. We will start by performing a standard similarity search using the FAISS vector store, then apply our `rerank_documents()` method to reorder the results based on LLM-based semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2IfptcniK0Xa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top initial documents:\n",
      "\n",
      "Document 1:\n",
      "managed retreats. \n",
      "Extreme Weather Events \n",
      "Climate change is linked to an increase in the frequency and severity of extreme weather \n",
      "events, such as hurricanes, heatwaves, droughts, and heavy rainfall...\n",
      "\n",
      "Document 2:\n",
      "development of eco-friendly fertilizers and farming techniques is essential for reducing the \n",
      "agricultural sector's carbon footprint. \n",
      "Chapter 3: Effects of Climate Change \n",
      "The effects of climate chan...\n",
      "\n",
      "Document 3:\n",
      "Heatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \n",
      "Changing Seasons \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
      "...\n",
      "Query: What are the impacts of climate change on biodiversity?\n",
      "\n",
      "Top reranked documents:\n",
      "\n",
      "Document 1:\n",
      "Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
      "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles \n",
      "and biodiversity. The...\n",
      "\n",
      "Document 2:\n",
      "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
      "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
      "fisheries. Pr...\n",
      "\n",
      "Document 3:\n",
      "Heatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \n",
      "Changing Seasons \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "\n",
    "# Retrieve top 15 documents using vector similarity\n",
    "initial_docs = vectorstore.similarity_search(query, k=15)\n",
    "\n",
    "# Apply LLM-based reranking\n",
    "reranked_docs = rerank_documents(query, initial_docs)\n",
    "\n",
    "# Print the top 3 initial documents (from similarity search)\n",
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document\n",
    "\n",
    "\n",
    "# Print the top reranked documents (after LLM scoring)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmeZI1GFK0jM"
   },
   "source": [
    "- We define a query relevant to our document.\n",
    "- Using `vectorstore.similarity_search()`, we retrieve the top 15 most similar chunks from the vector database.\n",
    "- Then, we pass these retrieved documents through our reranker, which uses a language model to score and sort them more intelligently.\n",
    "- Then, we compare the top documents before and after reranking to see how the LLM changes the order based on a deeper understanding of the query.\n",
    "\n",
    "We can see that these reranked results are more focused, precise, and contextually aligned with the intent of the query.\n",
    "\n",
    "#### Wrap reranking into a custom retriever\n",
    "To make the reranking process seamlessly integrate with a RAG pipeline, we encapsulate it in a custom retriever. This allows us to plug it into LangChain’s RetrievalQA class just like any other retriever — but under the hood, we now benefit from our LLM-based reranking logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bB6VqdglYkUc"
   },
   "outputs": [],
   "source": [
    "# Create a custom retriever class that incorporates LLM-based reranking\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        # Step 1: Retrieve an initial set of documents (more than we need)\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        # Step 2: Rerank and return only the top N most relevant ones\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cxcx7apVYmwp"
   },
   "source": [
    "- Here we define a class `CustomRetriever` that inherits from both `BaseRetriever` (LangChain interface) and `BaseModel` (to support field validation).\n",
    "- The `vectorstore` is passed into the retriever as a parameter.\n",
    "- In `get_relevant_documents()`, we first retrieve a broader set of documents (e.g., top 30 from FAISS) and then narrow it down using the LLM-based `rerank_documents()` function.\n",
    "- The final output is a smaller set of top-ranked documents, now selected not just by similarity but by semantic relevance using the language model.\n",
    "\n",
    "#### Connect to a RetrievalQA chain\n",
    "With our custom retriever ready, we can now use it in a `RetrievalQA` pipeline. This pipeline retrieves relevant documents using our reranker, then passes them to the LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RBr2X7kHK0t3"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the custom retriever\n",
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Instantiate the LLM for answering questions\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\")\n",
    "\n",
    "# Create the RetrievalQA chain with the custom retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1ldXYDrK05h"
   },
   "source": [
    "- We create the `llm` with a temperature of `0` for deterministic responses.\n",
    "- We pass our `CustomRetriever` into the QA pipeline.\n",
    "- The `RetrievalQA` chain is configured with the `\"stuff\"` chain type — meaning it will simply concatenate the retrieved documents and feed them to the LLM as context.\n",
    "- Setting `return_source_documents=True` ensures that we can inspect which chunks the model used to generate its answer.\n",
    "\n",
    "#### Example query: Run the RAG pipeline with reranked retrieval\n",
    "Now that the pipeline is fully set up, we can pass in a natural language query and run the full RAG process. This involves three main steps under the hood:\n",
    "1. Initial retrieval via the vector store,\n",
    "2. Reranking using the LLM to find the most semantically relevant chunks,\n",
    "3. Answer generation by feeding those reranked chunks into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A0DjRj-1K1GI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: Climate change impacts biodiversity in several ways, including:\n",
      "\n",
      "1. Habitat Loss: Changes in temperature and precipitation patterns can lead to the loss of habitats, such as tropical rainforests and boreal forests, which are crucial for many species.\n",
      "\n",
      "2. Species Extinction: As habitats are altered or destroyed, many species may not be able to adapt quickly enough to survive, leading to increased rates of extinction.\n",
      "\n",
      "3. Ocean Acidification: Increased CO2 levels lead to ocean acidification, which affects marine ecosystems, particularly coral reefs. This can result in coral bleaching and mortality, threatening marine biodiversity.\n",
      "\n",
      "4. Disruption of Food Webs: Changes in species distributions and interactions can disrupt food webs, affecting the survival of various marine and terrestrial species.\n",
      "\n",
      "5. Altered Migration Patterns: Climate change can affect the migration patterns of species, leading to mismatches in timing between species and their food sources or breeding grounds.\n",
      "\n",
      "Overall, climate change poses significant threats to biodiversity, impacting ecosystems and the services they provide.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
      "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles \n",
      "and biodiversity. The...\n",
      "\n",
      "Document 2:\n",
      "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
      "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
      "fisheries. Pr...\n"
     ]
    }
   ],
   "source": [
    "# Run the full QA pipeline\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIkhucZEK1RT"
   },
   "source": [
    "- We invoke the `qa_chain` by passing in the query as a dictionary. The chain internally:\n",
    "  - Uses our custom retriever to find and rerank documents,\n",
    "  - Concatenates the top documents into a context window,\n",
    "  - And feeds them into the LLM to generate a natural language answer.\n",
    "  \n",
    "- The result is returned as a dictionary. The field `result['result']` contains the model's final answer.\n",
    "- The field `result['source_documents']` provides the actual document chunks that were used to generate the answer — this is crucial for transparency and debugging.\n",
    "\n",
    "This gives us a better explainable question-answering system — powered by both retrieval and reasoning — all while maintaining full visibility into how the answer was derived.\n",
    "\n",
    "### Example: Why reranking improves retrieval quality\n",
    "To demonstrate the value of using reranking with an LLM, we will walk through a toy example. We define a list of short text chunks — some contain surface-level keyword matches (like \"The capital of France is...\"), while others include richer context and reasoning (such as detailed descriptions of Paris).\n",
    "\n",
    "By comparing the baseline FAISS retrieval to our custom reranked retriever, we can see how reranking helps us surface the most meaningful and informative content, rather than just text that shares similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ESRZKG2tK1b_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Advanced Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is great.\n"
     ]
    }
   ],
   "source": [
    "# Sample input text chunks with varying levels of semantic relevance\n",
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower.\n",
    "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\",\n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "docs = [Document(page_content=sentence) for sentence in chunks] # Wrap each chunk as a Document\n",
    "\n",
    "# Function to compare baseline and reranked retrieval side-by-side\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings) # Build a temporary vectorstore for testing\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "\n",
    "    # Baseline: Retrieve top 2 results using vector similarity only\n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
    "    for i, doc in enumerate(baseline_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "    # Advanced: Retrieve top 2 reranked results using LLM scoring\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "    advanced_docs = custom_retriever.get_relevant_documents(query)\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "# Run the comparison\n",
    "query = \"what is the capital of france?\"\n",
    "compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZs1ARKscNDE"
   },
   "source": [
    "- This example illustrates a common issue with standard vector similarity: it favors short, keyword-aligned snippets over more contextually relevant ones. The top baseline results are likely generic and similar, like “The capital of France is...,” which are short but not very informative.\n",
    "- In contrast, the reranked results are chosen by the LLM based on their semantic relevance. However, since the model used here is `gpt-4o-mini-2024-07-18` and not the full `gpt-4o`, the model's ability to deeply understand and improve the results is somewhat limited, resulting in less-than-expected improvements in reranking.\n",
    "\n",
    "This highlights how reranking works by interpreting meaning rather than just matching words, but the choice of model can still affect the outcome.\n",
    "\n",
    "## Method 2: Reranking with a cross-encoder model\n",
    "In this step, we define a custom retriever that leverages a cross-encoder model to rerank documents based on a deeper, pairwise semantic evaluation. Unlike embedding-based similarity or LLM-based rating (which evaluate query and document independently), a cross-encoder scores each query-document pair jointly, often yielding state-of-the-art results in many information retrieval tasks.\n",
    "\n",
    "Cross-encoders can be slower than other methods (since each query-document pair is passed through the model), but they make up for it with more accurate relevance scoring, especially for nuanced or complex queries.\n",
    "\n",
    "This cross-encoder approach is particularly effective when:\n",
    "- We have a small number of documents to rerank.\n",
    "- Accuracy is more important than speed.\n",
    "- The retrieval task demands more nuanced understanding of the query-document relationship (e.g. subtle distinctions in meaning or intent).\n",
    "\n",
    "It provides a strong alternative to the LLM-based reranker, especially when we want a more deterministic and cost-efficient solution without calling a external generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IUf27u1TfgjA"
   },
   "outputs": [],
   "source": [
    "# Load a pretrained cross-encoder model fine-tuned for ranking\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwhFDmbGfhMu"
   },
   "source": [
    "Here we load a cross-encoder model from Hugging Face. This model has been trained on MS MARCO — a large-scale dataset for passage ranking — making it a good fit for reranking retrieved documents based on relevance.\n",
    "\n",
    "#### Define the CrossEncoderRetriever class\n",
    "We now define a custom retriever class that integrates both a vector store for initial retrieval and the cross-encoder model for reranking the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T_f_6y3kK1zF"
   },
   "outputs": [],
   "source": [
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Initial retrieval\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "\n",
    "        # Prepare pairs for cross-encoder\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "        # Get cross-encoder scores\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "\n",
    "        # Sort documents by score\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return top reranked documents\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSOukexQLsuk"
   },
   "source": [
    "- This class extends LangChain's `BaseRetriever` and uses `pydantic` for schema validation.\n",
    "- It takes in a `vectorstore` (for fast initial retrieval) and a `cross_encoder` (for scoring).\n",
    "- `k` controls how many documents are initially retrieved using FAISS, and `rerank_top_k` sets how many reranked documents will be returned.\n",
    "\n",
    "In `get_relevant_documents`:\n",
    "  - We first retrieve `k` documents using traditional vector similarity (`similarity_search()` from FAISS).\n",
    "  - Each document is then paired with the query and passed into the cross-encoder. The model scores each pair based on how relevant the document is to the query.\n",
    "  - These scores are used to sort the documents from most to least relevant.\n",
    "  - Finally, the top `rerank_top_k` documents are returned — now reranked using deeper semantic reasoning.\n",
    "\n",
    "In `async def aget_relevant_documents`:\n",
    "  - For completeness, we include the async method required by the retriever base class — but since it’s not implemented, this method will raise an error if used.\n",
    "\n",
    "\n",
    "#### Example: Running a QA pipeline with cross-encoder reranking\n",
    "Now that we have defined our `CrossEncoderRetriever`, we can plug it into a full RAG pipeline. We will set up the retriever, initialize the LLM, and use `RetrievalQA` to answer a query using the most relevant documents — reranked by the cross-encoder.\n",
    "\n",
    "The goal here is to use the best of both worlds: fast FAISS-based search for initial recall, and accurate reranking via a cross-encoder before passing the results to the language model for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Veou2Wq2iaoh"
   },
   "outputs": [],
   "source": [
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    cross_encoder=cross_encoder,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycmsFakPictF"
   },
   "source": [
    "- We instantiate the `CrossEncoderRetriever`, passing in our FAISS vector store and the cross-encoder model.\n",
    "- `k=10` means the retriever will first pull 10 documents based on vector similarity.\n",
    "- These are then reranked by the cross-encoder, and only the top 5 (`rerank_top_k=5`) are passed along to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iDYmNie0iyj6"
   },
   "outputs": [],
   "source": [
    "# Set up the LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeIa78uViy9R"
   },
   "source": [
    "- We instantiate the OpenAI GPT-4o model with `temperature=0` for consistent and deterministic answers.\n",
    "- This model will be used to generate a final answer based on the selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Cz8C_4fyjG7q"
   },
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain with the cross-encoder retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lttdrn1sjHSn"
   },
   "source": [
    "- We build a `RetrievalQA` chain, combining our LLM and custom retriever.\n",
    "- The `chain_type=\"stuff\"` setting means all the top documents are simply concatenated (\"stuffed\") into the prompt for the LLM.\n",
    "- `return_source_documents=True` allows us to inspect which documents the model used to generate its answer — useful for debugging and transparency.\n",
    "\n",
    "We run an example query that explores the effects of climate change on biodiversity. This query will flow through the retriever → reranker → LLM pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4yEeNPLILs9u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: The impacts of climate change on biodiversity include habitat loss, species extinction, and disruptions to ecosystems. Changes in temperature and precipitation patterns can alter the timing and length of seasons, affecting plant and animal life cycles. Additionally, extreme weather events, such as hurricanes and droughts, can have devastating effects on communities and ecosystems. Coral reefs, for example, are highly sensitive to changes in temperature and acidity, leading to coral bleaching and mortality, which threatens marine biodiversity. Overall, climate change poses significant risks to the survival of various species and the health of ecosystems.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Tropical rainforests are particularly important for carbon storage. Deforestation in the \n",
      "Amazon, Congo Basin, and Southeast Asia has significant impacts on global carbon cycles \n",
      "and biodiversity. The...\n",
      "\n",
      "Document 2:\n",
      "development of eco-friendly fertilizers and farming techniques is essential for reducing the \n",
      "agricultural sector's carbon footprint. \n",
      "Chapter 3: Effects of Climate Change \n",
      "The effects of climate chan...\n",
      "\n",
      "Document 3:\n",
      "Heatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \n",
      "Changing Seasons \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
      "...\n",
      "\n",
      "Document 4:\n",
      "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
      "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
      "fisheries. Pr...\n",
      "\n",
      "Document 5:\n",
      "managed retreats. \n",
      "Extreme Weather Events \n",
      "Climate change is linked to an increase in the frequency and severity of extreme weather \n",
      "events, such as hurricanes, heatwaves, droughts, and heavy rainfall...\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFTi638QLtMe"
   },
   "source": [
    "Here we can assess how well the retriever worked — and whether the final response is grounded in the retrieved evidence. This method offers context-aware document selection, leading to more accurate and reliable responses from the LLM."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
