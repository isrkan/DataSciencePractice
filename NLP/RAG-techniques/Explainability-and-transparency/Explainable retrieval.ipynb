{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr8HFxL5cBlS"
   },
   "source": [
    "# Explainable retrieval in document search\n",
    "This notebook demonstrates how to build an explainable retriever — a system that not only finds relevant documents for a user query but also explains why those documents were selected. This is useful when we want to make our retrieval process transparent, interpretable, and trustworthy. Being able to justify why a piece of text was retrieved is just as important as the retrieval itself.\n",
    "\n",
    "Most document retrieval systems — especially those using vector similarity — return results as a black box. We get a result, but We are not told why. This can be frustrating, especially when trust or traceability is important. We add natural language explanations using an LLM. After we retrieve a document, we pass both the document and the original query to the LLM and ask it to explain why that document is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CewL9mv3bxk4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYQKPOZicAKQ"
   },
   "source": [
    "### Prepare the document collection\n",
    "Here we define the “document collection” — a few sample texts. These are what we will be searching through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ljC6MM1Jf6Te"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The sky is blue because of the way sunlight interacts with the atmosphere.\",\n",
    "    \"Photosynthesis is the process by which plants use sunlight to produce energy.\",\n",
    "    \"Global warming is caused by the increase of greenhouse gases in Earth's atmosphere.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du6sKS5Tf6gh"
   },
   "source": [
    "We are keeping it simple for now with just three facts, but this approach works just as well for paragraphs or entire documents.\n",
    "\n",
    "### Prepare the core retrieval components\n",
    "To build our explainable retriever, we need:\n",
    "- Embeddings: To convert text into vector space.\n",
    "- Vector store (FAISS): To store and search those vectors.\n",
    "- LLM (ChatOpenAI): To explain results in natural language.\n",
    "\n",
    "#### Create the vector store\n",
    "Now let's embed texts using OpenAI embeddings and store them using FAISS for similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K5C-fXtff6t7"
   },
   "outputs": [],
   "source": [
    "# Convert input texts to embeddings and store them\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldbExlkqf68a"
   },
   "source": [
    "- First, we load OpenAI’s embedding model. This is a tool that turns each sentence into a numerical vector based on its meaning.\n",
    "- Then, we pass our list of `texts` into that embedding model.\n",
    "- The results — those numerical representations — are then stored in FAISS.\n",
    "\n",
    "#### Set up the retriever\n",
    "We now create a retriever that lets us search through the vector store. It will return the top 5 most relevant pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s0KzUZQ2hd6g"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLZoIis2heIM"
   },
   "source": [
    "What this does is wrap our FAISS vector store in a retriever interface. It will return the top 5 most relevant pieces of text.\n",
    "\n",
    "### Build the explanation generator\n",
    "Here, we use a prompt to tell the language model: \"Explain why this chunk of text is relevant to this query.\" And we wire that prompt into GPT-4o-mini to create a full explanation chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f3ZLuejSheVW"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "# Define a prompt that asks the LLM to explain why a document is relevant to the query\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    Analyze the relationship between the following query and the retrieved context.\n",
    "    Explain why this context is relevant to the query and how it might help answer the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Explanation:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Chain the prompt to the LLM\n",
    "explain_chain = explain_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9zYGD-cheih"
   },
   "source": [
    "- First, we are initializing an instance of OpenAI’s GPT model. The `temperature=0` ensures that the model behaves deterministically — it will give the same output for the same input every time. This is great for explanations, where we want consistent reasoning rather than creative variations.\n",
    "- Then, we are defining a prompt template. This is a reusable structure that we will fill in later with a real query and a real chunk of retrieved content. This kind of prompt engineering helps the LLM give structured and focused responses.\n",
    "  - `input_variables=[\"query\", \"context\"]`: This tells the system which variables will be filled in when the template is used.\n",
    "  - The `template` itself is a clear instruction to the LLM: it gets the query and the retrieved content and is asked to generate an explanation of their connection.\n",
    "- Then, we use LangChain's LCEL (LangChain Expression Language) syntax to create a chain by \"piping\" the filled-in prompt directly into the language model. What this means practically is: for every query + retrieved document pair, the system will inject those values into the prompt template, send it to the model, and return the model’s generated explanation.\n",
    "\n",
    "### Run a query and get explanations\n",
    "Now we are ready to try a real query. We will retrieve the relevant results and pass them through the explanation generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lQ6vGc1MiRyF"
   },
   "outputs": [],
   "source": [
    "query = \"Why is the sky blue?\"\n",
    "\n",
    "# Retrieve matching documents\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Generate explanations for each result\n",
    "explained_results = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Prepare inputs\n",
    "    input_data = {\n",
    "        \"query\": query,\n",
    "        \"context\": doc.page_content\n",
    "    }\n",
    "    # Ask the LLM to explain the relevance\n",
    "    explanation = explain_chain.invoke(input_data).content\n",
    "\n",
    "    # Collect the result with its explanation\n",
    "    explained_results.append({\n",
    "        \"content\": doc.page_content,\n",
    "        \"explanation\": explanation\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deju0Pu_iSBV"
   },
   "source": [
    "- First, the user provides a natural language query.\n",
    "- Then we call `.get_relevant_documents(query)` on the retriever. Behind the scenes, the retriever takes the query, converts it into an embedding, and compares it to the precomputed embeddings in the FAISS vector store. It returns the top matches — in this setup, we have configured it to return the top 5 (`k=5`).\n",
    "- So now we have a list of documents that are likely relevant — but we don’t yet know why they were chosen. That is where the explanation chain comes in. For each retrieved document, we take:\n",
    "  - The original query (the user's question),\n",
    "  - The document content (the candidate match),\n",
    "  - And format them into a structured input for the LLM (via the explanation prompt we defined earlier).\n",
    "  - We pass this to the language model, which responds with a natural-language explanation of how the document is related to the query.\n",
    "- We then store each document along with its explanation in a list of dictionaries, so we can easily display or analyze them later.\n",
    "\n",
    "### Show the final results\n",
    "Let’s print out each result and its explanation. This makes the retrieval process clear and understandable — we see not just *what* was retrieved, but *why*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZyJNvAsbiSOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: The sky is blue because of the way sunlight interacts with the atmosphere.\n",
      "Explanation: The context provided directly addresses the query by explaining the reason behind the phenomenon of a blue sky. The query asks for an explanation of why the sky appears blue, and the context succinctly states that this is due to the interaction of sunlight with the atmosphere.\n",
      "\n",
      "The relevance of the context lies in its focus on the scientific principles involved in the scattering of light. Specifically, it implies that the blue color of the sky is a result of Rayleigh scattering, where shorter wavelengths of light (blue) are scattered more than longer wavelengths (red) when sunlight passes through the Earth's atmosphere. This explanation is fundamental to understanding the query.\n",
      "\n",
      "By providing this context, it helps answer the query by offering a clear and concise reason for the blue appearance of the sky, which is likely what the person asking the question is seeking. The context serves as a foundational piece of information that can lead to a deeper exploration of atmospheric science and optics if the user wishes to learn more.\n",
      "\n",
      "Result 2:\n",
      "Content: Global warming is caused by the increase of greenhouse gases in Earth's atmosphere.\n",
      "Explanation: The context provided—\"Global warming is caused by the increase of greenhouse gases in Earth's atmosphere\"—is not directly relevant to the query \"Why is the sky blue?\" The query seeks to understand the scientific explanation behind the color of the sky, which is primarily due to Rayleigh scattering of sunlight by the atmosphere. \n",
      "\n",
      "The relevance of the context to the query is minimal because it does not address the specific phenomenon of the sky's color. Instead, it discusses a broader environmental issue related to climate change, which is not connected to the optical properties of the atmosphere that determine the color of the sky.\n",
      "\n",
      "To effectively answer the query, the context should include information about light scattering, the composition of the atmosphere, and how these factors contribute to the perception of a blue sky during the day. For example, a relevant context might explain that shorter wavelengths of light (blue) are scattered more than longer wavelengths (red) when sunlight passes through the atmosphere, leading to the blue appearance of the sky.\n",
      "\n",
      "In summary, while the context provided discusses an important environmental issue, it does not help answer the query about why the sky is blue, as it lacks the necessary scientific explanation related to atmospheric optics.\n",
      "\n",
      "Result 3:\n",
      "Content: Photosynthesis is the process by which plants use sunlight to produce energy.\n",
      "Explanation: The context provided about photosynthesis is not directly relevant to the query \"Why is the sky blue?\" The query seeks to understand the scientific explanation behind the color of the sky, which is primarily due to Rayleigh scattering of sunlight by the Earth's atmosphere. \n",
      "\n",
      "In contrast, the context discusses photosynthesis, a biological process that involves plants converting sunlight into energy. While both topics involve sunlight, the connection is tenuous at best. The explanation for the blue sky involves atmospheric physics rather than biological processes.\n",
      "\n",
      "To effectively answer the query, relevant context would need to include information about light scattering, the composition of the atmosphere, and how different wavelengths of light are affected by atmospheric particles. This would provide a clear understanding of why the sky appears blue during the day. \n",
      "\n",
      "In summary, the context about photosynthesis does not help answer the query about the color of the sky, as it addresses a different scientific phenomenon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(explained_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Explanation: {result['explanation']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
