{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fefb661-5276-4e07-95bc-28c020653f75",
   "metadata": {},
   "source": [
    "# Context enrichment window for document retrieval\n",
    "\n",
    "This notebook demonstrates how to use a context enrichment window technique for document retrieval in a vector database. Traditional vector search systems, such as FAISS, return isolated chunks of text based on the query. These chunks may lack surrounding context, making it difficult to fully understand the information. The aim is to improve the quality of search results by retrieving not only relevant chunks but also their neighboring chunks, providing more context for understanding the retrieved information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7733bca6-b44e-4a82-9e77-76dd354ffa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import pymupdf\n",
    "from typing import List\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbfc414-7216-4c30-a46c-8940cd09e9ce",
   "metadata": {},
   "source": [
    "## Document preprocessing\n",
    "Let's start by reading the content of a PDF file and splitting it into manageable chunks.\r\n",
    "### Read PDF to string\n",
    "Here, we read the PDF content and convert it into a string using the `pymupdf` library. This will allow us to process the text further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c12408-59c7-488e-8590-6bfcf5515659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Open the PDF document\n",
    "doc = pymupdf.open(path)\n",
    "content = \"\"\n",
    "\n",
    "# Iterate over each page and extract text\n",
    "for page_num in range(len(doc)):\n",
    "    # Get the current page\n",
    "    page = doc[page_num]\n",
    "    # Extract the text content from the current page and append it to the content string\n",
    "    content += page.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd555a-d99b-45ec-aac6-4dd6385fd40a",
   "metadata": {},
   "source": [
    "The PDF file is opened using `pymupdf.open()`, and we iterate over all pages to extract the text. The extracted text is then concatenated into one large string. This will allow us to process it further and split it into manageable chunks.\n",
    "\n",
    "### Split text into chunks\n",
    "After reading the content of the PDF, we split the text into smaller chunks. This is done to make the text more manageable and easier to search through. The chunk size is set, and we introduce overlap between chunks to ensure relevant context is retained across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18ee38b-7949-4a1a-bafc-391cd83a55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into chunks with overlap\n",
    "chunks_size = 400\n",
    "chunk_overlap = 200\n",
    "chunks = []\n",
    "start = 0\n",
    "\n",
    "# Loop to split the content into chunks until the end of the text\n",
    "while start < len(content):\n",
    "    # Calculate the end position of the current chunk\n",
    "    end = start + chunks_size\n",
    "    # Extract the current chunk from the conten\n",
    "    chunk = content[start:end]\n",
    "    # Append the chunk to the list, including its index in metadata\n",
    "    chunks.append(Document(page_content=chunk, metadata={\"index\": len(chunks), \"text\": content}))\n",
    "    # Move the starting index forward, ensuring overlap between chunks\n",
    "    start += chunks_size - chunk_overlap  # Adjust to create overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a6539-c65d-4e85-82e7-844fb1b788ed",
   "metadata": {},
   "source": [
    "We are defining a `chunks_size` of 400 characters for each chunk, and an `chunk_overlap` of 200 characters. The loop continues to run as long as the `start` position is less than the length of the `content`. Essentially, the loop breaks when we reach the end of the text.\n",
    "- **Extracting a chunk**:\n",
    "   - `end = start + chunks_size`: This calculates the end position of the current chunk. It's set to `start + chunks_size`, which means each chunk will be 400 characters long.\n",
    "   - `chunk = content[start:end]`: This slices the text from the `start` position to the `end` position. It gives us a substring (i.e., a chunk) of the document. This chunk will be stored in a `Document` object.\n",
    "- **Storing the chunk**:\n",
    "   - `chunks.append(Document(page_content=chunk, metadata={\"index\": len(chunks), \"text\": content}))`: Here, we store each chunk in the `chunks` list. Each chunk is wrapped in a `Document` object, where:\n",
    "     - `page_content=chunk` stores the actual content of the chunk.\n",
    "     - `metadata={\"index\": len(chunks), \"text\": content}` adds additional metadata. The `index` tracks the chunkâ€™s position in the text (useful for later retrieval). The `text` metadata holds the full text of the document, although this might be redundant in this case.\n",
    "- **Update the `start` position**:\n",
    "   - `start += chunks_size - chunk_overlap`: This is the critical line for overlap. It updates the `start` position to be `chunks_size - chunk_overlap` characters ahead of the current `start`. In simpler terms, if the chunk size is 400 and the overlap is 200, this means after each chunk, we \"skip\" the part that overlaps with the next chunk. As a result, the next chunk starts 200 characters before the end of the previous chunk, ensuring that the two chunks share some context.\n",
    "\n",
    "## Vector store creation\n",
    "Now, let's create a vector store where we store the text chunks as vectors. We will convert the text chunks into numerical vector representations using OpenAI's embeddings, and then store them in a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51029c1c-de2c-41e7-b9b5-6e1660e45ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector store from the document chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Create a retriever for the vector store to fetch relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a039be9-9c06-4d6a-98a2-3a53dffe678c",
   "metadata": {},
   "source": [
    "- Here, we initialize the `OpenAIEmbeddings` class, which loads a model to convert the text of each chunk into a vector.\n",
    "- The `FAISS.from_documents` method takes the list of `chunks` (which contains the document text) and the initialized `embeddings` model to convert each chunk into a vector. These vectors are then stored in a FAISS vector store.\n",
    "- The `as_retriever()` method converts the FAISS vector store into a retriever object. This retriever is used to search for relevant documents based on the vectors stored in the FAISS index. The `search_kwargs={\"k\": 1}` parameter ensures that when we search for a query, only 1 document (the most relevant one) is returned. We can adjust this number (`k`) to return more documents if needed.\n",
    "\n",
    "\n",
    "## Context-enriched retrieval\n",
    "With the vector store in place, we can now enhance the retrieval process by not just returning a single relevant chunk, but also its surrounding context. This will help improve the comprehensiveness of the search results.\n",
    "\n",
    "#### Define the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adf2547-e291-49ec-ad78-244db507b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"Explain the role of deforestation and fossil fuels in climate change.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ccde4-1a62-4441-8a2e-d496ddaa459d",
   "metadata": {},
   "source": [
    "This is the search query that will be used to retrieve the relevant chunks from the vector store.\n",
    "\n",
    "#### Retrieve relevant chunks (the baseline chunk) using the retriever\n",
    "We now use the `retriever` (which was set up with the FAISS vector store) to fetch the chunks of text that are most relevant to the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b947b8ba-fe13-4b86-b5e0-2c1c4a609267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the relevant chunk using the retriever\n",
    "relevant_chunks = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57de9d-6b84-452e-b411-e101ced9cbc9",
   "metadata": {},
   "source": [
    "The `get_relevant_documents(query)` function returns the most relevant chunk(s) of text that match the query, based on the vector search. This gives us the starting point for retrieving more context from the document. Since the retriever is set to return only one chunk (because of `k=1`), `relevant_chunks` will be a list containing the most relevant chunk to the query.\n",
    "\n",
    "### Retrieve neighboring chunks with context enrichment\n",
    "Now, we define a function, which allows us to retrieve a specific chunk by its index from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173be599-cd7d-490b-a1a3-cfdcfc8ff55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_by_index(vectorstore, target_index: int) -> Document:\n",
    "    \"\"\"\n",
    "    Retrieve a chunk from the vectorstore based on its index in the metadata.\n",
    "    \n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the chunks.\n",
    "    target_index (int): The index of the chunk to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[Document]: The retrieved chunk as a Document object, or None if not found.\n",
    "    \"\"\"\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get('index') == target_index:\n",
    "            return doc  # Return the document if the index matches\n",
    "    return None  # Return None if no chunk with the target index is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be17b21-5320-4e24-9a96-74be20436a73",
   "metadata": {},
   "source": [
    "This function retrieves a chunk by searching through all documents in the vector store and matching the index stored in the metadata. If a chunk with the given index is found, it is returned. Otherwise, the function returns `None`.\n",
    "\n",
    "#### Iterate over the relevant chunks\n",
    "For each relevant chunk retrieved from the vector store, we fetch the neighboring chunks (before and after). The neighboring chunks are then sorted by index to ensure they appear in the correct order. We concatenate the chunks, considering the overlap between them to maintain context continuity. This enriched chunk is then added to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fd7c77-e0f8-47ac-99d7-66964bf3f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of neighboring chunks to retrieve\n",
    "num_neighbors = 1\n",
    "\n",
    "# Prepare the list of enriched chunk sequences\n",
    "result_sequences = []\n",
    "\n",
    "# Iterate over the relevant chunks\n",
    "for chunk in relevant_chunks:\n",
    "    current_index = chunk.metadata.get('index')\n",
    "    if current_index is None:\n",
    "        continue\n",
    "    \n",
    "    # Determine the range of chunks to retrieve (before and after the relevant chunk)\n",
    "    start_index = max(0, current_index - num_neighbors)\n",
    "    end_index = current_index + num_neighbors + 1  # +1 because range is exclusive at the end\n",
    "\n",
    "    # Retrieve all chunks in the range\n",
    "    neighbor_chunks = []\n",
    "    for i in range(start_index, end_index):\n",
    "        # Retrieve the chunk by its index\n",
    "        neighbor_chunk = get_chunk_by_index(vectorstore, i)\n",
    "        if neighbor_chunk:\n",
    "            neighbor_chunks.append(neighbor_chunk)\n",
    "\n",
    "    # Sort the chunks by their index to ensure correct order\n",
    "    neighbor_chunks.sort(key=lambda x: x.metadata.get('index', 0))\n",
    "\n",
    "    # Concatenate the chunks, accounting for overlap\n",
    "    concatenated_text = neighbor_chunks[0].page_content\n",
    "    for i in range(1, len(neighbor_chunks)):\n",
    "        current_chunk = neighbor_chunks[i].page_content\n",
    "        overlap_start = max(0, len(concatenated_text) - chunk_overlap)\n",
    "        concatenated_text = concatenated_text[:overlap_start] + current_chunk\n",
    "\n",
    "    # Append the concatenated result to the final list\n",
    "    result_sequences.append(concatenated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d520565-713a-4af7-a028-d139311196cb",
   "metadata": {},
   "source": [
    "- `num_neighbors = 1` means that we want to retrieve 1 chunk before and 1 chunk after the relevant chunk.\n",
    "- In `result_sequences` we store the context-enriched sequences after processing the relevant chunks and their neighboring chunks.\n",
    "\n",
    "We loop over each relevant chunk returned by the retriever. For each chunk:\n",
    "- We retrieve its index from the metadata (`current_index = chunk.metadata.get('index')`). If the index is missing, we skip that chunk (`continue`), as we need the index to fetch neighboring chunks.\n",
    "- We calculate the indices of the neighboring chunks that we want to retrieve:\n",
    "    - `start_index`: We subtract the number of neighbors from the current index to get the index of the first neighboring chunk before the relevant chunk. The `max(0, ...)` ensures we donâ€™t go below index 0.\n",
    "    - `end_index`: This is one step beyond the current index plus the number of neighbors, so we can retrieve chunks that come after the relevant chunk. The `+1` ensures we include the chunk after the relevant one because Python ranges are exclusive at the end.\n",
    "- Then, we fetch neighboring chunks. For each index within the range of neighboring chunks, we call the `get_chunk_by_index` function to fetch the corresponding chunk from the vector store. If the chunk is found (i.e., it's not `None`), we add it to the `neighbor_chunks list`. This helps us build a list of chunks that are relevant to the context of the current chunk.\n",
    "- Once we have gathered the neighboring chunks, we sort them by their index (`x.metadata.get('index', 0)`). This ensures that the chunks are in the correct order, which is critical for maintaining the continuity of the content.\n",
    "- Later, we concatenate chunks with overlap. We start with the first neighboring chunk and progressively add the next ones, ensuring there is overlap between consecutive chunks. The overlap ensures that context from the previous chunk is retained when merging them together. The overlap is managed by slicing the previous concatenated chunk to remove the overlapping part, then appending the next chunkâ€™s content. This creates a smooth flow between the chunks.\n",
    "- After merging the neighboring chunks, we append the resulting sequence to the `result_sequences list`. This list holds all the enriched context sequences for further processing or output.\n",
    "\n",
    "### Compare the baseline chunk the enriched chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de762393-9245-424b-8f4d-e81b08dcc70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular retrieval:\n",
      "\n",
      "ntribute \n",
      "to climate change. These forests are vital for regulating the Earth's climate and supporting \n",
      "indigenous communities and wildlife. \n",
      "Agriculture \n",
      "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
      "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
      "higher heat-trapping capability than CO2, albeit in smaller \n",
      "\n",
      "Retrieval with context enrichment:\n",
      "\n",
      "n. \n",
      "Boreal Forests \n",
      "Boreal forests, found in the northern regions of North America, Europe, and Asia, also play a \n",
      "crucial role in sequestering carbon. Logging and land-use changes in these regions contribute \n",
      "to climate change. These forests are vital for regulating the Earth's climate and supporting \n",
      "indigenous communities and wildlife. \n",
      "Agriculture \n",
      "Agriculture contributes to climate change through methane emissions from livestock, rice \n",
      "paddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \n",
      "higher heat-trapping capability than CO2, albeit in smaller quantities. \n",
      "Livestock Emissions \n",
      "Ruminant animals, such as cows and sheep, produce methane during digestion. Manure \n",
      "management practices also contribute to methane and nitrous oxide emissions. Innov\n"
     ]
    }
   ],
   "source": [
    "print(\"Regular retrieval:\\n\")\n",
    "print(relevant_chunks[0].page_content)  # The content of the baseline chunk\n",
    "\n",
    "print(\"\\nRetrieval with context enrichment:\\n\")\n",
    "print(result_sequences[0])  # Example of enriched context for the first relevant chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
