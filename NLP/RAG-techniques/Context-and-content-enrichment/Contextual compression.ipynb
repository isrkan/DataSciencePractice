{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67225169-3d2a-4e4c-9cdd-023108a237d3",
   "metadata": {},
   "source": [
    "# Contextual compression in document retrieval\n",
    "\n",
    "This notebook demonstrates how to implement contextual compression in a document retrieval system using LangChain and OpenAI's language models. In traditional document retrieval systems, queries typically return entire documents or large chunks of text, which may include irrelevant sections. Contextual compression helps address this issue by intelligently extracting only the most relevant content from the document, improving both the relevance and the efficiency of information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf52e78-14be-40df-b237-053ecd450e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f36eaf-c83e-4eb6-93f8-c53cc30872d1",
   "metadata": {},
   "source": [
    "### Load PDF documents\n",
    "We will use `PyPDFLoader` to extract text from the PDF. It reads the PDF page by page and stores the extracted text in a list of document objects, where each document contains the content of a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16791da1-41bd-40b2-bfdf-01e1631e35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c8ecb-22b4-4b3e-8e0b-cc31c1ef513d",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "##### Split the document into chunks\n",
    "We use the `RecursiveCharacterTextSplitter` to split the document into smaller chunks. This is ideal when working with large documents to make them manageable for embedding generation and for easier retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c945be38-778a-49fd-9f07-7ea536f4105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e0492-0cfd-4966-9823-8671a98244c6",
   "metadata": {},
   "source": [
    "we are splitting the document into chunks of size 1000 characters with 200 characters of overlap between chunks.\n",
    "- `chunk_size` makes it more manageable for indexing and retrieval.\n",
    "- `chunk_overlap` ensures that the context is preserved when the text is split. This helps maintain the flow of information between chunks.\n",
    "- `length_function` tells the splitter to calculate the length of the chunks based on the number of characters, ensuring that the chunks are exactly the specified size.\n",
    "\n",
    "##### Replace tabs with spaces\n",
    "In many cases, PDFs may contain tab characters (`\\t`) that were used for indentation but aren't necessary for the final processed text. We will replace these with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716d07b7-81eb-444b-ae5e-677ba7855ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace tab characters with spaces in the text content\n",
    "for text in texts:\n",
    "    text.page_content = text.page_content.replace('\\t', ' ')  # Replace tabs with spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38612d-5f3c-4bff-81b8-7fa66b3c3493",
   "metadata": {},
   "source": [
    "### Generate embeddings using OpenAI\n",
    "Once the text is cleaned and processed, we can create embeddings for each of the chunks using OpenAI API. These embeddings represent the meaning of the text in a high-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf5c7ac-0e02-4127-be39-8f514f770fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365d57f-1fcc-4c8e-848b-daeb5087cbe1",
   "metadata": {},
   "source": [
    "We will also use FAISS to efficiently store and index the embeddings, which allows us to perform similarity search and query efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b68843-80b5-4335-9055-c188c681097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using FAISS\n",
    "vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b9b43-4e20-4581-ae02-0ab6ab032d53",
   "metadata": {},
   "source": [
    "Here, we create a FAISS vector store by:\n",
    "- Generating embeddings for each chunk of text.\n",
    "- Storing the embeddings in FAISS, which allows fast similarity search later.\n",
    "\n",
    "This function automatically creates a flat (brute-force) index by default.\n",
    "\n",
    "### Setup the retriever\n",
    "Now that we have created the embeddings and stored them in FAISS, we can query the vector store to retrieve relevant information based on user queries. The retriever will help us fetch the top N most relevant document chunks based on a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d3b6d7-010f-46da-bade-bd8696ebc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb4700-a2b6-435c-b5ab-e3a14a29e317",
   "metadata": {},
   "source": [
    "### LLM-based contextual compressor\n",
    "Next, we introduce the LLM-based contextual compressor. This component uses a LLM to intelligently compress and extract only the most relevant content from the retrieved document chunks. The LLMChainExtractor is used for this purpose, which processes the retrieved chunks and extracts the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab860d42-d1b0-4479-8ce8-922828caf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model (ChatOpenAI) with specific configuration\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "# Create the LLMChainExtractor as a compressor\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f416d-ebe2-42e2-b8dc-dac80747b404",
   "metadata": {},
   "source": [
    "- We initialize the GPT-4-based language model. The `temperature=0` ensures deterministic, focused output, and `max_tokens=4000` specifies the maximum number of tokens the model can process.\n",
    "- Then, we use the `LLMChainExtractor` to extract relevant parts from the document chunks using the language model. It helps in compressing the document content, retaining only the most pertinent information. In other words, this process sets up an extraction chain that includes the following steps:\n",
    "    - Processing: The model processes the document chunks.\n",
    "    - Extracting: The model identifies and extracts the relevant sections based on the query and document context.\n",
    "    - Compressing: The extracted information is compressed to retain only the essential parts, ensuring brevity while maintaining relevance.\n",
    "\n",
    "### Contextual compression retriever\n",
    "We now combine the retriever and compressor into a ContextualCompressionRetriever. This retriever first fetches the most relevant document chunks using the base retriever and then compresses them using the compressor to extract the most important content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904acb94-d824-4774-947e-4b6ab84a751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the retriever with the compressor into a contextual compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848906-eb11-4c6d-ad75-c6b0a524be24",
   "metadata": {},
   "source": [
    "The `ContextualCompressionRetriever` combines both of these components — the retriever and the compressor — to produce a more powerful and efficient retrieval process:\n",
    "1. First stage - Retrieve relevant chunks - When a query is made, the base retriever searches through the vector store (which contains vector representations of the document chunks) and returns the top document chunks that are relevant to the query.\n",
    "2. Second stage: Compress and extract - Once the relevant chunks are retrieved, the compressor uses the `LLMChainExtractor` to compress the chunks, ensuring that only the most relevant and concise parts are kept. The compressor applies a language model to extract key information and remove unnecessary content, leaving only the essential details.\n",
    "\n",
    "### Question-answering chain\n",
    "Finally, we create a QA chain that integrates the contextual compression retriever to form a complete system capable of retrieving and answering queries with contextually compressed information. The QA chain takes a user's query, retrieves the relevant compressed context, and generates an answer based on the compressed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39de24a8-daa6-4031-9a02-d59b95d0f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QA chain with the compressed retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2ff12-dbc0-45aa-8f82-61c704fc35cb",
   "metadata": {},
   "source": [
    "The `RetrievalQA.from_chain_type` method is used to create the QA chain. This method brings together the LLM and the retriever (compression retriever) in a unified structure. The goal is to use the LLM to generate accurate answers from the retrieved and compressed context.\n",
    "- `llm=llm`: The language model is initialized earlier in the code as a ChatOpenAI model. This model is responsible for answering the query based on the retrieved content. The model works by processing the compressed document chunks and generating an answer that is relevant and coherent.\n",
    "- `retriever=compression_retriever`: The contextual compression retriever is used as the retriever in this QA chain. This retriever performs two main functions: first, it retrieves relevant chunks based on the query, and then it compresses these chunks to extract only the most relevant information. The compressed context is then passed to the language model to generate a focused answer.\n",
    "- `return_source_documents=True`: This flag ensures that the source documents that contributed to generating the answer are returned alongside the answer itself. This adds transparency to the process by showing which parts of the document the system relied on to generate the response. This is useful for users to validate the information and understand where the answer came from.\n",
    "\n",
    "### Test the system\n",
    "Now that we have set up the QA chain with the contextual compression retriever, we can test the system by sending it a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02aae609-9d04-4051-bb83-5608816c3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the main topic of the document?\"\n",
    "\n",
    "# Invoke the QA chain with the query\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92171-fb41-4870-abfa-13be76440e1c",
   "metadata": {},
   "source": [
    "The `qa_chain.invoke({\"query\": query})` command is where the magic happens. This line triggers the entire question-answering process, which involves the following steps:\n",
    "   - Retrieve relevant chunks: The contextual compression retriever fetches the most relevant document chunks that could contain information related to the query.\n",
    "   - Compression: The retriever then compresses these chunks to retain only the most critical content.\n",
    "   - Answer generation: The language model processes the compressed context and generates an answer to the query.\n",
    "   \n",
    "The result of invoking this QA chain will be a dictionary containing two key pieces of information:\n",
    "   - `\"result\"`: The answer to the query based on the compressed context.\n",
    "   - `\"source_documents\"`: A list of the document chunks or sources that contributed to generating the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6874c44c-f2ed-417f-a0ec-55b0cc8d1617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic of the document is climate change, focusing on its causes, effects, and potential solutions through global and local climate action, international collaboration, and national strategies. It discusses frameworks like the UNFCCC and the Paris Agreement, as well as various policies and practices aimed at reducing greenhouse gas emissions and promoting sustainability.\n",
      "\n",
      "Source documents: [Document(metadata={'source': 'Understanding_Climate_Change.pdf', 'page': 9}, page_content='Chapter 6: Global and Local Climate Action \\nInternational Collaboration \\nUnited Nations Framework Convention on Climate Change (UNFCCC) \\nThe UNFCCC is an international treaty aimed at addressing climate change. It provides a \\nframework for negotiating specific protocols and agreements, such as the Kyoto Protocol and \\nthe Paris Agreement. Global cooperation under the UNFCCC is crucial for coordinated \\nclimate action. \\nParis Agreement \\nThe Paris Agreement, adopted in 2015, aims to limit global warming to well below 2 degrees \\nCelsius above pre-industrial levels, with efforts to limit the increase to 1.5 degrees Celsius. \\nCountries submit nationally determined contributions (NDCs) outlining their climate action \\nplans and targets. \\nNational Strategies \\nCarbon Pricing \\nCarbon pricing mechanisms, such as carbon taxes and cap-and-trade systems, incentivize \\nemission reductions by assigning a cost to carbon emissions. These policies encourage'), Document(metadata={'source': 'Understanding_Climate_Change.pdf', 'page': 6}, page_content='protect ecosystems. Practices such as agroforestry, precision farming, and regenerative agriculture offer pathways to a more sustainable and resilient food system. By understanding the causes, effects, and potential solutions to climate change, we can take informed actions to protect our planet for future generations. Global cooperation, innovation, and commitment are key to addressing this pressing challenge.'), Document(metadata={'source': 'Understanding_Climate_Change.pdf', 'page': 0}, page_content='Chapter 2: Causes of Climate Change \\nGreenhouse Gases \\nThe primary cause of recent climate change is the increase in greenhouse gases in the \\natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \\noxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \\nfor life on Earth, as it keeps the planet warm enough to support life. However, human \\nactivities have intensified this natural process, leading to a warmer climate. \\nFossil Fuels \\nBurning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \\nnatural gas used for electricity, heating, and transportation. The industrial revolution marked \\nthe beginning of a significant increase in fossil fuel consumption, which continues to rise \\ntoday.'), Document(metadata={'source': 'Understanding_Climate_Change.pdf', 'page': 6}, page_content='Countries implement various policies to meet their climate goals, including carbon pricing, renewable energy incentives, and emissions regulations. National strategies must align with global targets while addressing local needs and capacities. Local governments and communities play a crucial role in climate action. Local initiatives can include urban planning, public transportation improvements, and community-based conservation. Grassroots movements and public awareness campaigns are also important for driving change at the local level. Continuous research and innovation are vital for developing new technologies and strategies to combat climate change. This includes advancements in renewable energy, carbon capture and storage, and sustainable agriculture. Collaboration between governments, industries, and academia is essential for fostering innovation.')]\n"
     ]
    }
   ],
   "source": [
    "# Print the result and source documents\n",
    "print(result[\"result\"])\n",
    "print(\"\\nSource documents:\", result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795dd23-d7f4-47aa-b204-937700ff8fc5",
   "metadata": {},
   "source": [
    "The system successfully processed the query by retrieving, compressing, and generating a focused answer from the relevant document content. validate the answers and trace the content that was used for generating the response.\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
