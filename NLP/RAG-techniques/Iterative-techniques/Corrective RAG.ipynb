{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rmhac8EsAvD"
   },
   "source": [
    "# Corrective RAG\n",
    "\n",
    "In many real-world applications, users expect question-answering systems to provide relevant, accurate, and up-to-date answers — even if the system’s local knowledge base (e.g., PDFs or internal documents) doesn’t have a direct answer.\n",
    "\n",
    "Standard RAG systems typically retrieve information based on similarity search and pass it to a language model. However, this approach can fail when retrieved content is irrelevant, ambiguous, or incomplete.\n",
    "\n",
    "This notebook implements a corrective RAG (CRAG) system — an enhanced, adaptive RAG pipeline that addresses such limitations by dynamically correcting itself. When local retrieval fails, it evaluates the retrieval quality, decides whether to trust it or fall back to web search, and refines both sources to synthesize a high-quality final answer. This enables CRAG to make intelligent decisions about how to retrieve and trust information. The goal is to enhance response quality and reliability, especially when internal document knowledge is insufficient or outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gbXDEpKZr_NQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwLEspPCsIVU"
   },
   "source": [
    "We use `DuckDuckGoSearchResults` as a fallback search utility for up-to-date online knowledge\n",
    "\n",
    "#### Load and index the document into a vector store\n",
    "To answer questions based on a document, we first need to turn it into a format our language model can search. Language models on their own don’t \"know\" the content of a PDF unless we explicitly make it available in a structured way. That is where embedding and vector search come in.\n",
    "\n",
    "This process builds a memory-like structure from our document that can be semantically searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GRAbYJufsIJg"
   },
   "outputs": [],
   "source": [
    "# Define the path to the PDF document we want to index\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and parse the PDF document into a list of text objects (usually one per page)\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()  # Extracts text from each page\n",
    "\n",
    "    # Split the document into smaller chunks, with overlap to maintain semantic continuity\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Each chunk will be around 1000 characters\n",
    "        chunk_overlap=chunk_overlap,  # Overlap 200 characters with the previous chunk\n",
    "        length_function=len  # Use raw character count to determine length\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # Replace tab characters with spaces\n",
    "    for doc in texts:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "\n",
    "    # Convert text chunks into dense vectors using OpenAI's embedding model\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Store the resulting embeddings into a FAISS index for fast similarity search\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Build the vector store from the PDF\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGZMFqzpsH9w"
   },
   "source": [
    "In this step, we:\n",
    "- Load the PDF into memory as raw text\n",
    "- Chunk the text into overlapping segments so that each chunk preserves context from its neighbors (this helps avoid cutting off important ideas mid-sentence)\n",
    "- Embed each chunk using OpenAI's embeddings model, converting text into numerical vectors\n",
    "- Store these vectors in a FAISS vector store, which allows for fast similarity searches based on user queries later\n",
    "\n",
    "So later, when a user asks a question, the system can search semantically — even if the question uses different wording than the document — and fetch the most relevant chunks of text from the original PDF.\n",
    "\n",
    "#### Initialize the language model\n",
    "We will use a lightweight OpenAI model to handle reasoning, classification, and generation throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-TXEVcHvsHx3"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaVKY1x_sHmL"
   },
   "source": [
    "This model will be used for multiple sub-tasks: relevance scoring, query rewriting, knowledge refinement, and final response generation. It is configured to minimize variability (`temperature = 0`) and to keep responses concise and safe from excessive verbosity (via `max_tokens`).\n",
    "\n",
    "#### Initialize the web search tool\n",
    "We will use DuckDuckGo, a privacy-preserving search engine, for external search when internal sources are weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UmJ5FQsssHam"
   },
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchResults(backend=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOeycWx2sHOD"
   },
   "source": [
    "This initializes a `DuckDuckGoSearchResults` tool from LangChain, which can be used inside chains to execute real-time web searches. It performs a backend HTTP request to DuckDuckGo’s search engine with our query and returns a list of the top results — including snippets, titles, and URLs.\n",
    "\n",
    "DuckDuckGo is chosen because it doesn't track users, doesn't require API keys, and provides fast, general-purpose search results by accessing to publicly available information. It is particularly useful in corrective RAG when we need to “correct” for poor document retrieval by pulling in additional knowledge from the web.\n",
    "\n",
    "Now, we have a reasoning engine (`llm`) and an external knowledge source (`search`) to rely on when document-based info is insufficient. The search object acts as a callable tool later in the pipeline.\n",
    "\n",
    "### Define core logic chains\n",
    "The strength of Corrective RAG lies in its dynamic decision-making — the ability to assess whether retrieved content is good enough, and to intelligently refine or seek external knowledge when it isn’t. This step defines three LLM-powered utility chains that enable that reasoning:\n",
    "- Relevance evaluator — Decides how useful a document is for a given question.\n",
    "- Knowledge refiner — Extracts structured, bullet-pointed insights from dense text.\n",
    "- Query rewriter — Optimizes user questions into web-search-friendly format.\n",
    "\n",
    "Let’s implement each of these.\n",
    "\n",
    "#### Relevance evaluator – Semantic scoring chain\n",
    "This function determines how closely a retrieved chunk of text matches a user query. Instead of relying on raw vector similarity (which can be noisy), we ask the language model to judge the match using its own understanding of meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lc0GkcHZzHOi"
   },
   "outputs": [],
   "source": [
    "# Retrieval Evaluator\n",
    "\n",
    "# Define input schema for structured relevance evaluation output\n",
    "class RetrievalEvaluatorInput(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of the document to the query. the score should be between 0 and 1.\")\n",
    "\n",
    "# Use the language model to assign a relevance score between 0 and 1 for a document-query pair\n",
    "def retrieval_evaluator(query: str, document: str) -> float:\n",
    "    # Define a prompt to ask the model for a relevance judgment\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"document\"],\n",
    "        template=\"On a scale from 0 to 1, how relevant is the following document to the query? Query: {query}\\nDocument: {document}\\nRelevance score:\"\n",
    "    )\n",
    "    # Combine the prompt with the model and enforce the output schema\n",
    "    chain = prompt | llm.with_structured_output(RetrievalEvaluatorInput)\n",
    "    # Provide inputs and retrieve the model's structured output\n",
    "    input_variables = {\"query\": query, \"document\": document}\n",
    "    result = chain.invoke(input_variables).relevance_score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqybZd7nzG_0"
   },
   "source": [
    "This function wraps a prompt inside a structured output chain, where the language model is asked to produce a numerical score (between 0 and 1) that quantifies how relevant the document is to the query. It is a more nuanced approach than raw vector cosine similarity, because it factors in true semantic alignment — helping us decide intelligently whether to trust the content or seek corrections.\n",
    "\n",
    "\n",
    "#### Knowledge refiner – Bullet-point summarization chain\n",
    "When we retrieve a long paragraph, it is often noisy or verbose. This module extracts just the essentials in bullet point form — clean, digestible, and ready for reasoning or inclusion in a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OoJcO8oMzSzE"
   },
   "outputs": [],
   "source": [
    "# Knowledge Refinement\n",
    "\n",
    "# Define output schema for refined key points\n",
    "class KnowledgeRefinementInput(BaseModel):\n",
    "    key_points: str = Field(..., description=\"The document to extract key information from.\")\n",
    "\n",
    "# Extract key bullet points from a given document\n",
    "def knowledge_refinement(document: str) -> List[str]:\n",
    "    # Prompt the LLM to summarize key insights as bullet points\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"document\"],\n",
    "        template=\"Extract the key information from the following document in bullet points:\\n{document}\\nKey points:\"\n",
    "    )\n",
    "    # Create a chain that outputs the structured key points\n",
    "    chain = prompt | llm.with_structured_output(KnowledgeRefinementInput)\n",
    "    # Get structured key points from model output\n",
    "    input_variables = {\"document\": document}\n",
    "    result = chain.invoke(input_variables).key_points\n",
    "    # Return list of cleaned, non-empty bullet lines\n",
    "    return [point.strip() for point in result.split('\\n') if point.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaLku8K5zSXV"
   },
   "source": [
    "Here, we build a prompt chain that forces the model to return a summarized set of key takeaways. The output is post-processed into a clean list by stripping empty lines or whitespace. This ensures retrieved knowledge — whether from internal docs or web results — is concise and actionable for downstream use.\n",
    "\n",
    "\n",
    "#### Query rewriter – Web search optimizer chain\n",
    "Often, natural user questions don’t translate well to search engine queries (they are too specific, vague, or conversational). This chain rewrites them into search-optimized phrases — improving the odds of getting good hits when we fall back to web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1iMjsC5AtPHp"
   },
   "outputs": [],
   "source": [
    "# Web Search Query Rewriter\n",
    "\n",
    "# Define output schema for rewritten query\n",
    "class QueryRewriterInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to rewrite.\")\n",
    "\n",
    "# Rewrite a user query to make it better suited for search engines\n",
    "def rewrite_query(query: str) -> str:\n",
    "    # Prompt to rephrase query for improved search performance\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"Rewrite the following query to make it more suitable for a web search:\\n{query}\\nRewritten query:\"\n",
    "    )\n",
    "    # Chain model output to structured query result\n",
    "    chain = prompt | llm.with_structured_output(QueryRewriterInput)\n",
    "    # Call model and extract the rewritten query\n",
    "    input_variables = {\"query\": query}\n",
    "    return chain.invoke(input_variables).query.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r7xhk8wtQj7"
   },
   "source": [
    "This function helps the model take a natural language question and reshape it into something more aligned with how people write queries for search engines — usually shorter, noun-based, and keyword-rich. By using structured output again, we ensure that the LLM returns exactly what we need: a single rewritten string, reliably.\n",
    "\n",
    "\n",
    "### Helper function to parse search results - — Clean titles & links from search output\n",
    "Once we run a DuckDuckGo web search (especially after rewriting a vague or incomplete query), we get back a blob of structured JSON. But we don’t need the whole thing — just clean titles and links that we can display, reference, or use in follow-up LLM steps.\n",
    "\n",
    "Let’s write a helper that extracts those essentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M1mvJry8tQWD"
   },
   "outputs": [],
   "source": [
    "# Parse a JSON-formatted string of search results into a list of (title, link) tuples\n",
    "def parse_search_results(results_string: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a JSON string of search results into a list of title-link tuples.\n",
    "\n",
    "    Args:\n",
    "        results_string (str): A JSON-formatted string containing search results.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples, where each tuple contains the title and link of a search result.\n",
    "                               If parsing fails, an empty list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the JSON string into a Python list of result dicts\n",
    "        results = json.loads(results_string)\n",
    "        # For each result dict, extract a title and a link; fallback if missing\n",
    "        return [(result.get('title', 'Untitled'), result.get('link', '')) for result in results]\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle JSON decoding errors by returning an empty list\n",
    "        print(\"Error parsing search results. Returning empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOcyH86xtQIg"
   },
   "source": [
    "After a DuckDuckGo search is triggered, we get back a JSON string — each item being a small dictionary with fields like \"title\", \"snippet\", \"link\", and so on. This function is focused purely on distilling those into minimal (title, link) pairs. If the string is malformed or empty, it returns an empty list and logs the issue, preventing our pipeline from crashing downstream.\n",
    "\n",
    "### Sub-functions for the CRAG process\n",
    "The following utility functions represent the core logic that glues together our corrective RAG system — from document retrieval to response generation. These modular subroutines keep the pipeline clean, interpretable, and adaptable to different retrieval or generation strategies.\n",
    "\n",
    "#### Document retrieval from FAISS\n",
    "We start with pulling relevant documents from the vector store (FAISS) using semantic similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mvf_mOjLVCjI"
   },
   "outputs": [],
   "source": [
    "# Retrieve top-k documents from a FAISS vector index based on the input query\n",
    "def retrieve_documents(query: str, faiss_index: FAISS, k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on a query using a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "        faiss_index (FAISS): The FAISS index used for similarity search.\n",
    "        k (int): The number of top documents to retrieve. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of the retrieved document contents.\n",
    "    \"\"\"\n",
    "    # Perform similarity search using vector distance\n",
    "    docs = faiss_index.similarity_search(query, k=k)\n",
    "    # Return only the content (not metadata or embeddings)\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujsHiN2WVDMp"
   },
   "source": [
    "This function connects to the FAISS index to retrieve the most semantically similar documents for a user query. It uses approximate nearest neighbor (ANN) search under the hood — matching the vectorized query to the top `k` most relevant embedded chunks. This is our internal memory source.\n",
    "\n",
    "#### Relevance scoring\n",
    "Once we have documents, we still need to evaluate how well they match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_vJ5y9MWVeFD"
   },
   "outputs": [],
   "source": [
    "# Evaluate each document's relevance to the given query using LLM-based semantic scoring\n",
    "def evaluate_documents(query: str, documents: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of documents based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string.\n",
    "        documents (List[str]): A list of document contents to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: A list of relevance scores for each document.\n",
    "    \"\"\"\n",
    "    # Run the LLM-based scoring function for each document\n",
    "    return [retrieval_evaluator(query, doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65qnUbN4Vdnz"
   },
   "source": [
    "Rather than relying solely on vector distance (which can be noisy), this function leverages the LLM to assign a 0–1 relevance score for each document. It is essentially a second-pass semantic filter — helpful in surfacing more contextually aligned sources.\n",
    "\n",
    "#### Web search with rewriting and knowledge extraction\n",
    "When local documents are not sufficient, this function performs an augmented web search — rewrites the query, runs the search, refines the output, and collects sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xjwPNq0pV7ne"
   },
   "outputs": [],
   "source": [
    "# Run a full web search pipeline — rewrite, search, extract knowledge, and parse sources\n",
    "def perform_web_search(query: str) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Perform a web search based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[Tuple[str, str]]]:\n",
    "            - A list of refined knowledge obtained from the web search.\n",
    "            - A list of tuples containing titles and links of the sources.\n",
    "    \"\"\"\n",
    "    # Rewrite user query to better suit web search engine expectations\n",
    "    rewritten_query = rewrite_query(query)\n",
    "    # Run the rewritten query through DuckDuckGo\n",
    "    web_results = search.run(rewritten_query)\n",
    "    # Summarize web results into bullet-pointed insights\n",
    "    web_knowledge = knowledge_refinement(web_results)\n",
    "    # Parse result metadata (titles and URLs) for attribution\n",
    "    sources = parse_search_results(web_results)\n",
    "    return web_knowledge, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Acw4wkNPV7Wl"
   },
   "source": [
    "This full pipeline upgrades vague or domain-specific user queries into web-searchable prompts, scrapes live DuckDuckGo results, summarizes them, and extracts source links. This is our “external brain” when the vector store lacks sufficient context.\n",
    "\n",
    "#### Final response generation with source attribution\n",
    "Once we have usable knowledge (from local or web), we generate a natural-language response, optionally including sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nN-yCPp5tX-N"
   },
   "outputs": [],
   "source": [
    "# Use the LLM to generate a complete answer using knowledge and source metadata\n",
    "def generate_response(query: str, knowledge: str, sources: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response to a query using knowledge and sources.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string.\n",
    "        knowledge (str): The refined knowledge to use in the response.\n",
    "        sources (List[Tuple[str, str]]): A list of tuples containing titles and links of the sources.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"knowledge\", \"sources\"],\n",
    "        template=\"Based on the following knowledge, answer the query. Include the sources with their links (if available) at the end of your answer:\\nQuery: {query}\\nKnowledge: {knowledge}\\nSources: {sources}\\nAnswer:\"\n",
    "    )\n",
    "    # Format source strings (title: link) for display\n",
    "    input_variables = {\n",
    "        \"query\": query,\n",
    "        \"knowledge\": knowledge,\n",
    "        \"sources\": \"\\n\".join([f\"{title}: {link}\" if link else title for title, link in sources])\n",
    "    }\n",
    "    # Use LLM to synthesize final answer\n",
    "    response_chain = response_prompt | llm\n",
    "    return response_chain.invoke(input_variables).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyyQ8KaptZkU"
   },
   "source": [
    "It gives the LLM everything it needs: the refined knowledge and the query itself, along with structured attribution info (titles + links). The prompt encourages the model to cite its sources — a step toward responsible AI output.\n",
    "\n",
    "### CRAG process\n",
    "Finally, we define the full orchestration pipeline that performs dynamic correction based on document relevance. This is the decision-making brain of the Corrective RAG system. It routes user queries through the right path — deciding whether to trust retrieved content, fall back to the web, or intelligently merge both. This approach adds resilience and robustness to our RAG workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "29Jw4KlhtZN-"
   },
   "outputs": [],
   "source": [
    "# Full corrective RAG pipeline\n",
    "def crag_process(query: str, faiss_index: FAISS) -> str:\n",
    "    \"\"\"\n",
    "    Process a query by retrieving, evaluating, and using documents or performing a web search to generate a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to process.\n",
    "        faiss_index (FAISS): The FAISS index used for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response based on the query.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "    # Step 1: Try to retrieve relevant documents from local FAISS index\n",
    "    retrieved_docs = retrieve_documents(query, faiss_index)\n",
    "\n",
    "    # Step 2: Evaluate how semantically aligned each document is\n",
    "    eval_scores = evaluate_documents(query, retrieved_docs)\n",
    "\n",
    "    print(f\"\\nRetrieved {len(retrieved_docs)} documents\")\n",
    "    print(f\"Evaluation scores: {eval_scores}\")\n",
    "\n",
    "    # Step 3: Decide which action to use (threshold-based logic) based on evaluation scores\n",
    "    max_score = max(eval_scores)\n",
    "    sources = []\n",
    "\n",
    "    if max_score > 0.7:\n",
    "        # Confident in internal content — no correction needed\n",
    "        print(\"\\nAction: Correct - Using retrieved document\")\n",
    "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
    "        final_knowledge = best_doc\n",
    "        sources.append((\"Retrieved document\", \"\"))\n",
    "    elif max_score < 0.3:\n",
    "        # Retrieved data is weak — fallback to live web search\n",
    "        print(\"\\nAction: Incorrect - Performing web search\")\n",
    "        final_knowledge, sources = perform_web_search(query)\n",
    "    else:\n",
    "        # Ambiguous case — fuse the best retrieved chunk with external knowledge\n",
    "        print(\"\\nAction: Ambiguous - Combining retrieved document and web search\")\n",
    "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
    "        # Summarize the best local document\n",
    "        retrieved_knowledge = knowledge_refinement(best_doc)\n",
    "        # Get web knowledge and sources\n",
    "        web_knowledge, web_sources = perform_web_search(query)\n",
    "        # Combine both sets of knowledge\n",
    "        final_knowledge = \"\\n\".join(retrieved_knowledge + web_knowledge)\n",
    "        sources = [(\"Retrieved document\", \"\")] + web_sources\n",
    "\n",
    "    print(\"\\nFinal knowledge:\")\n",
    "    print(final_knowledge)\n",
    "\n",
    "    print(\"\\nSources:\")\n",
    "    for title, link in sources:\n",
    "        print(f\"{title}: {link}\" if link else title)\n",
    "\n",
    "    # Step 4: Use LLM to generate final answer based on knowledge and sources\n",
    "    print(\"\\nGenerating response...\")\n",
    "    response = generate_response(query, final_knowledge, sources)\n",
    "\n",
    "    print(\"\\nResponse generated\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZAgQSZxtZAN"
   },
   "source": [
    "This function orchestrates the full decision tree for corrective generation. First, it checks whether the internal FAISS-based documents are good enough. If they are (based on an LLM-assigned score > 0.7), it proceeds directly. If they are clearly poor (< 0.3), it skips the internal data entirely and runs a full web search. In edge cases (scores between 0.3–0.7), it merges insights from both — a kind of hybrid mode that reduces hallucination and improves relevance.\n",
    "\n",
    "Each path results in structured, cleaned-up knowledge and a source list, which is then passed into the generation model. This means our response is always grounded either in a trusted internal corpus or in fresh, attributed information from the web.\n",
    "\n",
    "\n",
    "### Example: Query with high Rrelevance to internal documents\n",
    "This test case demonstrates how the system handles a well-covered topic that is already included in the internal knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SEsSknaktmUY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What are the main causes of climate change?\n",
      "\n",
      "Retrieved 3 documents\n",
      "Evaluation scores: [0.9, 0.9, 0.7]\n",
      "\n",
      "Action: Correct - Using retrieved document\n",
      "\n",
      "Final knowledge:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "Sources:\n",
      "Retrieved document\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response generated\n",
      "Query: What are the main causes of climate change?\n",
      "Answer: The main causes of climate change can be attributed primarily to the increase in greenhouse gases in the atmosphere, which is largely driven by human activities. Here are the key factors:\n",
      "\n",
      "1. **Greenhouse Gases**: The primary cause of recent climate change is the rise in greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). These gases trap heat from the sun, creating a \"greenhouse effect\" that is essential for maintaining the Earth's temperature. However, human activities have significantly intensified this natural process, leading to a warmer climate.\n",
      "\n",
      "2. **Fossil Fuels**: The burning of fossil fuels (coal, oil, and natural gas) for energy is a major contributor to the increase in CO2 levels. This includes their use in electricity generation, heating, and transportation. The industrial revolution marked a significant rise in fossil fuel consumption, a trend that continues to escalate today.\n",
      "\n",
      "3. **Coal**: Specifically, coal is one of the fossil fuels that, when burned, releases substantial amounts of CO2 into the atmosphere, further exacerbating climate change.\n",
      "\n",
      "These factors collectively contribute to the ongoing changes in the Earth's climate, leading to various environmental impacts.\n",
      "\n",
      "Sources: Retrieved document.\n"
     ]
    }
   ],
   "source": [
    "# A factual question well-aligned with internal corpus content\n",
    "query = \"What are the main causes of climate change?\"\n",
    "\n",
    "# Process the query using the CRAG pipeline\n",
    "result = crag_process(query, vectorstore)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oSg_JdbtlzN"
   },
   "source": [
    "Here, the internal documents — containing detailed environmental data — are highly relevant to the query. The retrieval step successfully pulls one or more useful passages, and the model assigns a high semantic relevance score (above the threshold). Because of that, CRAG confidently skips external search and directly uses retrieved content for generation. This path represents the “correct” correction state: no intervention needed, only internal data is used.\n",
    "\n",
    "### Example: Query with low relevance to internal documents\n",
    "This example shows what happens when the system encounters a completely out-of-domain or fictional topic that is not present in our indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dSTQx8Ljtp0l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: how did harry beat quirrell?\n",
      "\n",
      "Retrieved 3 documents\n",
      "Evaluation scores: [0.0, 0.0, 0.0]\n",
      "\n",
      "Action: Incorrect - Performing web search\n",
      "Error parsing search results. Returning empty list.\n",
      "\n",
      "Final knowledge:\n",
      "[]\n",
      "\n",
      "Sources:\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response generated\n",
      "Query: how did harry beat quirrell?\n",
      "Answer: In \"Harry Potter and the Sorcerer's Stone,\" Harry Potter defeats Professor Quirrell during the climax of the story. Quirrell, who is possessed by Lord Voldemort, attempts to steal the Sorcerer's Stone to gain immortality. When Harry confronts Quirrell in the underground chamber, he is unable to touch Harry without suffering severe pain. This is because Harry is protected by the sacrificial love of his mother, Lily Potter, which creates a magical barrier against Voldemort.\n",
      "\n",
      "As Quirrell tries to seize the Stone from Harry, he experiences intense agony when he makes contact with him. This ultimately leads to Quirrell's defeat, as he cannot withstand the protective magic that Harry carries. The confrontation ends with Quirrell's demise, allowing Harry to prevent Voldemort from obtaining the Stone.\n",
      "\n",
      "Sources:\n",
      "1. \"Harry Potter and the Sorcerer's Stone\" by J.K. Rowling (book)\n",
      "2. [Harry Potter Wiki - Quirinus Quirrell](https://harrypotter.fandom.com/wiki/Quirinus_Quirrell)\n"
     ]
    }
   ],
   "source": [
    "# A fictional question unlikely to be covered in the local index\n",
    "query = \"how did harry beat quirrell?\"\n",
    "\n",
    "# Process the query through CRAG to trigger fallback behavior\n",
    "result = crag_process(query, vectorstore)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0D5fViktqJ0"
   },
   "source": [
    "Here, the vector store returns irrelevant or empty results, and the evaluator assigns very low semantic scores. The system interprets this as a signal that internal data is not useful and invokes a full web search instead. This demonstrates the “incorrect” path — where corrective behavior is fully triggered. The query is rewritten for search engine compatibility, fresh content is retrieved and refined, and the final answer is generated from that external source. This fallback is what gives CRAG its resilience and adaptability."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
