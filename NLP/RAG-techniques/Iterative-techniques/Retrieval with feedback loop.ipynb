{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyLk6aAv49bV"
   },
   "source": [
    "# RAG system with feedback loop\n",
    "\n",
    "In this notebook, we implement a RAG pipeline enhanced with a user feedback loop. The goal is to improve the accuracy, relevance, and personalization of answers over time by:\n",
    "- Dynamically adjusting document relevance based on user feedback.\n",
    "- Periodically fine-tuning the document index with high-quality interactions.\n",
    "\n",
    "This kind of feedback-aware architecture is ideal for applications like education, customer support, and expert systems where relevance and quality of responses matter — and evolve — with use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vDKw_61B44DO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import PromptTemplate\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import pymupdf\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brmcPE8z5UZ0"
   },
   "source": [
    "### Load the PDF\n",
    "The first step in any RAG system is to ingest source content. In our case, we are loading and reading a PDF file to build our knowledge base.\n",
    "\n",
    "Here, we read the PDF content and convert it into a string using the `pymupdf` library. This will allow us to process the text further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Rzv3lHT5Trx"
   },
   "outputs": [],
   "source": [
    "# Path to the PDF document\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Open the PDF document located at the specified path\n",
    "doc = pymupdf.open(path)\n",
    "content = \"\"\n",
    "# Iterate over each page in the document\n",
    "for page_num in range(len(doc)):\n",
    "    # Get the current page\n",
    "    page = doc[page_num]\n",
    "    # Extract the text content from the current page and append it to the content string\n",
    "    content += page.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfTRLf3658M4"
   },
   "source": [
    "The PDF file is opened using `pymupdf.open()`, and we iterate over all pages to extract the text. The extracted text is then concatenated into one large string. This will allow us to process it further and split it into manageable chunks.\n",
    "\n",
    "\n",
    "### Chunking and embedding the text\n",
    "Now that we have the text, we split it into overlapping chunks and convert those into semantic embeddings. These chunks are what the retriever will later search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H6RPhpbwAEny"
   },
   "outputs": [],
   "source": [
    "# Define chunking parameters\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "# Create the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "# Split the text into manageable overlapping segments\n",
    "chunks = text_splitter.create_documents([content])\n",
    "\n",
    "# Initialize metadata for scoring\n",
    "for chunk in chunks:\n",
    "    chunk.metadata['relevance_score'] = 1.0\n",
    "\n",
    "# Generate embeddings and create the vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "204RgeyP_3Fq"
   },
   "source": [
    "We divide the raw text into overlapping sections to preserve context during retrieval. Each chunk gets embedded into a vector space using OpenAI’s embedding model and stored in a FAISS index for fast similarity search.\n",
    "\n",
    "### Setting up the retriever and QA chain\n",
    "We will now define a retriever to fetch relevant chunks and connect it with a LLM using a RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZMYqwcKd57qc"
   },
   "outputs": [],
   "source": [
    "# Convert vector index into a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Load the LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "# Wrap everything in a RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh_4Zjhf57EE"
   },
   "source": [
    "This QA system works by retrieving relevant chunks of text from the vector index and passing them to the language model to generate contextually relevant answers. We are using LangChain's built-in `RetrievalQA` class, which is a wrapper that combines:\n",
    "- A retriever — this pulls in the most relevant text chunks (in this case, from our FAISS vectorstore).\n",
    "- An LLM — which then uses those chunks to generate a response to the user’s query.\n",
    "\n",
    "### Function to format user feedback in a dictionary\n",
    "Once a response is generated, we can collect user feedback on its relevance and quality, and persist that information locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XcqgSuKU56jp"
   },
   "outputs": [],
   "source": [
    "# Create structured feedback\n",
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance),\n",
    "        \"quality\": int(quality),\n",
    "        \"comments\": comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR34W1fI56R6"
   },
   "source": [
    "### Function to store the feedback in a json file\n",
    "Feedback is stored in JSON lines format for easy incremental access and later learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hVn_q4P96H7G"
   },
   "outputs": [],
   "source": [
    "# Save feedback to disk\n",
    "def store_feedback(feedback):\n",
    "    with open(\"data/feedback_data.json\", \"a\") as f:\n",
    "        json.dump(feedback, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_gEz4oG6IX-"
   },
   "source": [
    "### Function to load past feedback file\n",
    "We will need to access past feedbacks to learn from them and adjust our document ranking accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lglGZGTL6O4t"
   },
   "outputs": [],
   "source": [
    "def load_feedback_data():\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(\"data/feedback_data.json\", \"r\") as f:\n",
    "            for line in f:\n",
    "                feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQiIIYYr6QVF"
   },
   "source": [
    "This function safely loads all prior feedback entries from the local file, making them available for relevance adjustments. This ensures that the system can leverage cumulative user knowledge to evolve over time.\n",
    "\n",
    "\n",
    "### Function to adjust files relevance scores based on the feedbacks file\n",
    "To make the system adaptive, we use the LLM to reason whether a past feedback is relevant to the current query and document context. This allows us to dynamically reweight document scores before retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MX94mz_p6QDL"
   },
   "outputs": [],
   "source": [
    "# Define the expected structured response from the LLM\n",
    "class Response(BaseModel):\n",
    "    answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n",
    "\n",
    "# Function to adjust relevance scores using prior user feedback\n",
    "def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n",
    "    # Create a prompt template for relevance checking\n",
    "    relevance_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"feedback_query\", \"doc_content\", \"feedback_response\"],\n",
    "        template=\"\"\"\n",
    "        Determine if the following feedback response is relevant to the current query and document content.\n",
    "        You are also provided with the Feedback original query that was used to generate the feedback response.\n",
    "        Current query: {query}\n",
    "        Feedback query: {feedback_query}\n",
    "        Document content: {doc_content}\n",
    "        Feedback response: {feedback_response}\n",
    "\n",
    "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Initialize the LLM to use for reasoning\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "    # Create an LLMChain for relevance checking - Combine prompt and LLM with structured output (expects 'Yes' or 'No')\n",
    "    relevance_chain = relevance_prompt | llm.with_structured_output(Response)\n",
    "\n",
    "    # Loop through each retrieved document\n",
    "    for doc in docs:\n",
    "        relevant_feedback = []  # Holds feedback relevant to this doc+query pair\n",
    "\n",
    "        # Loop through all historical feedback entries\n",
    "        for feedback in feedback_data:\n",
    "            # Use LLM to check relevance\n",
    "            input_data = {\n",
    "                \"query\": query,  # Current question\n",
    "                \"feedback_query\": feedback['query'],  # Feedback's original question\n",
    "                \"doc_content\": doc.page_content[:1000],  # Truncated doc content for LLM context\n",
    "                \"feedback_response\": feedback['response']  # The response the user rated\n",
    "            }\n",
    "            # Let the LLM judge if the feedback applies to this document and query\n",
    "            result = relevance_chain.invoke(input_data).answer\n",
    "\n",
    "            # If LLM says \"yes\", we treat this feedback as relevant\n",
    "            if result == 'yes':\n",
    "                relevant_feedback.append(feedback)\n",
    "\n",
    "        # If we found relevant feedback, adjust the relevance score based on feedback\n",
    "        if relevant_feedback:\n",
    "            # Compute average relevance rating from user\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            # Adjust document's score: scale it relative to a neutral value (3)\n",
    "            doc.metadata['relevance_score'] *= (avg_relevance / 3)  # Assuming a 1-5 scale, 3 is neutral\n",
    "\n",
    "    # Finally, sort the documents by adjusted score in descending order\n",
    "    return sorted(docs, key=lambda x: x.metadata['relevance_score'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3ukbtuH6ZmL"
   },
   "source": [
    "This method dynamically adjusts the relevance score of each document using both user feedback and reasoning from the LLM. It ensures that more trusted documents are prioritized in future retrievals.\n",
    "\n",
    "We take each document that was retrieved for a new query, and we ask the LLM. The LLM gets the new question, the current document (trimmed), the feedback question, and the feedback's rated response. It evaluates this and replies either “Yes” or “No”. If “Yes”, we treat that feedback as if it applies to this situation too.\n",
    "\n",
    "Once we have gathered relevant feedback for a document, we compute the average relevance score from those feedback items, and use that to multiply the document's score, with 3 as our neutral baseline.\n",
    "\n",
    "By the end, documents are re-ranked based on this adjusted score, meaning the ones more validated by historical feedback rise to the top.\n",
    "\n",
    "### Function to fine tune the vector index to include also queries + answers that received good feedbacks\n",
    "Periodically, we can also retrain the vectorstore by incorporating high-scoring user interactions to our index to reinforce strong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SF31K_mc6bS8"
   },
   "outputs": [],
   "source": [
    "def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n",
    "    # Filter high-quality responses\n",
    "    good_responses = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "\n",
    "    # Extract queries and responses, and create new documents\n",
    "    additional_texts = []\n",
    "    for f in good_responses:\n",
    "        # Merge question and answer into a single context block\n",
    "        combined_text = f['query'] + \" \" + f['response']\n",
    "        additional_texts.append(combined_text)\n",
    "\n",
    "    # make the list a string\n",
    "    additional_texts = \" \".join(additional_texts)\n",
    "\n",
    "    # Create a new index with original and high-quality texts\n",
    "    all_texts = texts + additional_texts\n",
    "\n",
    "\n",
    "    # Define chunking parameters\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 200\n",
    "\n",
    "    # Split the text into manageable overlapping segments\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.create_documents([all_texts])\n",
    "\n",
    "    # Initialize metadata for scoring\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata['relevance_score'] = 1.0\n",
    "\n",
    "    # Generate embeddings and create the vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    new_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return new_vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeyG-Dqn6hn8"
   },
   "source": [
    "This block expands the document base with reliable user-evaluated interactions, strengthening the system’s contextual grounding and personalization over time.\n",
    "\n",
    "First, we go through all the user feedback and pick only the ones that were rated highly — so we are not polluting our knowledge base with weak or misleading data. Then we treat these high-quality Q&A examples just like source content — we stitch the question and its answer together and toss it into our data pool.\n",
    "\n",
    "We combine everything — old documents and new Q&A — into one big string and slice it into manageable chunks, like we did with the original PDF. Each chunk gets embedded into a vector and inserted into a FAISS index. From this point on, our retriever is not just searching the original document — it is also able to surface these strong answers when relevant.\n",
    "\n",
    "The beauty here is that we are not retraining a language model. We are just evolving the retrieval layer with user input. That is efficient, safe, and very aligned with real-world needs.\n",
    "\n",
    "### Full system demo: Query → Feedback → Adjustment\n",
    "Here’s how the full pipeline works from query to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2GPBoanD6lCE"
   },
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "query = \"What is the greenhouse effect?\"\n",
    "\n",
    "# Step 1: Get response from RAG system\n",
    "response = qa_chain.invoke(query)[\"result\"]\n",
    "\n",
    "# Step 2: Assume the user provides high scores\n",
    "relevance = 5\n",
    "quality = 5\n",
    "# Collect feedback\n",
    "feedback = get_user_feedback(query, response, relevance, quality)\n",
    "\n",
    "# Step 3: Store feedback\n",
    "store_feedback(feedback)\n",
    "\n",
    "# Step 4: Adjust relevance scores for future retrievals\n",
    "docs = retriever.invoke(query)\n",
    "adjusted_docs = adjust_relevance_scores(query, docs, load_feedback_data())\n",
    "\n",
    "# Update the retriever with adjusted docs\n",
    "retriever.search_kwargs['k'] = len(adjusted_docs)  # Set k to number of adjusted docs\n",
    "retriever.search_kwargs['docs'] = adjusted_docs   # Inject the re-scored documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIgZ6LtQ6nO1"
   },
   "source": [
    "This executes a full loop: generate → collect feedback → adapt ranking → prepare for improved next retrieval.\n",
    "\n",
    "A query is asked, a response is generated, and the user gives feedback. But instead of just logging that feedback and moving on, we immediately put it to use.\n",
    "\n",
    "We fetch the documents again, and now we use our adjustment logic: does any of the previous feedback apply to these documents? If so, we change how important those documents are. The ones supported by high-quality past feedback are bumped up in ranking, the others stay the same or get nudged down.\n",
    "\n",
    "Then, instead of re-indexing the vectorstore (which is heavier), we do something much lighter: we tell the retriever, “Hey, next time you use this query, take these docs and treat them as pre-ranked based on learned preferences.” It's an effective way to make our system feel smarter without retraining anything. It gives the illusion of learning — because in a way, it is.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Finetune the vectorstore periodicly\n",
    "This should be scheduled periodically (e.g., daily) to keep the system sharp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u38bv0qJ6qvf"
   },
   "outputs": [],
   "source": [
    "# Periodically (e.g., daily or weekly), fine-tune the index\n",
    "feedback_data = load_feedback_data()\n",
    "new_vectorstore = fine_tune_index(feedback_data, content)\n",
    "retriever = new_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI15Imcc6rXq"
   },
   "source": [
    "This ensures the index keeps learning from its best interactions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
