{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BK5tY_OGEJr"
   },
   "source": [
    "# Self-RAG\n",
    "\n",
    "In this notebook, we implement a self-RAG pipeline — a retrieval-augmented generation approach that introduces decision-making and quality control steps into the generation workflow. Unlike standard RAG, which always retrieves documents and generates text based on them, self-RAG adds layers of introspection:\n",
    "- Should retrieval be done at all?\n",
    "- Are the retrieved documents relevant?\n",
    "- Is the generated answer supported by the documents?\n",
    "- Is the answer useful to the user?\n",
    "\n",
    "This method leads to more controlled, relevant, and grounded responses, especially in real-world applications where quality and reliability matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l5mcRiAcvkHW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBR6NNFeGM0C"
   },
   "source": [
    "#### Load and index the document into a vector store\n",
    "To answer questions based on a document, we first need to turn it into a format our language model can search. Language models on their own don’t \"know\" the content of a PDF unless we explicitly make it available in a structured way. That is where embedding and vector search come in.\n",
    "\n",
    "This process builds a memory-like structure from your document that can be semantically searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TOfgnYACGMoq"
   },
   "outputs": [],
   "source": [
    "# Define the path to the PDF document we want to index\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load and parse the PDF document into a list of text objects (usually one per page)\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split the document into smaller chunks, with overlap to maintain semantic continuity\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,  # Each chunk will be around 1000 characters\n",
    "        chunk_overlap=chunk_overlap,  # Overlap 200 characters with the previous chunk\n",
    "        length_function=len  # Use raw character count to determine length\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # Replace tab characters with spaces\n",
    "    for doc in texts:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "\n",
    "    # Convert text chunks into dense vectors using OpenAI's embedding model\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Store the resulting embeddings into a FAISS index for fast similarity search\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Build the vector store from the PDF\n",
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcwZyrtOGMdE"
   },
   "source": [
    "In this step, we:\n",
    "- Load the PDF into memory as raw text\n",
    "- Chunk the text into overlapping segments so that each chunk preserves context from its neighbors (this helps avoid cutting off important ideas mid-sentence)\n",
    "- Embed each chunk using OpenAI's embeddings model, converting text into numerical vectors\n",
    "- Store these vectors in a FAISS vector store, which allows for fast similarity searches based on user queries later\n",
    "\n",
    "So later, when a user asks a question, the system can search semantically — even if the question uses different wording than the document — and fetch the most relevant chunks of text from the original PDF.\n",
    "\n",
    "#### Initialize the language model\n",
    "We will use a lightweight OpenAI model to handle reasoning, classification, and generation throughout the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JYWgQmovGMR9"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", max_tokens=1000, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws002h4TGMF7"
   },
   "source": [
    "We rely on this model not just for generating answers, but also for making structured decisions (like whether to retrieve or how good an answer is). Keeping temperature at 0 ensures deterministic outputs.\n",
    "\n",
    "### Designing structured prompts for the self-RAG pipeline\n",
    "Self-RAG is not just about pulling information and generating text. It is a structured, self-reflective reasoning system — and to support that, we need a set of prompts that serve specific roles in the pipeline.\n",
    "\n",
    "Each stage in the process — whether it is deciding if we even need to retrieve documents, checking whether retrieved content is relevant, generating a response, or evaluating how strong and useful that response is — requires a different kind of instruction to the language model.\n",
    "\n",
    "This is where prompt templates come in. We carefully craft prompts that tell the model exactly what its job is at each step. But instead of treating the output like a blob of text, we define schemas (using `pydantic` models) to enforce structure — this makes the output reliable and machine-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "coXBhwWBGL6O"
   },
   "outputs": [],
   "source": [
    "### Define specialized prompts and structured response schemas for each reasoning stage ###\n",
    "\n",
    "# ---- Schema: Retrieval decision ----\n",
    "# Decide if retrieval is needed\n",
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if retrieval is necessary\", description=\"Output only 'Yes' or 'No'.\")\n",
    "\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Given the query '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Schema: Relevance check ----\n",
    "# Determines if a retrieved chunk is relevant to the user's query\n",
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if context is relevant\", description=\"Output only 'Relevant' or 'Irrelevant'.\")\n",
    "\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Schema: Response generation ----\n",
    "# Generates an actual answer given the context and query\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Generated response\", description=\"The generated response.\")\n",
    "\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Schema: Support assessment ----\n",
    "# Checks if the generated response is actually supported by the retrieved context\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Determines if response is supported\", description=\"Output 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"Given the response '{response}' and the context '{context}', determine if the response is supported by the context. Output 'Fully supported', 'Partially supported', or 'No support'.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Schema: Utility rating ----\n",
    "# Scores the usefulness of the response on a 1 to 5 scale\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., title=\"Utility rating\", description=\"Rate the utility of the response from 1 to 5.\")\n",
    "\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"Given the query '{query}' and the response '{response}', rate the utility of the response from 1 to 5.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Create LLM chains for each step ----\n",
    "# Each chain combines a template and a schema to ensure consistent LLM outputs\n",
    "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
    "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
    "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
    "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
    "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X129lIPXGLsP"
   },
   "source": [
    "- Each `PromptTemplate` defines how we talk to the language model: the phrasing, inputs, and instructions for each task. Each prompt is tailored to a single, specific task, making it easy to test and optimize independently.\n",
    "- The corresponding `BaseModel` class (e.g., `RetrievalResponse`, `UtilityResponse`) defines the expected output format. This makes sure outputs are well-structured and easy to parse (no free-form text that we have to regex or guess).\n",
    "- We combine the prompt with the model using `with_structured_output(...)`, which ensures that the model's raw response is parsed into a Python object with named fields (rather than unstructured text).\n",
    "- The chaining (`prompt | llm.with_structured_output(...)`) composes the logic: first the prompt is filled with inputs, then the LLM processes it, and finally the output is parsed and validated using the schema.\n",
    "- Each \"chain\" (like `retrieval_chain`) becomes a decision point we can invoke in the pipeline — pass in a query, get back a clean, validated result.\n",
    "\n",
    "This gives us strong control over LLM behavior, making each component in self-RAG deterministic, interpretable, and easy to debug.\n",
    "\n",
    "\n",
    "### Defining the self RAG logic flow\n",
    "This is where everything comes together into an actual thinking pipeline. Based on the user query, the system dynamically determines the best course of action and selects the most trustworthy and useful response.\n",
    "\n",
    "Each of these decisions happens through a structured reasoning chain, so the model is effectively self-regulating — questioning its own steps before answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_3IsCNwDGLcb"
   },
   "outputs": [],
   "source": [
    "def self_rag(query, vectorstore, top_k=3):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "    # Step 1: Decide if we need external knowledge\n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    input_data = {\"query\": query}\n",
    "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
    "    print(f\"Retrieval decision: {retrieval_decision}\")\n",
    "\n",
    "    if retrieval_decision == 'yes':\n",
    "        # Step 2: Retrieve top-k similar chunks from the vector store\n",
    "        print(\"Step 2: Retrieving relevant documents...\")\n",
    "        docs = vectorstore.similarity_search(query, k=top_k)\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        print(f\"Retrieved {len(contexts)} documents\")\n",
    "\n",
    "        # Step 3: Evaluate relevance of retrieved documents and filter out irrelevant chunks\n",
    "        print(\"Step 3: Evaluating relevance of retrieved documents...\")\n",
    "        relevant_contexts = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            if relevance == 'relevant':\n",
    "                relevant_contexts.append(context)\n",
    "\n",
    "        print(f\"Number of relevant contexts: {len(relevant_contexts)}\")\n",
    "\n",
    "        # If no relevant contexts found, generate without retrieval\n",
    "        if not relevant_contexts:\n",
    "            print(\"No relevant contexts found. Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No relevant context found.\"}\n",
    "            return generation_chain.invoke(input_data).response\n",
    "\n",
    "        # Step 4: Generate response using relevant contexts\n",
    "        print(\"Step 4: Generating responses using relevant contexts...\")\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            print(f\"Generating response for context {i+1}...\")\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            response = generation_chain.invoke(input_data).response\n",
    "\n",
    "            # Step 5: Evaluate how well the response is supported by its context\n",
    "            print(f\"Step 5: Assessing support for response {i+1}...\")\n",
    "            input_data = {\"response\": response, \"context\": context}\n",
    "            support = support_chain.invoke(input_data).response.strip().lower()\n",
    "            print(f\"Support assessment: {support}\")\n",
    "\n",
    "            # Step 6: Score the usefulness of the response\n",
    "            print(f\"Step 6: Evaluating utility for response {i+1}...\")\n",
    "            input_data = {\"query\": query, \"response\": response}\n",
    "            utility = int(utility_chain.invoke(input_data).response)\n",
    "            print(f\"Utility score: {utility}\")\n",
    "\n",
    "            # Collect response with its metadata\n",
    "            responses.append((response, support, utility))\n",
    "\n",
    "        # Choose the best response — prioritize strong support, then high utility\n",
    "        print(\"Selecting the best response...\")\n",
    "        best_response = max(responses, key=lambda x: (x[1] == 'fully supported', x[2]))\n",
    "        print(f\"Best response support: {best_response[1]}, utility: {best_response[2]}\")\n",
    "        return best_response[0]\n",
    "    else:\n",
    "        # If no retrieval is needed, generate directly without context\n",
    "        print(\"Generating without retrieval...\")\n",
    "        input_data = {\"query\": query, \"context\": \"No retrieval necessary.\"}\n",
    "        return generation_chain.invoke(input_data).response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXZxvdlNG0Ct"
   },
   "source": [
    "- The function begins by calling the retrieval decision chain to determine whether any background knowledge is needed at all. This avoids unnecessary vector searches and keeps the system efficient.\n",
    "- If retrieval is necessary, we query the vector store (FAISS) and grab the top-k similar chunks based on the query's embedding.\n",
    "- We then screen these chunks using the relevance chain, only keeping those deemed genuinely relevant to the query.\n",
    "- For each relevant context, the system generates a response — and critically, it doesn't stop there.\n",
    "- Every response is passed through two evaluations:\n",
    "  - Is it supported by the context it used?\n",
    "  - How useful is it to the query (1–5)?\n",
    "- Finally, the response with the strongest backing and highest utility is selected and returned.\n",
    "\n",
    "This is what makes self-RAG intelligent: it is not just pulling text — it is thinking through whether the context matters, evaluating its own answers, and justifying the final response it returns.\n",
    "\n",
    "\n",
    "### Running Self-RAG — High vs. low relevance queries\n",
    "Time to see it in action. We will test the self-RAG pipeline with two very different types of queries:\n",
    "- A relevant query that clearly relates to the loaded document (\"Understanding Climate Change\").\n",
    "- An off-topic query that has nothing to do with the content.\n",
    "\n",
    "This helps validate that the system can:\n",
    "- Retrieve and ground answers when useful context is available.\n",
    "- Avoid hallucination and gracefully fall back when no relevant information exists.\n",
    "\n",
    "#### Test case 1: On-topic query — High relevance\n",
    "This is a straightforward test where the question should align well with the content of the climate change PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NpJZmYofGzzI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is the impact of climate change on the environment?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: yes\n",
      "Step 2: Retrieving relevant documents...\n",
      "Retrieved 3 documents\n",
      "Step 3: Evaluating relevance of retrieved documents...\n",
      "Document 1 relevance: relevant\n",
      "Document 2 relevance: relevant\n",
      "Document 3 relevance: relevant\n",
      "Number of relevant contexts: 3\n",
      "Step 4: Generating responses using relevant contexts...\n",
      "Generating response for context 1...\n",
      "Step 5: Assessing support for response 1...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 1...\n",
      "Utility score: 5\n",
      "Generating response for context 2...\n",
      "Step 5: Assessing support for response 2...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 2...\n",
      "Utility score: 5\n",
      "Generating response for context 3...\n",
      "Step 5: Assessing support for response 3...\n",
      "Support assessment: fully supported\n",
      "Step 6: Evaluating utility for response 3...\n",
      "Utility score: 5\n",
      "Selecting the best response...\n",
      "Best response support: fully supported, utility: 5\n",
      "\n",
      "Final response:\n",
      "areas are particularly vulnerable, as infrastructure may not be equipped to handle the volume of water. Flooding can result in property damage, displacement of communities, and contamination of water supplies, posing health risks. \n",
      "\n",
      "Ecosystem Disruption \n",
      "Climate change also disrupts ecosystems, leading to shifts in species distribution and loss of biodiversity. As temperatures rise, many species struggle to adapt, resulting in altered habitats and increased extinction rates. Coral reefs, for example, are highly sensitive to temperature changes and are experiencing bleaching events, which threaten marine biodiversity.\n",
      "\n",
      "Managed Retreats \n",
      "In response to these challenges, some communities are considering managed retreats, which involve relocating people and infrastructure away from vulnerable areas. This strategy aims to reduce risk and enhance resilience to future climate impacts, but it also raises social, economic, and ethical questions about displacement and community identity.\n",
      "\n",
      "Overall, the impact of climate change on the environment is profound, affecting weather patterns, ecosystems, and human livelihoods. Addressing these challenges requires comprehensive strategies that include mitigation efforts to reduce greenhouse gas emissions and adaptation measures to enhance resilience.\n"
     ]
    }
   ],
   "source": [
    "# Query that matches the domain of the document\n",
    "query = \"What is the impact of climate change on the environment?\"\n",
    "# Run the self-RAG pipeline with this query\n",
    "response = self_rag(query, vectorstore)\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf19_xSiGzeS"
   },
   "source": [
    "This query should trigger retrieval and return a high-quality, grounded answer from the climate change PDF. This demonstrates how Self-RAG leans on the knowledge base when it makes sense to do so.\n",
    "\n",
    "\n",
    "### Test case 2: Off-topic query — Low/No Relevance\n",
    "Here we are asking a question about Harry Potter — something that’s obviously unrelated to climate science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t-xPPUlxG5XZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: how did harry beat quirrell?\n",
      "Step 1: Determining if retrieval is necessary...\n",
      "Retrieval decision: no\n",
      "Generating without retrieval...\n",
      "\n",
      "Final response:\n",
      "Harry Potter defeated Professor Quirrell in the first book, \"Harry Potter and the Sorcerer's Stone,\" through a combination of luck and the protective magic of his mother's sacrifice. Quirrell was trying to steal the Sorcerer's Stone for Voldemort, who was possessing him. When Quirrell attempted to touch Harry, he was unable to do so because of the love and protection that Harry's mother, Lily Potter, had bestowed upon him by sacrificing her life to save him. This protection caused Quirrell great pain and ultimately led to his defeat, allowing Harry to prevent Voldemort from obtaining the Stone.\n"
     ]
    }
   ],
   "source": [
    "# Query that is unrelated to the climate change domain\n",
    "query = \"how did harry beat quirrell?\"\n",
    "# Run the self-RAG pipeline\n",
    "response = self_rag(query, vectorstore)\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQqtpJ5NgUxG"
   },
   "source": [
    "This query is off-topic. Self-RAG should identify that no relevant documents exist and still attempt to answer appropriately (or at least transparently). This test shows that the model can admit when it does not know — and that is a feature, not a flaw."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
