{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr5uf9gUShqs"
   },
   "source": [
    "# Adaptive RAG system\n",
    "\n",
    "In traditional RAG systems, retrieval is often a generic step — every query is handled with the same strategy regardless of its nature. But not all queries are the same. Some require precision, others benefit from multiple perspectives or personalized context. This notebook presents an Adaptive RAG system that intelligently selects the most suitable retrieval strategy for a given query type, using LLMs both for classification and context enhancement.\n",
    "\n",
    "The goal is to build a pipeline that automatically classifies the type of query (e.g., factual, analytical, opinion-based, or contextual) and applies a retrieval strategy that best matches the needs of that query. The retrieved results are then used by a language model to generate accurate and nuanced answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RaFQnzGBSfwY"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import Dict, Any, List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdlJUYN5S5lV"
   },
   "source": [
    "### Query classification\n",
    "To choose the best retrieval strategy for a given user query, we first need to understand the intent behind that query. Is the user asking for a factual piece of information, trying to analyze a concept, exploring opinions, or expecting an answer tailored to their specific context?\n",
    "\n",
    "This step is critical. Instead of relying on hardcoded rules or heuristics, we leverage an LLM to classify the query based on its semantics. The classifier returns one of four high-level categories:\n",
    "- Factual – Looking for specific, verifiable information.\n",
    "- Analytical – Seeks an explanation or breakdown, often multi-faceted.\n",
    "- Opinion – Asks for subjective perspectives or diverse viewpoints.\n",
    "- Contextual – Relies on user context or environment to interpret the query.\n",
    "\n",
    "This enables the system to adaptively route the query to the most appropriate strategy in the next step.\n",
    "\n",
    "Let's define the `QueryClassifier` class, which uses a prompt to guide the LLM in making this decision. We will also define a simple Pydantic model to structure the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HqvQun0kS5ZP"
   },
   "outputs": [],
   "source": [
    "# Define the schema for the classifier's output using Pydantic.\n",
    "# This ensures the LLM returns a result in a structured format, specifically one of our four categories.\n",
    "class categories_options(BaseModel):\n",
    "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
    "\n",
    "\n",
    "# Class that wraps the classification chain\n",
    "class QueryClassifier:\n",
    "    def __init__(self):\n",
    "        # Initialize the LLM with zero temperature for deterministic output.\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "        # Define the prompt template used to classify a query.\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
    "        )\n",
    "        # Chain the prompt with the LLM and define the output schema. This ensures we always get a well-structured response that conforms to the expected format.\n",
    "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
    "\n",
    "    # Define the classify method to run the query through the chain and extract the predicted category.\n",
    "    def classify(self, query):\n",
    "        print(\"\\n\\nclasiffying query\")\n",
    "        return self.chain.invoke(query).category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYFerh8jS5Ni"
   },
   "source": [
    "This component uses LLM to determine which strategy to follow next. It returns one of the predefined categories. This `QueryClassifier` uses a few-shot LLM prompt and expects structured output. The result is a single word: the query’s category.\n",
    "- The `PromptTemplate` defines how we instruct the LLM to perform the classification. It keeps the task clear and narrow.\n",
    "- `ChatOpenAI` is initialized with zero temperature, which makes the model output consistent and non-random—crucial for classification.\n",
    "- The combination of `PromptTemplate` and `ChatOpenAI` is piped (`|`) together with a structured output wrapper (`with_structured_output`) that parses the LLM output into a well-defined Pydantic model (`categories_options`).\n",
    "- Finally, `self.chain.invoke(query)` runs the query through the prompt and LLM, and parses the output to get the predicted category.\n",
    "\n",
    "### Base retrieval engine\n",
    "Before we define the specialized retrieval strategies, we first create a shared foundation for them to build upon.\n",
    "\n",
    "This `BaseRetrievalStrategy` class encapsulates all the common logic for setting up retrieval: breaking down long documents into smaller chunks, embedding those chunks into a high-dimensional vector space, and storing them in a fast vector index (FAISS). It also instantiates an LLM, which child classes can reuse to enhance or rank the retrieved results. All strategies will inherit from a shared retrieval logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O-F0dYjKS5CU"
   },
   "outputs": [],
   "source": [
    "# Base retrieval engine from which all specialized strategies will inherit\n",
    "class BaseRetrievalStrategy:\n",
    "    def __init__(self, texts):\n",
    "        # Initialize an embedding model from OpenAI to vectorize text\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        # Split long documents into smaller chunks for better retrieval performance\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "        self.documents = text_splitter.create_documents(texts)\n",
    "\n",
    "        # Build the FAISS vector store from the document embeddings\n",
    "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        # Initialize the LLM once for all retrieval tasks\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "    # Basic retrieve method that searches for top-k most similar documents\n",
    "    def retrieve(self, query, k=4):\n",
    "        return self.db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwZS3K9XS43G"
   },
   "source": [
    "Here, we are setting up the core building blocks needed for all retrieval strategies.\n",
    "- **Embeddings**: We convert all the input text into numerical vector representations using `OpenAIEmbeddings`2. These vectors capture semantic meaning, so similar texts end up close to each other in the vector space.\n",
    "- **Chunking**: Since large documents can be too long for embedding models or miss fine-grained details, we split the text into manageable 800-character chunks with no overlap. This gives better resolution during retrieval.\n",
    "- **Vector store**: We store these vectors using FAISS, which is an efficient similarity search engine. It allows fast lookup of the most relevant chunks based on cosine similarity.\n",
    "- **LLM**: We initialize a GPT-4o model so that any child class can use it to enhance queries, rank results, or do more sophisticated logic.\n",
    "\n",
    "The `retrieve` method here is a simple similarity search. Child strategies (like Factual, Analytical, etc.) can override this method to plug in more advanced behavior like query expansion, document reranking, sub-query generation, etc.\n",
    "\n",
    "### Strategy 1: Factual retrieval\n",
    "This strategy is designed for queries that expect concrete, verifiable information—things like dates, definitions, or specific facts. To improve precision, it enhances the user query before performing the search, then uses the LLM again to rank results based on how relevant they are.\n",
    "\n",
    "Compared to a basic vector search, this method injects more semantic understanding into the process—both in how we frame the query and how we interpret the results. It’s slower than raw similarity search but produces much more accurate answers when factual precision matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SiAXZLnvS4rr"
   },
   "outputs": [],
   "source": [
    "# Schema used to capture the LLM-generated relevance score for a document\n",
    "class relevant_score(BaseModel):\n",
    "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
    "\n",
    "# Factual retrieval strategy that builds on the BaseRetrievalStrategy\n",
    "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving factual\")\n",
    "        # Use LLM to enhance the user's query for better matching\n",
    "        enhanced_query_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
    "        )\n",
    "        query_chain = enhanced_query_prompt | self.llm\n",
    "        enhanced_query = query_chain.invoke(query).content\n",
    "        print(f'enhande query: {enhanced_query}')\n",
    "\n",
    "        # Perform similarity search using the enhanced query and over-fetch results\n",
    "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
    "\n",
    "        # Use LLM to rank the relevance of each retrieved document (1–10)\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"doc\"],\n",
    "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "\n",
    "        ranked_docs = []\n",
    "        print(\"ranking docs\")\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
    "            # Ask the LLM to assign a relevance score for each document\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "        # Sort documents by score in descending order and return top-k\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9_0lho4S4gU"
   },
   "source": [
    "This strategy improves the factual retrieval by refining the question and only returning documents with the highest LLM-judged relevance.\n",
    "- It first enhances the query using the LLM. This helps bridge the gap between user phrasing and the language used in the documents. For example, “When was Tesla founded?” might become “Provide the founding year of the Tesla company.”\n",
    "- Then it retrieves twice as many documents as needed—this is just to give the ranking step more material to work with.\n",
    "- The next key step is ranking. The LLM is asked to evaluate how relevant each document is on a scale from 1 to 10. This gives us a more intelligent filter than relying solely on embedding similarity.\n",
    "- After ranking, it sorts and slices the top `k` documents to return only the most relevant ones.\n",
    "\n",
    "It’s especially useful in knowledge-intensive domains like legal, scientific, or technical documentation.\n",
    "\n",
    "### Strategy 2: Analytical retrieval\n",
    "Analytical questions usually don’t have a single, straightforward answer. Instead, they require combining insights from multiple angles. For instance, a question like “How successful is New York as a global city?” needs economic data, demographic context, infrastructure, maybe even historical growth patterns.\n",
    "\n",
    "This strategy addresses that by breaking the question into smaller sub-questions, fetching documents for each one, and finally curating a diverse set of sources using the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-yejcE_PS4VF"
   },
   "outputs": [],
   "source": [
    "# Schema for selecting document indices based on diversity and relevance\n",
    "class SelectedIndices(BaseModel):\n",
    "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
    "\n",
    "# Schema for the LLM to return multiple sub-queries\n",
    "class SubQueries(BaseModel):\n",
    "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
    "\n",
    "# Analytical retrieval strategy based on sub-query decomposition and diversity scoring\n",
    "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4):\n",
    "        print(\"retrieving analytical\")\n",
    "\n",
    "        # Step 1: Generate k sub-questions that explore the topic more fully\n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Generate {k} sub-questions for: {query}\"\n",
    "        )\n",
    "        llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
    "\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
    "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        # Step 2: Perform similarity search for each sub-query (over-fetching to widen scope)\n",
    "        all_docs = []\n",
    "        for sub_query in sub_queries:\n",
    "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
    "\n",
    "        # Step 3: Use LLM to select a final set of k diverse and relevant documents\n",
    "        diversity_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
    "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
    "        )\n",
    "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "\n",
    "        # Convert docs into a preview format so the LLM can choose among them\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "        # Format the documents into a brief numbered list (truncated for prompt length)\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "\n",
    "        # Step 4: Let the LLM choose the final k documents by index\n",
    "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "\n",
    "        # Step 5: Return only the selected documents, with index bounds safety\n",
    "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffVqLav2S4Kb"
   },
   "source": [
    "This strategy relies heavily on decomposition and diversification. Instead of trying to answer an open-ended or multifaceted question head-on, it breaks the problem into smaller, more concrete questions. These sub-questions are generated by the LLM and treated as independent search prompts.\n",
    "\n",
    "It then searches the vector DB multiple times—once for each sub-query—to build a pool of potentially useful documents. Of course, some of these documents will overlap or be redundant. That’s where the final LLM step comes in.\n",
    "\n",
    "The model is asked to pick a balanced and diverse subset from the entire result pool. Instead of filtering based only on similarity, this lets the system ensure that the final selection includes multiple perspectives or types of evidence.\n",
    "\n",
    "The result is a curated mix of documents that represent different dimensions of the original question—ideal for synthesis, comparison, or in-depth reasoning.\n",
    "\n",
    "### Strategy 3: Opinion retrieval\n",
    "Opinion-based queries don’t rely on a single source of truth. Instead, they benefit from diversity—different angles, personal stances, or even conflicting views. This strategy uses the LLM to first identify possible viewpoints, then curates a representative selection of documents aligned with those views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TD9gtT3YS3_f"
   },
   "outputs": [],
   "source": [
    "# Strategy for retrieving diverse opinions on subjective or open-ended topics\n",
    "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=3):\n",
    "        print(\"retrieving opinion\")\n",
    "        # Step 1: Use the LLM to extract k potential viewpoints on the given topic\n",
    "        viewpoints_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
    "        )\n",
    "        viewpoints_chain = viewpoints_prompt | self.llm\n",
    "        input_data = {\"query\": query, \"k\": k}\n",
    "        # The output is expected to be a string of viewpoints separated by newlines\n",
    "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
    "        print(f'viewpoints: {viewpoints}')\n",
    "\n",
    "        # Step 2: Use similarity search to retrieve documents for each viewpoint\n",
    "        all_docs = []\n",
    "        for viewpoint in viewpoints:\n",
    "            # Boost relevance by combining original query with viewpoint context\n",
    "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
    "\n",
    "        # Step 3: Use LLM to classify and select the most diverse and representative opinions\n",
    "        opinion_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"docs\", \"k\"],\n",
    "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
    "        )\n",
    "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "\n",
    "        # Format the docs into an indexed summary to help the LLM make selection decisions\n",
    "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
    "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "\n",
    "        # Step 4: Let the LLM choose the final k documents by index\n",
    "        selected_indices = opinion_chain.invoke(input_data).indices\n",
    "        print(f'selected diverse and relevant documents')\n",
    "\n",
    "        # Step 5: Return the selected subset of documents\n",
    "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUme4QBHS30k"
   },
   "source": [
    "This strategy is tuned for subjective, debatable, or personal-opinion questions like “Is remote work good for productivity?” or “Should AI be regulated more strictly?” These don’t have a correct answer—they invite a variety of stances.\n",
    "\n",
    "The first move is to ask the LLM for a few distinct viewpoints. Think of this like surfacing the axes of debate. These might include political, cultural, ethical, or practical perspectives depending on the query.\n",
    "\n",
    "Each viewpoint is then used to anchor a separate similarity search, widening the scope and ensuring that multiple angles are considered—not just the most dominant or popular one.\n",
    "\n",
    "After that, the LLM acts as a selector and classifier. It reviews a pool of candidate documents and identifies the most representative and diverse ones, filtering out redundancy or bias. This is especially useful in avoiding echo chambers or one-sided responses.\n",
    "\n",
    "Finally, we return the best-matching documents—each ideally aligned with a unique opinion.\n",
    "\n",
    "This retrieval path is especially valuable for generating balanced summaries, debate-style content, or constructing contrasting points of view for synthesis.\n",
    "\n",
    "### Strategy 4: Contextual retrieval\n",
    "When a user’s query depends on additional context—like their role, goals, preferences, or prior actions—retrieving relevant information requires more than just matching keywords. This strategy uses the LLM to first reinterpret the query through the lens of that context, and then performs a context-aware ranking of the documents it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qWSevb4JS3pX"
   },
   "outputs": [],
   "source": [
    "# Strategy for handling queries that depend on user context or prior information\n",
    "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def retrieve(self, query, k=4, user_context=None):\n",
    "        print(\"retrieving contextual\")\n",
    "        # Step 1: Use LLM to incorporate user context into the query\n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
    "        )\n",
    "        context_chain = context_prompt | self.llm\n",
    "        # If no context is provided, use a fallback message\n",
    "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
    "        # The LLM returns a rewritten query that better reflects the user's intent\n",
    "        contextualized_query = context_chain.invoke(input_data).content\n",
    "        print(f'contextualized query: {contextualized_query}')\n",
    "\n",
    "        # Step 2: Perform a similarity search using the rewritten contextualized query. We retrieve more documents initially to allow room for intelligent filtering\n",
    "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
    "\n",
    "        # Step 3: Use LLM to rank the relevance of retrieved documents considering the query and user context\n",
    "        ranking_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\", \"doc\"],\n",
    "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
    "        )\n",
    "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
    "        print(\"ranking docs\")\n",
    "\n",
    "        ranked_docs = []\n",
    "        for doc in docs:\n",
    "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
    "            # Parse the LLM’s relevance rating and pair it with the doc\n",
    "            score = float(ranking_chain.invoke(input_data).score)\n",
    "            ranked_docs.append((doc, score))\n",
    "\n",
    "\n",
    "        # Step 4: Sort documents by relevance and return the top results\n",
    "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in ranked_docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyLpG347TZwx"
   },
   "source": [
    "This strategy is all about personalization. It assumes that the same query—say, \"What should I read next?\"—could mean totally different things depending on who’s asking. Maybe the user is a software engineer looking for technical papers, or a student exploring philosophy.\n",
    "\n",
    "First, the LLM reformulates the query using the provided context. This step is crucial because embedding-based retrieval systems are very literal. If the query doesn't fully express the user's need, the retrieved results can miss the mark.\n",
    "\n",
    "After retrieving a wide set of candidate documents, we call on the LLM again—this time to rank them. But it doesn't just consider the textual overlap. Instead, it interprets how well each document fits the user’s context, giving us a much more tailored set of results.\n",
    "\n",
    "This approach is useful in scenarios like:\n",
    "- Personalized assistants that adjust to user goals or preferences\n",
    "- Multi-turn dialogue systems with conversational memory\n",
    "- Enterprise retrieval where user role and intent matter\n",
    "\n",
    "Contextual retrieval is essentially contextualization both ways—first in how the query is phrased, and second in how relevance is judged.\n",
    "\n",
    "### Putting it all together: Adaptive retriever\n",
    "This component is the brain of the retrieval system. It acts like an intelligent router: it first figures out what kind of question it's dealing with, then dynamically picks the most appropriate retrieval strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UoUtRF49S3TW"
   },
   "outputs": [],
   "source": [
    "# Central orchestrator that selects the best retrieval strategy based on query type\n",
    "class AdaptiveRetriever:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        # Load the query classifier that will label incoming queries\n",
    "        self.classifier = QueryClassifier()\n",
    "        # Preload all strategies with the same knowledge base (texts)\n",
    "        self.strategies = {\n",
    "            \"Factual\": FactualRetrievalStrategy(texts),\n",
    "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
    "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
    "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
    "        }\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Step 1: Determine query type using LLM-based classifier\n",
    "        category = self.classifier.classify(query)\n",
    "        # Step 2: Pick and run the matching retrieval strategy\n",
    "        strategy = self.strategies[category]\n",
    "        return strategy.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78sRH5I_S3Ht"
   },
   "source": [
    "This `AdaptiveRetriever` acts like a switchboard operator. When a user sends in a query, it doesn’t just throw it into a vector search blindly—it first asks the classifier to label the question as one of four types: Factual, Analytical, Opinion, or Contextual. That classification is based on the semantics of the query itself, using an LLM fine-tuned to recognize intent.\n",
    "\n",
    "Then, the system selects the appropriate retrieval strategy accordingly. Each strategy knows how to handle its specific case—whether it needs to reformulate the query, decompose it, or consider user context. The `AdaptiveRetriever` lets us combine all these approaches in a modular, extensible way.\n",
    "\n",
    "### Integrating with LangChain’s `BaseRetriever`\n",
    "\n",
    "To make our adaptive retriever compatible with LangChain’s tools and agents, we wrap it in a class that inherits from LangChain’s BaseRetriever interface. This gives us compatibility with chains, agents, and any LangChain component that expects a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N3VOzfHKS27N"
   },
   "outputs": [],
   "source": [
    "# LangChain-compatible wrapper around our custom adaptive retriever\n",
    "class PydanticAdaptiveRetriever(BaseRetriever):\n",
    "    # Embed the AdaptiveRetriever as an internal field\n",
    "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        # Allow non-pydantic objects like AdaptiveRetriever to be used inside this class\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Required sync interface for LangChain retrievers\n",
    "        return self.adaptive_retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Optional async version for compatibility with async chains/agents\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # For now, we reuse the sync method; we can parallelize internally if needed\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFPg2IMZTiz1"
   },
   "source": [
    "This class is a bridge between our custom logic and LangChain’s standardized retriever ecosystem. LangChain expects any retriever to implement the `get_relevant_documents` method—and optionally, the async version as well. So here, we wrap your `AdaptiveRetriever` in a LangChain-friendly shell by subclassing `BaseRetriever`.\n",
    "\n",
    "The `adaptive_retriever` field is excluded from Pydantic serialization (e.g., when storing metadata or configurations) since it's not a standard serializable field—it holds a complex object. The `Config` subclass tells Pydantic to allow such non-standard types, which avoids validation issues.\n",
    "\n",
    "Ultimately, this small wrapper unlocks full interoperability with LangChain pipelines, memory modules, and tools. We can now plug this into an agent, use it in a retriever chain, or swap it in wherever LangChain expects a retriever. It’s the final step that turns our custom logic into a fully reusable component.\n",
    "\n",
    "### End-to-end adaptive RAG pipeline\n",
    "This class orchestrates the entire retrieval-and-generation process. It decides how to search, what to retrieve, and how to turn that into a coherent answer. It’s the complete RAG system—from raw query to final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S_ty8ccxTigJ"
   },
   "outputs": [],
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        # Initialize the adaptive retriever pipeline with input documents\n",
    "        adaptive_retriever = AdaptiveRetriever(texts)\n",
    "\n",
    "        # Wrap it in a LangChain-compatible retriever\n",
    "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
    "\n",
    "        # Instantiate the LLM for final answer generation\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)\n",
    "\n",
    "        # Define the prompt template used to guide answer generation\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Wrap the prompt with LangChain's PromptTemplate\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        # Create a simple LLM chain: prompt -> LLM\n",
    "        self.llm_chain = prompt | self.llm\n",
    "\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        # Step 1: Retrieve the most relevant documents for the query\n",
    "        docs = self.retriever.invoke(query)\n",
    "\n",
    "        # Step 2: Prepare the context for the LLM by joining the retrieved texts\n",
    "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
    "\n",
    "        # Step 3: Invoke the LLM with the context-rich prompt to generate the answer\n",
    "        return self.llm_chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8ijRc0_TqTd"
   },
   "source": [
    "This class is the top-level orchestrator—the part that a user or agent interacts with directly. When a query comes in, it sends it to the adaptive retriever, which figures out what type of question it is (factual, analytical, opinion, or contextual), and applies the right strategy to gather relevant documents.\n",
    "\n",
    "Once the context is retrieved, it’s passed to the LLM using a clearly defined prompt that emphasizes factuality and avoids hallucination. The LLM then generates a final answer using this filtered, context-aware input.\n",
    "\n",
    "The result is a fully adaptive, query-aware RAG pipeline that can intelligently handle a wide variety of question types without any manual intervention.\n",
    "\n",
    "### Test run: Showcase the adaptive system with different types of queries\n",
    "This section demonstrates how the adaptive retrieval system dynamically selects the right strategy based on the nature of the input question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ntmZy3DXTqEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "clasiffying query\n",
      "retrieving factual\n",
      "enhande query: To improve the specificity and depth of your query, you might consider rephrasing it as follows:\n",
      "\n",
      "\"What is the average distance between the Earth and the Sun in kilometers and miles, and how does this distance vary throughout the year due to the Earth's elliptical orbit?\" \n",
      "\n",
      "This enhanced query not only asks for the average distance but also seeks additional context about the variation in distance, which can lead to more comprehensive information retrieval.\n",
      "ranking docs\n",
      "Answer: I don't know.\n",
      "\n",
      "\n",
      "clasiffying query\n",
      "retrieving analytical\n",
      "sub queries for comprehensive analysis: [\"What is the relationship between the Earth's distance from the Sun and the amount of solar radiation received?\", \"How do variations in the Earth's orbit influence seasonal climate patterns?\", \"What role does the Earth's distance from the Sun play in long-term climate changes, such as ice ages?\", \"How does the Earth's distance from the Sun compare to that of other planets in terms of climate effects?\"]\n",
      "selected diverse and relevant documents\n",
      "Answer: I don't know.\n",
      "\n",
      "\n",
      "clasiffying query\n",
      "retrieving analytical\n",
      "sub queries for comprehensive analysis: ['What are the leading scientific theories regarding the origin of life on Earth, and what controversies surround them?', 'How do different scientific disciplines (e.g., biology, chemistry, astronomy) contribute to the debate on the origins of life?', \"What role do religious and philosophical beliefs play in shaping scientists' views on the origin of life?\", 'How have recent discoveries in astrobiology influenced the discussion on the possibility of life originating from extraterrestrial sources?']\n",
      "selected diverse and relevant documents\n",
      "Answer: I don't know.\n",
      "\n",
      "\n",
      "clasiffying query\n",
      "retrieving contextual\n",
      "contextualized query: How can Earth's position in the Solar System inform the design of life-supporting environments?\n",
      "ranking docs\n",
      "Answer: Earth’s place in the Solar System teaches us that specific conditions, such as distance from the Sun, presence of water, and a suitable atmosphere, are crucial for supporting life. It highlights the importance of these factors in designing life-supporting environments, as they must replicate or mimic the conditions found on Earth to sustain living organisms.\n"
     ]
    }
   ],
   "source": [
    "# Sample document collection (you can expand this with more data)\n",
    "texts = [\n",
    "    \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\"\n",
    "    ]\n",
    "\n",
    "# Instantiate the Adaptive RAG system\n",
    "rag_system = AdaptiveRAG(texts)\n",
    "\n",
    "# Test 1: A factual question – should trigger the FactualRetrievalStrategy\n",
    "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
    "print(f\"Answer: {factual_result}\")\n",
    "\n",
    "# Test 2: An analytical question – should break the query into sub-questions\n",
    "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
    "print(f\"Answer: {analytical_result}\")\n",
    "\n",
    "# Test 3: An opinion-based question – should collect diverse perspectives\n",
    "opinion_result = rag_system.answer(\"What are some controversial beliefs scientists hold about how life began on Earth?\").content\n",
    "print(f\"Answer: {opinion_result}\")\n",
    "\n",
    "# Test 4: A context-dependent question – simulates a contextual reformulation\n",
    "contextual_result = rag_system.answer(\"In the context of designing life-supporting environments, what does Earth’s place in the Solar System teach us?\").content\n",
    "print(f\"Answer: {contextual_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmHo_rd4TiSK"
   },
   "source": [
    "This test block is where everything comes together. We are feeding in different types of questions—some factual, some analytical, others more subjective or dependent on context. The `AdaptiveRAG` system automatically runs each query through the classifier to determine its type and then uses the appropriate retrieval strategy underneath. It is a full end-to-end RAG engine that adapts to user intent, not just keywords."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
