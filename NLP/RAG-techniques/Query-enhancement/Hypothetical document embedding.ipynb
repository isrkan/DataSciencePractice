{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0859db1-5c09-4dda-891c-27b9d45b2122",
   "metadata": {},
   "source": [
    "# Hypothetical document embedding (HyDE) in document retrieval\n",
    "\n",
    "In this notebook, we implement a Hypothetical Document Embedding (HyDE) system for document retrieval. The purpose of HyDE is to transform a query question into a detailed hypothetical document, which can bridge the gap between short queries and long, complex documents in the vector space. This approach improves the retrieval of relevant documents, especially when the query is brief but the document is detailed.\n",
    "\n",
    "Traditional retrieval systems often face a problem known as the semantic gap. A short user query might not match the longer documents well in the vector space. HyDE solves this by expanding the query into a more comprehensive, hypothetical document that answers the question in detail. This helps the query's vector representation to better match the documents in the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c209d3b-d844-413c-89ab-2e90cb49626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee6eaba-f2e3-40e9-a4da-a507287c5e11",
   "metadata": {},
   "source": [
    "### Load PDF document and Split it into chunks\n",
    "We will use `PyPDFLoader` to extract text from the PDF. It reads the PDF page by page and stores the extracted text in a list of document objects, where each document contains the content of a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f364d71-c39b-4137-be1e-4dc808d68640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the PDF document\n",
    "path = \"Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# Load PDF documents\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c6702-c1ba-4bb4-a9e6-72d990125f8c",
   "metadata": {},
   "source": [
    "We now split the documents into smaller chunks using the `RecursiveCharacterTextSplitter`. This will help us avoid dealing with very large chunks of text, making document retrieval faster and more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13fb70b-de36-4b00-9930-a9d773d25562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks with overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Size of each chunk\n",
    "    chunk_overlap=100,  # Overlap between consecutive chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e96055-23d5-45d3-8a88-e28ac10bff32",
   "metadata": {},
   "source": [
    "we are splitting the document into chunks of size 500 characters with 100 characters of overlap between chunks.\n",
    "- `chunk_size` makes it more manageable for indexing and retrieval.\n",
    "- `chunk_overlap` ensures that the context is preserved when the text is split. This helps maintain the flow of information between chunks.\n",
    "- `length_function` tells the splitter to calculate the length of the chunks based on the number of characters, ensuring that the chunks are exactly the specified size.\n",
    "\n",
    "#### Replace tabs with spaces\n",
    "In many cases, PDFs may contain tab characters (`\\t`) that were used for indentation but aren't necessary for the final processed text. We will replace these with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7714ba3-2b13-4349-b8cb-d602ec403c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace tab characters with spaces in the text content\n",
    "for text in texts:\n",
    "    text.page_content = text.page_content.replace('\\t', ' ')  # Replace tabs with spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2f665-4a77-4fdc-bdf5-c2a00d8ece95",
   "metadata": {},
   "source": [
    "### Generate embeddings using OpenAI\n",
    "Once the text is cleaned and processed, we can create embeddings for each of the chunks using OpenAI API. These embeddings represent the meaning of the text in a high-dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3a83bd-851b-4730-b510-8b616f5969ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54653c8f-fc8a-4ff5-9d2e-9cd878b8d024",
   "metadata": {},
   "source": [
    "We will also use FAISS to efficiently store and index the embeddings, which allows us to perform similarity search and query efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb13df1-5648-43c2-9c60-fea611669e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store using FAISS\n",
    "vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5548eac-edc3-48da-9e47-44ea7c935acb",
   "metadata": {},
   "source": [
    "Here, we create a FAISS vector store by:\n",
    "- Generating embeddings for each chunk of text.\n",
    "- Storing the embeddings in FAISS, which allows fast similarity search later.\n",
    "\n",
    "This function automatically creates a flat (brute-force) index by default.\n",
    "\n",
    "#### Initialize the language model\n",
    "Next, we initialize the language model that will generate a hypothetical document from a query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71345cb-40c1-4b20-a7b5-519125ab49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM for hypothetical document generation\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76a27f-f351-40cb-a6d7-61fd845d86e3",
   "metadata": {},
   "source": [
    "- **`ChatOpenAI`**: This uses OpenAI's ChatGPT model (specifically the GPT-4 variant) for generating detailed responses.\n",
    "- **`temperature=0`**: This ensures that the model's output is deterministic, meaning it will give consistent responses for the same input.\n",
    "- **`max_tokens=4000`**: This sets the maximum number of tokens (words) that the model can generate in a single response.\n",
    "\n",
    "#### Create the prompt template for generating hypothetical documents\n",
    "To guide the language model in generating a relevant document, we create a prompt template that defines how the model should respond to a query. This template asks the model to generate a document that answers the query in a detailed and comprehensive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2896448-3e85-4478-9865-8d847cfc2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for generating the hypothetical document\n",
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"chunk_size\"],\n",
    "    template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. \n",
    "    The document should be detailed and in-depth. The document size should be exactly {chunk_size} characters.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ba4e7-8f32-4f8b-9ab9-e43e5b9822e0",
   "metadata": {},
   "source": [
    "- **`input_variables=[\"query\", \"chunk_size\"]`**: These are the variables the model will use. `query` is the user’s input question, and `chunk_size` determines the length of the document.\n",
    "- **`template`**: This defines the exact structure of the prompt sent to the model.\n",
    "\n",
    "#### Create the LLM chain for hypothetical document generation\n",
    "We create an LLM chain that connects the prompt template with the language model. This chain ensures that the query is processed through the prompt template first and then passed to the GPT-4 model for document generation. This enables a smooth flow from input to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4984eff-c905-4a28-808e-c89587d81338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLM chain for hypothetical document generation\n",
    "hyde_chain = hyde_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd31f2-5cc4-4475-9b27-ae4feed7b4f6",
   "metadata": {},
   "source": [
    "### Generate the hypothetical document\n",
    "Now, we use the prompt chain to generate the hypothetical document. We will take the query, pass it to the model, and receive a detailed hypothetical document in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f43d838-17fc-4819-8343-df7dab854349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input variables for the prompt\n",
    "input_variables = {\"query\": \"What is the main cause of climate change?\", \"chunk_size\": 500}\n",
    "\n",
    "# Generate the hypothetical document using the model\n",
    "hypothetical_doc = hyde_chain.invoke(input_variables).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788cb26-2d48-47ef-9e3f-ae6a910df343",
   "metadata": {},
   "source": [
    "- **`input_variables`**: The `query` is the user’s input question, and `chunk_size` is the length of the document we want to generate.\n",
    "- **`invoke(input_variables)`**: This invokes the language model with the provided input variables, generating the hypothetical document.\n",
    "\n",
    "### Perform similarity search in the vector store\n",
    "With the hypothetical document generated, we now use it to search for similar documents in our FAISS vector store. This step retrieves documents from the original PDF that are most similar to the hypothetical document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9354fe69-a580-4eb9-9b2e-b817bd4c2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform similarity search in the vector store using the hypothetical document\n",
    "similar_docs = vector_store.similarity_search(hypothetical_doc, k=3)\n",
    "\n",
    "# Extract the content of the retrieved documents\n",
    "docs_content = [doc.page_content for doc in similar_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdb8d3-644a-458c-82ff-3f4e4b3a951d",
   "metadata": {},
   "source": [
    "- **`similarity_search(hypothetical_doc, k=3)`**: This searches for the top 3 most similar documents to the hypothetical document.\n",
    "- **`docs_content`**: This stores the content of the top 3 similar documents retrieved.\n",
    "\n",
    "Finally, we print the hypothetical document and the retrieved documents to see how well the system performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca34430d-2306-4842-a8a8-051629f73134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Document:\n",
      "\n",
      "**The Main Cause of Climate Change**\n",
      "\n",
      "Climate change primarily results from human activities, particularly the burning of fossil fuels such as coal, oil, and natural gas. This process releases significant amounts of carbon dioxide (CO2) and other greenhouse gases into the atmosphere, enhancing the greenhouse effect. Deforestation further exacerbates the issue by reducing the number of trees that can absorb CO2. Additionally, industrial processes, agriculture, and waste management contribute to emissions. Collectively, these factors disrupt the Earth's climate systems, leading to global warming and associated environmental impacts.\n",
      "\n",
      "Retrieved Documents:\n",
      "\n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhouse gases. \n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential\n",
      "\n",
      "\n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked\n",
      "\n",
      "\n",
      "Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. \n",
      "Historical Context\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the hypothetical document and the retrieved documents\n",
    "print(\"Hypothetical Document:\\n\")\n",
    "print(hypothetical_doc + \"\\n\")\n",
    "\n",
    "print(\"Retrieved Documents:\\n\")\n",
    "for content in docs_content:\n",
    "    print(content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67432e-9d92-4b6c-9d46-9044f41aac3c",
   "metadata": {},
   "source": [
    "By expanding the query into a detailed hypothetical document, we improve the chances of retrieving more relevant and accurate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
