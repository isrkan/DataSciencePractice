{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759b6556-a9d5-40d7-b720-1cd3cc3363d5",
   "metadata": {},
   "source": [
    "# Query transformations for improved retrieval in RAG systems\n",
    "\n",
    "This notebook demonstrates the use of three query transformation techniques designed to improve the retrieval process in RAG systems. RAG systems often face challenges when handling complex or ambiguous queries. These techniques aim to enhance the relevance and comprehensiveness of information retrieved from a document store. The techniques here help to achieve that by modifying or expanding the user query before the retrieval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85aeafa1-701f-49a0-bb07-b7490b68a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad8321-a708-4f81-b262-c1fbed0212cf",
   "metadata": {},
   "source": [
    "## 1. Query rewriting\n",
    "\n",
    "The goal of query rewriting is to make queries more specific and detailed, increasing the chances of retrieving relevant information.\n",
    "We use the LLM to reformulate the user query. The model is given a prompt template that instructs it to take the original query and rewrite it in a more specific and detailed manner.\n",
    "\n",
    "#### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63d358d-b7f7-45a5-afec-30bd90f694cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM for query rewriting\n",
    "re_write_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f5f4f-afe6-4e97-9208-1a0c6e22d152",
   "metadata": {},
   "source": [
    "We use `ChatOpenAI`, an interface for interacting with OpenAI models.\n",
    "- `temperature=0`: This ensures that the model's responses are more deterministic (consistent).\n",
    "- `model_name=\"gpt-4o-mini-2024-07-18\"`: Specifies the version of GPT-4 we are using.\n",
    "- `max_tokens=4000`: Limits the maximum length of the response from the model.\n",
    "\n",
    "#### Create a prompt template for query rewriting\n",
    "Next, we create a prompt template that guides the model on how to rewrite the original query. The prompt instructs the model to take the user’s query and transform it into a more specific and detailed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94b3e6d-70ab-4bde-aa77-7d8b6ec5449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for query rewriting\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a13e3-4c90-4747-9124-78a5ff6f5b6d",
   "metadata": {},
   "source": [
    "`{original_query}` is a placeholder that will be replaced with the actual user query.\n",
    "\n",
    "#### Set up the prompt template\n",
    "Now, we bind the prompt template to a `PromptTemplate` object, which allows us to easily pass it to the model during the query rewriting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a313ce3-76b7-44c2-9a81-a7dccdeaf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt template\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f590e5-6196-47d0-bf55-400c4a7148b2",
   "metadata": {},
   "source": [
    "- `input_variables=[\"original_query\"]`: This defines that the input to the template will be the `original_query`.\n",
    "- `template=query_rewrite_template`: The template we created earlier is now passed to the `PromptTemplate`.\n",
    "\n",
    "#### Create an LLM chain for query rewriting\n",
    "Now, we create an LLM Chain that links the prompt template and the LLM. This chain allows us to process the original query and produce a rewritten version by passing the query through both the prompt and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4274f520-2182-4f1c-b71c-d527c5a3541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLM chain for query rewriting\n",
    "query_rewriter = query_rewrite_prompt | re_write_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83517c-9682-40ea-b62f-a889c56c8d24",
   "metadata": {},
   "source": [
    "The `|` operator links the prompt template (`query_rewrite_prompt`) with the LLM (`re_write_llm`). This creates a chain where the input query is first processed by the prompt, and then the output from the prompt is passed to the LLM for query rewriting.\n",
    "\n",
    "#### Pass the query to the model for rewriting\n",
    "We now pass the original query to the model by invoking it through the chain, and get the rewritten query in response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515e028c-f3d1-4e1d-98be-e68a92532ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the original query to the model for rewriting\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "rewritten_query = query_rewriter.invoke({\"original_query\": original_query}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6874125-ea26-420d-8519-eb6a0c3e0830",
   "metadata": {},
   "source": [
    "- `original_query` is the user's input query.\n",
    "- `invoke({\"original_query\": original_query})`: This sends the original query to the model, which processes it and returns a rewritten version.\n",
    "- `.content`: This extracts the rewritten query from the response returned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3cb735e-a20f-4ede-83a3-31770b6652df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Rewritten query: What specific effects does climate change have on various environmental factors such as biodiversity, ocean levels, weather patterns, and ecosystems?\n"
     ]
    }
   ],
   "source": [
    "# Print the original and rewritten queries\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nRewritten query:\", rewritten_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f70e8b-e369-4a13-8797-e4dc6513e0f6",
   "metadata": {},
   "source": [
    "The rewritten query should be more specific, detailed, and relevant for the RAG system to retrieve better results.\n",
    "\n",
    "## 2. Step-back prompting\n",
    "Step-back prompting generates a broader query that helps retrieve additional, relevant context. It allows the system to go beyond the user’s original query and fetch a more general set of documents that could provide background or supplementary information for a more specific query.\n",
    "\n",
    "#### Initialize the LLM\n",
    "We initialize the `ChatOpenAI` model that will be responsible for generating the broader \"step-back\" queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8ea953-2952-4973-a8be-59c76af2790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e58d86-49ee-4f14-b6a0-fe0885012d04",
   "metadata": {},
   "source": [
    "#### Create a prompt template for step-back query generation\n",
    "Next, we create a prompt template that guides the model to generate broader, more general queries. The prompt instructs the model to take the original query and generate a step-back query that will help retrieve useful background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0953a52-dc46-4b42-80b6-a79a0237e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for step-back prompting\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Step-back query:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206b38f-8ba1-42d5-bc0e-a021728847e6",
   "metadata": {},
   "source": [
    "`{original_query}` is a placeholder that will be replaced with the user’s input query.\n",
    "\n",
    "#### Set up the prompt template\n",
    "Now, we bind the step-back prompt template to a `PromptTemplate` object, which will be used to pass the original query to the model during the step-back query generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194b8014-db55-449e-9c1c-9bd2c3eda291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt template\n",
    "step_back_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=step_back_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bed8f6-e96e-47eb-8db3-eb09b7d37242",
   "metadata": {},
   "source": [
    "- `input_variables=[\"original_query\"]`: Defines that the input to the template will be the `original_query`.\n",
    "- `template=step_back_template`: Passes the step-back prompt template that we created earlier.\n",
    "\n",
    "#### Create an LLM chain for step-back prompting\n",
    "Now, we create an LLM chain that links the step-back prompt template and the LLM. This chain will process the original query and generate the step-back query by passing the query through both the prompt and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d00dca-52f0-4c53-ae57-b063ea0ed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for step-back prompting\n",
    "step_back_chain = step_back_prompt | step_back_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213d20a-1d41-43c8-9452-c6f0fe393cb8",
   "metadata": {},
   "source": [
    "This chain takes the original query, processes it through the prompt, and then passes the output to the model to generate the broader step-back query.\n",
    "\n",
    "#### Pass the original query to the model for step-back query generation\n",
    "We now pass the original query to the model using the LLM chain. The model processes the original query and returns the step-back query in response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223da5bd-429e-4f6b-9e6d-40648cbe019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the original query to the model for step-back query generation\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "step_back_query = step_back_chain.invoke({\"original_query\": original_query}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072f3eb-4168-49a0-a46c-09c43f60c31b",
   "metadata": {},
   "source": [
    "- `original_query` is the user's original query.\n",
    "- `invoke({\"original_query\": original_query})`: This sends the original query to the model for processing, which generates the step-back query.\n",
    "- `.content`: This extracts the step-back query from the response returned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe11d11f-5044-41e2-93d3-2c192f737e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Step-back query: What are the effects of environmental changes on ecosystems and biodiversity?\n"
     ]
    }
   ],
   "source": [
    "# Print the original and step-back queries\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nStep-back query:\", step_back_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1a782-4a6b-414e-ab0c-948af7e4306e",
   "metadata": {},
   "source": [
    "The step-back query is more general, focusing on a broader view.\n",
    "\n",
    "## 3. Sub-query decomposition\n",
    "Sub-query decomposition breaks down a complex query into smaller, simpler sub-queries. This allows the RAG system to retrieve relevant information on specific aspects of the original query, which can then be aggregated for a comprehensive answer.\n",
    "\n",
    "#### Initialize the LLM\n",
    "We initialize the `ChatOpenAI` model that will be responsible for generating the sub-queries. The model will take the original complex query and decompose it into 2-4 simpler sub-queries that are easier to handle by the retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ed68f9-d592-45d0-bef1-72f2ce2a17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini-2024-07-18\", max_tokens=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663af99b-2b7d-4443-96e4-6218f6d2989b",
   "metadata": {},
   "source": [
    "### Create a prompt template for sub-query decomposition\n",
    "Next, we create a prompt template that instructs the model on how to decompose the original complex query into simpler sub-queries. The prompt asks the model to break down the original query into a set of 2-4 sub-queries that address different aspects of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87d86e6-00ab-4d17-b921-45c1e2e14081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for sub-query decomposition\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1bd38-393b-4140-81cd-1088491cd104",
   "metadata": {},
   "source": [
    "`{original_query}` is a placeholder that will be replaced by the actual user's query. The example shows how a complex query can be decomposed into sub-queries, providing a clearer structure for the retrieval system.\n",
    "\n",
    "#### Set Up the prompt template\n",
    "Now, we bind the sub-query decomposition template to a `PromptTemplate` object, which will be used during the decomposition process to pass the original query to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c215ac-74d1-418f-b735-f33581824c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt template\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=subquery_decomposition_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968f32c-d77a-4562-96bb-511cfd358368",
   "metadata": {},
   "source": [
    "- `input_variables=[\"original_query\"]`: Defines that the input to the template will be the `original_query`.\n",
    "- `template=subquery_decomposition_template`: Passes the sub-query decomposition template that we created earlier.\n",
    "\n",
    "#### Create an LLM chain for sub-query decomposition\n",
    "We create an LLM chain that links the sub-query decomposition prompt template with the LLM. This chain will process the original query and generate the simpler sub-queries by passing the query through both the prompt and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5eca85-134e-4c58-af0e-5e0bb1914534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for sub-query decomposition\n",
    "subquery_decomposer_chain = subquery_decomposition_prompt | sub_query_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648e6d0-5204-489a-942e-7009ac5e8c36",
   "metadata": {},
   "source": [
    "This chain takes the original query, processes it through the prompt, and then passes the output to the model, generating the decomposed sub-queries.\n",
    "\n",
    "#### Pass the original query to the model for sub-query decomposition\n",
    "We now pass the original query to the model using the LLM chain. The model processes the original query and returns the decomposed sub-queries in response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf805658-5cdf-4821-9461-f4398b0f768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the original query to the model for sub-query decomposition\n",
    "original_query = \"What are the long-term effects of technological advancements on the society?\"\n",
    "\n",
    "sub_queries = subquery_decomposer_chain.invoke({\"original_query\": original_query}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d9651-660b-44ad-b19c-0901d43e50f0",
   "metadata": {},
   "source": [
    "- `original_query` is the user's original query.\n",
    "- `invoke({\"original_query\": original_query})`: This sends the original query to the model, which processes it and generates the simpler sub-queries.\n",
    "- `.content`: This extracts the sub-queries from the response returned by the model.\n",
    "\n",
    "#### Process and extract sub-queries\n",
    "Once we have the response from the model, we need to process the returned content. We clean the response and extract the sub-queries by splitting them from the output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76b486fe-5e57-49e9-9ee2-6b46d1ec8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and extract sub-queries from the model's response\n",
    "sub_queries = [q.strip() for q in sub_queries.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d25d36-3170-42a9-97ad-744d01e9b461",
   "metadata": {},
   "source": [
    "This step processes the model's output, which contains the sub-queries along with some additional text. We strip the extra whitespace, and filter out anything that doesn't look like a valid sub-query (e.g., introductory text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddfb3a1b-2d6e-43a2-8400-abb4a12d0d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-queries:\n",
      "1. Sub-queries for the original query \"What are the long-term effects of technological advancements on society?\":\n",
      "2. 1. How do technological advancements influence employment and job markets over time?\n",
      "3. 2. What are the effects of technological advancements on education and learning methods in society?\n",
      "4. 3. How do technological advancements impact social interactions and relationships among individuals?\n",
      "5. 4. What are the implications of technological advancements for privacy and security in society?\n"
     ]
    }
   ],
   "source": [
    "# Print the original and decomposed sub-queries\n",
    "print(\"\\nSub-queries:\")\n",
    "for i, sub_query in enumerate(sub_queries, 1):\n",
    "    print(f\"{i}. {sub_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e0572-242f-4674-8764-0e6de02e369f",
   "metadata": {},
   "source": [
    "The complex query has been broken down into smaller, more manageable pieces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
