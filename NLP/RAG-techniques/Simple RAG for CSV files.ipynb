{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647f8543-2ab8-4e42-995e-a7a11c950ca4",
   "metadata": {},
   "source": [
    "# Simple RAG system for CSV files\n",
    "\n",
    "In this notebook, we will build a RAG system that processes and queries CSV documents. We will use OpenAI embeddings and FAISS for efficient similarity search. The system will encode the CSV document content into a vector store, split the content into manageable chunks, and retrieve relevant information based on user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ae764d-8eaa-4274-883d-ad3342a326c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import faiss\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea161b95-eddc-4f97-acc0-aac647f212bd",
   "metadata": {},
   "source": [
    "### Load the CSV file\n",
    "We will load the CSV file into a pandas DataFrame. This dataset serves as the source of information that the RAG system will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82550dce-a766-4315-ae5a-b9d096b5e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Customer Id</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Company</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Phone 1</th>\n",
       "      <th>Phone 2</th>\n",
       "      <th>Email</th>\n",
       "      <th>Subscription Date</th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DD37Cf93aecA6Dc</td>\n",
       "      <td>Sheryl</td>\n",
       "      <td>Baxter</td>\n",
       "      <td>Rasmussen Group</td>\n",
       "      <td>East Leonard</td>\n",
       "      <td>Chile</td>\n",
       "      <td>229.077.5154</td>\n",
       "      <td>397.884.0519x718</td>\n",
       "      <td>zunigavanessa@smith.info</td>\n",
       "      <td>2020-08-24</td>\n",
       "      <td>http://www.stephenson.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1Ef7b82A4CAAD10</td>\n",
       "      <td>Preston</td>\n",
       "      <td>Lozano</td>\n",
       "      <td>Vega-Gentry</td>\n",
       "      <td>East Jimmychester</td>\n",
       "      <td>Djibouti</td>\n",
       "      <td>5153435776</td>\n",
       "      <td>686-620-1820x944</td>\n",
       "      <td>vmata@colon.com</td>\n",
       "      <td>2021-04-23</td>\n",
       "      <td>http://www.hobbs.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6F94879bDAfE5a6</td>\n",
       "      <td>Roy</td>\n",
       "      <td>Berry</td>\n",
       "      <td>Murillo-Perry</td>\n",
       "      <td>Isabelborough</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>+1-539-402-0259</td>\n",
       "      <td>(496)978-3969x58947</td>\n",
       "      <td>beckycarr@hogan.com</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>http://www.lawrence.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5Cef8BFA16c5e3c</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Olsen</td>\n",
       "      <td>Dominguez, Mcmillan and Donovan</td>\n",
       "      <td>Bensonview</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>001-808-617-6467x12895</td>\n",
       "      <td>+1-813-324-8756</td>\n",
       "      <td>stanleyblackwell@benson.org</td>\n",
       "      <td>2020-06-02</td>\n",
       "      <td>http://www.good-lyons.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>053d585Ab6b3159</td>\n",
       "      <td>Joanna</td>\n",
       "      <td>Bender</td>\n",
       "      <td>Martin, Lang and Andrade</td>\n",
       "      <td>West Priscilla</td>\n",
       "      <td>Slovakia (Slovak Republic)</td>\n",
       "      <td>001-234-203-0635x76146</td>\n",
       "      <td>001-199-446-3860x3486</td>\n",
       "      <td>colinalvarado@miles.net</td>\n",
       "      <td>2021-04-17</td>\n",
       "      <td>https://goodwin-ingram.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index      Customer Id First Name Last Name  \\\n",
       "0      1  DD37Cf93aecA6Dc     Sheryl    Baxter   \n",
       "1      2  1Ef7b82A4CAAD10    Preston    Lozano   \n",
       "2      3  6F94879bDAfE5a6        Roy     Berry   \n",
       "3      4  5Cef8BFA16c5e3c      Linda     Olsen   \n",
       "4      5  053d585Ab6b3159     Joanna    Bender   \n",
       "\n",
       "                           Company               City  \\\n",
       "0                  Rasmussen Group       East Leonard   \n",
       "1                      Vega-Gentry  East Jimmychester   \n",
       "2                    Murillo-Perry      Isabelborough   \n",
       "3  Dominguez, Mcmillan and Donovan         Bensonview   \n",
       "4         Martin, Lang and Andrade     West Priscilla   \n",
       "\n",
       "                      Country                 Phone 1                Phone 2  \\\n",
       "0                       Chile            229.077.5154       397.884.0519x718   \n",
       "1                    Djibouti              5153435776       686-620-1820x944   \n",
       "2         Antigua and Barbuda         +1-539-402-0259    (496)978-3969x58947   \n",
       "3          Dominican Republic  001-808-617-6467x12895        +1-813-324-8756   \n",
       "4  Slovakia (Slovak Republic)  001-234-203-0635x76146  001-199-446-3860x3486   \n",
       "\n",
       "                         Email Subscription Date                      Website  \n",
       "0     zunigavanessa@smith.info        2020-08-24   http://www.stephenson.com/  \n",
       "1              vmata@colon.com        2021-04-23        http://www.hobbs.com/  \n",
       "2          beckycarr@hogan.com        2020-03-25     http://www.lawrence.com/  \n",
       "3  stanleyblackwell@benson.org        2020-06-02   http://www.good-lyons.com/  \n",
       "4      colinalvarado@miles.net        2021-04-17  https://goodwin-ingram.com/  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'customers-100.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bbbbc-c02d-413d-8e3d-124d7be3fb00",
   "metadata": {},
   "source": [
    "The CSV file contains customer data with attributes such as first name, last name, company, etc. The goal is to use this data in a Q&A system that can provide information about customers based on specific queries.\n",
    "\n",
    "### Load and split the CSV data into chunks\n",
    "Now, we will load the CSV file into the `CSVLoader` from LangChain. This loader provides a specialized way to split the content into chunks suitable for embedding and retrieval tasks.\n",
    "\n",
    "- Each row from the CSV is treated as a separate \"document\" containing the data in that row. This means each row becomes an individual chunk. For example, if the CSV has multiple columns (e.g., `first_name`, `last_name`, `company`), each row is stored as a separate document containing the information from those columns, ensuring that our RAG system can work with the data as if it were a natural text document.\n",
    "- If the document is too large (for example, if there are large text columns in the CSV), it may be split into smaller chunks. This chunking ensures that when we generate embeddings, the text is of a manageable size, improving the performance of the embedding model and the search system. In our case, no explicit chunking based on text size happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53247653-2769-4bb4-bd95-43d38795806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file_path)\n",
    "\n",
    "# Load and split the CSV file into chunks\n",
    "docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c875b7-249e-4a1e-b20d-aa19fa9d4760",
   "metadata": {},
   "source": [
    "### Create embeddings for the chunks\n",
    "Next, we create embeddings for the chunks using OpenAI API. We use OpenAI's embeddings to convert the text into vectors. This allows us to capture semantic meaning.\n",
    "\n",
    "##### Initialize embedding model\n",
    "First, we will initialize `OpenAIEmbeddings` which will be used to generate vector embeddings for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca19cccf-f3b6-47b7-aa57-b2f62e780d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac98af-4360-419d-aadb-07c5abb9e9b2",
   "metadata": {},
   "source": [
    "##### Create a FAISS index\n",
    "Now, we use FAISS to create an index that will allow for fast similarity search. It creates a flat (non-compressed) index using L2 distance (Euclidean distance) as the metric for comparing vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8327e639-a5b9-4a6d-b41d-391989b93ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index for storing and querying the embeddings\n",
    "index = faiss.IndexFlatL2(len(OpenAIEmbeddings().embed_query(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707ca34-afd4-4a50-a77f-d1b3258f9634",
   "metadata": {},
   "source": [
    "It gets as input the size of the embedding vector. This is done by calling `embed_query(\" \")` on the `OpenAIEmbeddings()` object, which generates an embedding for a dummy query (a space `\" \"`). The length of the resulting vector is the size that the FAISS index will expect for each of the embeddings. It ensures that the index is properly set up to handle embeddings of the correct size.\n",
    "\n",
    "#####  Create the FAISS vector store\n",
    "This creates a FAISS vector store, which will manage both the document embeddings and the FAISS index. It provides a way to efficiently store, index, and search the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4d8b92-6798-459a-8e1d-2c2eb912a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS vector store\n",
    "vector_store = FAISS(\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7408b63-e876-4b53-8e16-1c5ba22c6ab9",
   "metadata": {},
   "source": [
    "- **`embedding_function=OpenAIEmbeddings()`**: This tells the FAISS vector store how to embed new documents. In our case, it will use the `OpenAIEmbeddings` object, which means it will generate OpenAI embeddings when required.\n",
    "- **`index=index`**: The previously created FAISS index is passed here. This index will store the embeddings, and during a search, it will find the closest (most similar) embeddings to the query.\n",
    "- **`docstore=InMemoryDocstore()`**: The `InMemoryDocstore()` is a simple in-memory document store. It will hold the documents that correspond to the embeddings in the vector store. This allows the system to retrieve the actual document content when we perform a search. In this case, it’s an in-memory store, which means it will not persist data after the script ends. We can use other types of document stores (like a database) if persistence is needed.\n",
    "- **`index_to_docstore_id={}`**: This is an empty dictionary, which is used to map each FAISS index entry to a specific document. It helps link a particular embedding (vector) back to the actual document in the document store (`docstore`). Wi will need to populate this dictionary with mappings between the index and documents, but in the current code, it is empty for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd0b70-e40c-4697-a248-2bcfb0e1ce1f",
   "metadata": {},
   "source": [
    "##### Add documents (embeddings) to the vector store\n",
    "Now, we will add the documents (in the form of their embeddings) to the FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b19884-fbba-4ce9-b6ca-9b4d9c7abbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fe11d5a3-5030-45b5-a600-f95ef2047f78',\n",
       " '949db4de-26df-4a47-b21d-5dc19f756d36',\n",
       " '12890ec2-0a5a-4637-9a28-f650fc3a7296',\n",
       " 'ae0e854b-efae-4bc4-a298-58821c21843e',\n",
       " 'c142918b-f13c-47e0-82bd-39e90150c1f1',\n",
       " '6206d751-5675-485a-ab49-0e3ea3545ff7',\n",
       " 'e0bbb04d-b4de-4709-af71-a91223a1b748',\n",
       " '36209884-1ab0-4b32-bae4-3c30652dd869',\n",
       " '71c44ea6-0397-4fc8-8b55-bb646be66c4e',\n",
       " 'b6963c1c-f1fb-4d7b-910c-4499d99415c8',\n",
       " '754e58fb-d0c0-4364-bea3-bd727980739c',\n",
       " 'cbcd12d4-c2ad-41af-9a64-4f0c0da01b9a',\n",
       " '10d35cc5-fc3a-4559-9a2a-c64d17801ab4',\n",
       " 'd8cc634e-97d2-42e5-a795-48403fee7761',\n",
       " 'e9f553c2-3da4-4535-8882-7019a69be99f',\n",
       " 'b5ad1bd8-5d12-49de-bf4e-a55eb7cb2c0a',\n",
       " '07e175a8-9ba7-4273-867a-d9cc5bd2e7b0',\n",
       " '4e3507f2-f807-41c6-ad74-9a366c84ea0e',\n",
       " '9a215089-197e-424d-8880-43b831f6101f',\n",
       " '1464164d-9e09-4ddd-a531-68ee27340d7f',\n",
       " '92209a57-6342-4de4-91b9-ff5c6c5b6eea',\n",
       " '45f80d87-5768-4f4a-a16d-00a1fd2b4d5d',\n",
       " 'e4d027f2-8d5d-4420-a9ef-000e9b1524a9',\n",
       " '652341b7-74d0-4700-832f-bf2f0998c885',\n",
       " '51cd0a44-86c4-4c81-9de3-f37bdf23a20c',\n",
       " '040c9930-c3b6-46d1-abbb-f14455744431',\n",
       " 'ab83e031-aa0e-4c09-9ea4-e2a7f6d8195b',\n",
       " 'b3d35178-c617-4e23-a736-c7ea5e213067',\n",
       " '7f15d611-ab72-49c3-8064-20188dce704e',\n",
       " '58719304-a50d-4275-a3a5-0b9d57b4bd0c',\n",
       " 'd45de75a-dcc9-4949-b8d3-1b5a9f9e5b83',\n",
       " '6fc44b02-eb3b-4f72-8f0b-95243be17312',\n",
       " '3e39752a-a7b0-4014-9899-7d56a00897a4',\n",
       " '0d2bd688-73ef-4153-9461-0fe5ccc833f9',\n",
       " 'af8c2691-426d-4434-956f-81cca56967f7',\n",
       " '6b1fe7bb-6351-4ffc-8527-57ef951f549e',\n",
       " '4bc34e08-2154-49f7-941b-6ac7dde75838',\n",
       " '4049a3e5-85f3-45be-aa25-ed46b57ca069',\n",
       " 'cba0eb88-34f3-4c39-ad87-27ebbd216768',\n",
       " 'fc373f5b-dde7-4216-b37c-0b0219c4575d',\n",
       " 'f669c4f9-d318-40d5-a364-9e9ecc91f8bf',\n",
       " 'fdec6399-2977-4ec8-9225-4a3d1fc4a5cc',\n",
       " 'a68e1d45-d629-4399-85fe-fe676c8b17d2',\n",
       " 'c33c2ba4-7912-44d1-95ca-b67e8b9175e4',\n",
       " 'b727d3e3-58ea-4970-96e8-cc1d71a2d925',\n",
       " 'e844d2bd-6d6d-439c-9532-ace3a4aa2038',\n",
       " '3e1eb7e6-4d92-42c3-9762-33c4c37b91c4',\n",
       " 'f9977633-4f5f-4ff7-9b4b-dbe58c5aff1a',\n",
       " '6278d86b-baf0-4538-b0aa-897c46ade55e',\n",
       " 'f5611376-abdb-4017-a406-7395dfc865d5',\n",
       " 'b0560221-3b43-4a40-aba5-d7a32f961255',\n",
       " 'dc95d942-9da5-4c6d-8ec9-1bc64c601fd8',\n",
       " 'b51263e8-d288-47c3-b908-d35f489d5e46',\n",
       " 'd75bf820-9981-492f-8a87-8eef60325af3',\n",
       " 'c5e80f45-6631-4fa0-9a2c-44c9c022b92b',\n",
       " '1e9214e6-9a58-42d3-ad9f-00c229c82a7c',\n",
       " '896007d0-f03b-4f8d-bc7c-e60d3e95a293',\n",
       " '47ef874a-498c-4f8e-ad23-6cb4f75011fd',\n",
       " '4a926566-1f31-4753-bb48-ac4d2ceb24dc',\n",
       " '3310ceb7-0a8e-44c0-b706-056168eeae2f',\n",
       " 'a09a6b3d-908b-4cef-9b9d-81e55d940b7f',\n",
       " 'f73a2a13-ca31-4e83-a63c-8aae6e7194ff',\n",
       " '8bd4934b-5c54-4812-aa84-8fd447f3f79c',\n",
       " '63f3e0b6-11e6-42ed-9e91-ae2b80e13014',\n",
       " '868124bf-c7b6-4c42-88c0-d41a4287f224',\n",
       " 'f1e334ac-ed03-4425-bd38-11e7eef01167',\n",
       " '4a93d46d-af4d-4cce-a0cb-436733bfd8ca',\n",
       " '88abffd4-4b61-4f15-a94b-4d28262f44d1',\n",
       " '23bc0ed1-5ffe-4f8c-8e01-293ef1135685',\n",
       " '0be7cb73-1219-46cd-8af0-ca757e152696',\n",
       " '25e300d4-fc15-4a8a-aa98-d916d9806850',\n",
       " '80485d7e-773e-494b-86b1-4821f05ae4d2',\n",
       " 'bb450f35-76b9-45ef-a240-60616ae5ab3e',\n",
       " '20cd64b9-0a4e-4699-9d08-541eea8ac9eb',\n",
       " 'ac38a621-c91d-4728-97a5-d1ca540c9b58',\n",
       " 'fe53c889-253d-49d6-b5e3-d138eeedda16',\n",
       " '1e6a235a-853b-41a6-96eb-09c8bc9939f5',\n",
       " '470261a7-bfb0-4df8-a9ce-16fb7c974569',\n",
       " '331b24cf-8f74-48b0-bb06-746ccb3fc7c9',\n",
       " 'edc1127d-3ff9-4fd3-a96e-4786a6f322a7',\n",
       " '5f19ad46-70ee-4dd4-9aca-14e9822e80b2',\n",
       " '3c1c3a10-7c9f-4d77-ab13-23ba143f0cf3',\n",
       " '10b5be7d-0638-4a7c-9a7c-ab8a8df49de6',\n",
       " 'ae47a699-dd2a-4aa5-9e6b-24f2c7ec9021',\n",
       " '93a9ee13-295b-48b8-9cb3-d6a145ac403d',\n",
       " 'ac78ae6f-c363-4677-b3a5-6f1ec4abe3ac',\n",
       " '406cfb56-823e-4041-a70f-6ec67daab29b',\n",
       " '41a2269d-9f8b-4f75-b554-00a435db94bd',\n",
       " 'dfd07c75-846d-4a6e-b3a0-605c3991036d',\n",
       " 'b81e8f57-8420-472f-8db1-5d9ed2aa42cb',\n",
       " '42bd39c7-1f91-43b3-a1f8-f0ec101ebc09',\n",
       " '7c0ec982-9666-48da-9aa4-620e5858e947',\n",
       " '502a6943-5c2a-49da-843e-70518111d924',\n",
       " '7b92efbd-9f40-4d88-85f6-5e95623a9183',\n",
       " 'e66d737c-baff-4306-9f13-a1f7bc155ba0',\n",
       " 'd3222b99-87ef-4af1-ac23-f234df662416',\n",
       " 'c5628f76-9109-43a8-839a-dc4d9e3cc689',\n",
       " 'b7d55c50-3fc5-498a-ad2c-c23638ced201',\n",
       " 'edebdcab-45ec-47a4-ada8-df24c00f25c5',\n",
       " '797c3e77-b370-4fa8-bb85-2eb6c3238126']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the documents to the vector store\n",
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c995a-de04-442b-9667-f95e3c29b1aa",
   "metadata": {},
   "source": [
    "What does this line do?\n",
    "1. **Embedding generation**: The `add_documents` method takes the list of documents (`docs`) and processes each document's content. For each document, the content is passed through the embedding function (`OpenAIEmbeddings()`) and converted into a high-dimensional numerical vector.\n",
    "2. **Storing embeddings in FAISS**: Once the embeddings are generated for each document, they are stored in the FAISS vector store. The vector store acts as a database where embeddings are indexed and can later be searched for similarity.\n",
    "3. **Vector store structure**: The vector store consists of two main components:\n",
    "     - The embeddings themselves, which are the high-dimensional vectors generated from the document content.\n",
    "     - The index, which helps FAISS to quickly perform similarity searches and retrieve the most relevant documents based on a query.\n",
    "   - When the documents are added to the vector store, they are linked to their corresponding embeddings in the FAISS index, allowing for fast and efficient retrieval later on.\n",
    "\n",
    "### Set up the retriever\n",
    "The retriever is responsible for fetching the most relevant documents when a query is issued. We configure the retriever to use the FAISS vector store we just created. This means that the vector store, which holds the embeddings and the documents, will now have the capability to search for documents that are relevant to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b51d227-d8b2-4fa5-9b28-59f99cc3f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6f8d0-cb06-4e5b-ab47-d49efcdf9fb2",
   "metadata": {},
   "source": [
    "### Set up the system prompt\n",
    "The system prompt defines how the LLM should respond to user queries. In this case, it instructs the LLM to use the context retrieved from the vector store to answer questions concisely. Then, we will use `ChatPromptTemplate` to create a structured chat prompt template. It allows us to specify how the conversation will flow and how the LLM should respond to both system and human (user) messages.\n",
    "\n",
    "The `system` and `human` keywords play important roles in controlling the flow and context of the interaction.\n",
    "  - **System message**: The first part of the tuple `(\"system\", system_prompt)` indicates that the `system_prompt` will be used to define the behavior and instructions for the LLM. It is not directly user input but an internal instruction for the LLM's performance. For example, it guides the model's behavior for the entire conversation or it defines the role of the assistant (e.g., \"You are a helpful assistant for answering questions based on retrieved context\").\n",
    "  - **Human message**: The second part `(\"human\", \"{input}\")` defines the input that will be provided by the user. This could be any question or statement the user is asking the assistant. The `{input}` is a placeholder for the user’s query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca680803-e53b-4e5b-8e2d-c5c7b63d232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c780254-1b0d-45b8-8c15-26060dddb99f",
   "metadata": {},
   "source": [
    "The `{context}` placeholder in the prompt is where the retrieved document content (the relevant chunks of text) will be inserted. The LLM will use this content to form its answers to the user’s query.\n",
    "\n",
    "### Create the question-answering chain and the RAG chain\n",
    "We now create two chains using Langchain, that handle the question-answering process: the question-answer chain and the RAG chain. The purpose of these chains is to tie together the LLM, the retrieved context, and the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52cc5dbf-371b-46f4-aab4-3a70daf08650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI Chat API\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Create the question-answer chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Set up the RAG chain with the retriever and question-answer chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09772aa1-0c73-4877-867e-00a877bfa6e6",
   "metadata": {},
   "source": [
    "The `create_stuff_documents_chain` creates a question-answering chain that combines the LLM with a prompt template.\n",
    "- **LLM**: This is the language model that will generate the answers.\n",
    "- **Prompt template**: This is the structured template (defined earlier) that instructs the model on how to answer and holds the actual query that the user asks.\n",
    "\n",
    "The `create_retrieval_chain` creates a RAG chain that connects the retriever (which fetches relevant context from the vector store) with the question-answer chain (which generates the final answer). By combining the retriever and question-answer chain, we create a RAG pipeline.\n",
    "- **Retriever**: This part searches the vector store (which contains embeddings of the documents) and retrieves the most relevant documents based on the query.\n",
    "- **Question-answer chain**: Once the context is retrieved, this chain processes both the retrieved context and the query to generate a response.\n",
    "\n",
    "### Test the system\n",
    "We will now query the RAG system by providing a user query and obtaining an answer based on the documents stored in the vector store. The process involves invoking the RAG chain to fetch relevant context and generate a response.\n",
    "\n",
    "This input is populated into the human part of the prompt template defined earlier. Processing the query:\n",
    "  - The retriever in the RAG chain will search the vector store for relevant documents related to the query.\n",
    "  - These documents, or chunks of context, are then passed to the question-answer chain, where the LLM uses the context and the query to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ba8b0f-3233-4ede-a1b3-cd0218d672e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheryl Baxter works for Rasmussen Group.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query to ask the assistant\n",
    "answer= rag_chain.invoke({\"input\": \"which company does sheryl Baxter work for?\"})\n",
    "\n",
    "# Display the answer\n",
    "answer['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
