{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2b45d5-b9f8-4b6a-b50b-72972585175e",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is a machine learning library. It is widely used for building deep learning models, particularly in research and academic settings. PyTorch provides a flexible and intuitive interface, making it easy to build and train neural networks.\n",
    "\n",
    "### Why use PyTorch?\n",
    "\n",
    "- Dynamic computation graph: PyTorch uses dynamic computation graphs (also known as define-by-run), which allow us to change the network's behavior on the fly. This is particularly useful for research and when working with dynamic input sizes or data.\n",
    "- Pythonic: PyTorch is designed to integrate seamlessly with the Python ecosystem. Its syntax and usage are intuitive and align closely with native Python operations, making it easy to learn and use.\n",
    "- GPU acceleration: PyTorch supports hardware acceleration with GPUs, which allows for faster training of deep learning models. It is also easy to switch between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9075afb8-f3dd-4cf7-837b-01d788168d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b6d50-b204-4ce9-96a0-454324ef1983",
   "metadata": {},
   "source": [
    "### Creating tensors\n",
    "Tensors are the core data structures in PyTorch, similar to NumPy arrays, but with added capabilities for GPU acceleration and automatic differentiation.\n",
    "\n",
    "#### Constant tensor\n",
    "Tensors created with `torch.tensor()` are immutable by default. These are ideal for storing data that doesn't change throughout the execution of a program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e373917-3a88-4ef5-8d66-e48954e77e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a constant tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"Tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffebdc-eb22-4c09-9f26-4444f918862b",
   "metadata": {},
   "source": [
    "**Syntax**: `torch.tensor(data, dtype=None, device=None, requires_grad=False)`\n",
    "  - `data`: The initial value of the tensor, usually a Python list or NumPy array.\n",
    "  - `dtype`: (Optional) The data type of the elements in the tensor. If not specified, it is inferred from the `value`.\n",
    "  - `device`: (Optional) The device on which to store the tensor (`cpu` or `cuda` for GPU).\n",
    "  - `requires_grad`: (Optional) If `True`, tracks operations on the tensor for automatic differentiation.\n",
    "  \n",
    "#### Variables\n",
    "In PyTorch, there is no separate `Variable` class as in older versions. Now, tensors themselves can track gradients if `requires_grad=True`. This makes the distinction between tensors and variables obsolete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f8421e-d2a6-471e-84b8-29531d1aa426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor that requires gradient\n",
    "variable_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "print(variable_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f659e-0b67-4119-aabd-4c09d0c61a0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This tensor can now be used in computations where gradients are required, such as in training neural networks.\n",
    "\n",
    "#### Random tensors\n",
    "Random tensors are commonly used for initializing weights in neural networks or generating synthetic data. PyTorch provides several functions to create random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff437be-4736-486b-9274-d355ec209dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0381, -0.7294],\n",
      "        [ 0.2614, -0.7849]])\n",
      "tensor([[0.9192, 0.7996],\n",
      "        [0.4066, 0.3508]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a random tensor with a normal distribution\n",
    "random_tensor = torch.randn((2, 2))\n",
    "print(random_tensor)\n",
    "\n",
    "# Creating a random tensor with a uniform distribution\n",
    "uniform_tensor = torch.rand((2, 2))\n",
    "print(uniform_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeb885-9bde-44ef-9c50-240b04b27012",
   "metadata": {},
   "source": [
    "**Syntax** for `torch.randn()`: `torch.randn(size, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "**Syntax** for `torch.rand()`: `torch.rand(size, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "#### Using built-in functions\n",
    "PyTorch provides several built-in functions for creating tensors with specific values, which are useful for initializing models and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e09733-08c2-4972-b993-9755ad815719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor filled with ones:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor filled with zeros:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Tensor filled with value 9:\n",
      " tensor([[9, 9],\n",
      "        [9, 9]])\n",
      "Identity matrix:\n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor filled with ones\n",
    "ones_tensor = torch.ones((3, 3))\n",
    "print(\"Tensor filled with ones:\\n\", ones_tensor)\n",
    "\n",
    "# Creating a tensor filled with zeros\n",
    "zeros_tensor = torch.zeros((3, 3))\n",
    "print(\"Tensor filled with zeros:\\n\", zeros_tensor)\n",
    "\n",
    "# Creating a tensor filled with a specified value\n",
    "filled_tensor = torch.full([2, 2], 9)\n",
    "print(\"Tensor filled with value 9:\\n\", filled_tensor)\n",
    "\n",
    "# Creating an identity matrix\n",
    "identity_tensor = torch.eye(3)\n",
    "print(\"Identity matrix:\\n\", identity_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bb108-9825-477f-bfdf-f33831a22d4e",
   "metadata": {},
   "source": [
    "Each of these functions accepts similar arguments as the `torch.tensor()` function for specifying the tensor's `dtype`, `device`, and `requires_grad` properties.\n",
    "\n",
    "### Setting the seed\n",
    "Setting a seed in PyTorch ensures that random operations produce the same results each time they are run, which is crucial for reproducibility in experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba284df9-0ee8-415c-9110-2d9e9fcf4367",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367, 0.1288],\n",
      "        [0.2345, 0.2303]])\n"
     ]
    }
   ],
   "source": [
    "# Setting the global random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Creating a random tensor with a set seed\n",
    "random_tensor_seeded = torch.randn((2, 2))\n",
    "print(random_tensor_seeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28496d-65a2-47fc-aedf-b021a4c98c55",
   "metadata": {},
   "source": [
    "**Syntax**: `torch.manual_seed(seed)`\n",
    "  - `seed`: An integer value used to initialize the random number generator. This value determines the sequence of random numbers generated.\n",
    "\n",
    "For GPU operations, it is also common to set the seed for CUDA operations:\n",
    "```python\n",
    "# Setting the seed for CUDA\n",
    "torch.cuda.manual_seed(42)\n",
    "```\n",
    "\n",
    "### Shuffling a tensor\n",
    "\n",
    "Shuffling is important in machine learning to prevent the model from learning patterns in the order of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e068dd-95cc-4db8-af48-f92a57cfc16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [7, 8],\n",
      "        [1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "data_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "# Shuffling the tensor\n",
    "shuffled_tensor = data_tensor[torch.randperm(data_tensor.size(0))]\n",
    "print(shuffled_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388c4c3-9bbc-47ef-9865-144435d0f6a9",
   "metadata": {},
   "source": [
    "**Syntax**: `torch.randperm(n)` - This function generates a random permutation of integers from `0` to `n-1`, which can be used to shuffle a tensor.\n",
    "  - `n`: The number of elements to permute.\n",
    "\n",
    "We can also set a seed specifically for shuffling before shuffling operation the to ensure reproducibility.\n",
    "\n",
    "### Tensor attributes\n",
    "Tensors in PyTorch have several important attributes that provide information about their properties. Understanding these attributes is crucial for working effectively with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6494b8-d787-49c8-863e-25f6afe240b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor: torch.Size([2, 3])\n",
      "Data type of the tensor: torch.int64\n",
      "Rank of the tensor: 2\n",
      "Rank of the tensor using ndim: 2\n",
      "Size of the tensor: 6\n"
     ]
    }
   ],
   "source": [
    "example_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Shape\n",
    "print(\"Shape of the tensor:\", example_tensor.shape)\n",
    "\n",
    "# Data type\n",
    "print(\"Data type of the tensor:\", example_tensor.dtype)\n",
    "\n",
    "# Rank (number of dimensions)\n",
    "print(\"Rank of the tensor:\", example_tensor.ndimension())\n",
    "# Rank using ndim\n",
    "print(\"Rank of the tensor using ndim:\", example_tensor.ndim)\n",
    "\n",
    "# Size (total number of elements)\n",
    "print(\"Size of the tensor:\", example_tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698e262-d435-4e5f-ba30-59048822ff2b",
   "metadata": {},
   "source": [
    "- **Shape**: The shape of a tensor is a tuple of integers that describes the number of elements in each dimension. It can be accessed using the `shape` attribute. For example, the shape of a 2x3 matrix is `(2, 3)`.\n",
    "- **Data type**: The data type (`dtype`) of a tensor indicates the type of elements contained in the tensor, such as `torch.int32`, `torch.float32`, etc. It is important to ensure that operations are performed on compatible data types.\n",
    "- **Rank**: The rank of a tensor refers to the number of dimensions it has. For instance, a scalar has a rank of 0, a vector has a rank of 1, and a matrix has a rank of 2. In PyTorch, we can access the rank using the `ndimension()` method or by accessing `len(tensor.shape)`.\n",
    "- **Size**: The size of a tensor represents the total number of elements in the tensor. It can be determined using `tensor.numel()` and is useful for understanding the overall data volume.\n",
    "\n",
    "### Converting data types in tensors\n",
    "In PyTorch, tensors can have different data types such as `float32`, `int32`, `int64`, etc. Converting tensors between these data types can be necessary for various reasons, such as ensuring compatibility with specific operations, reducing memory usage, or meeting the requirements of a model or algorithm.\n",
    "\n",
    "#### Casting\n",
    "The primary methods to change the data type of a tensor are using the `tensor.type()` or `tensor.to()` methods. These methods cast a tensor to a specified data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c5e614-d76d-4c5a-9b55-7dc7fa004d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor (float32):\n",
      " tensor([1.5000, 2.5000, 3.5000])\n",
      "Cast tensor (int32):\n",
      " tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with float32 data type\n",
    "tensor_float = torch.tensor([1.5, 2.5, 3.5], dtype=torch.float32)\n",
    "print(\"Original tensor (float32):\\n\", tensor_float)\n",
    "\n",
    "# Casting tensor to int32\n",
    "tensor_int = tensor_float.to(torch.int32)\n",
    "print(\"Cast tensor (int32):\\n\", tensor_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4beaa1-8797-443c-893a-830c9ee900fb",
   "metadata": {},
   "source": [
    "**Syntax**: `tensor.to(dtype)`\n",
    "  - `tensor`: The tensor whose data type we want to change.\n",
    "  - `dtype`: The desired data type.\n",
    "  \n",
    "When converting from a higher precision data type (e.g., `float64`) to a lower precision data type (e.g., `float32` or `int32`), there may be a loss of precision.\n",
    "\n",
    "#### Converting to floating-point types\n",
    "Converting integers to floating-point types can be necessary for computations that require decimal precision, such as in neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8dd66e9-222e-454e-8813-f19d9f134c51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor (int32):\n",
      " tensor([1, 2, 3], dtype=torch.int32)\n",
      "Cast tensor (float32):\n",
      " tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with int32 data type\n",
    "tensor_int = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(\"Original tensor (int32):\\n\", tensor_int)\n",
    "\n",
    "# Casting tensor to float32\n",
    "tensor_float = tensor_int.to(torch.float32)\n",
    "print(\"Cast tensor (float32):\\n\", tensor_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dcf4f-8e69-4f35-8ec5-81e4cf54201c",
   "metadata": {},
   "source": [
    "#### Converting to boolean type\n",
    "Boolean type conversion is useful for conditions or mask operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d047bae-9a56-4dfd-a401-4bf402d7917e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor (float32):\n",
      " tensor([0., 1., 2.])\n",
      "Cast tensor (bool):\n",
      " tensor([False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with float32 data type\n",
    "tensor_float = torch.tensor([0.0, 1.0, 2.0], dtype=torch.float32)\n",
    "print(\"Original tensor (float32):\\n\", tensor_float)\n",
    "\n",
    "# Casting tensor to boolean\n",
    "tensor_bool = tensor_float.to(torch.bool)\n",
    "print(\"Cast tensor (bool):\\n\", tensor_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac160a-5c60-4bac-b5d8-8c060388aca1",
   "metadata": {},
   "source": [
    "### Tensor operations\n",
    "In PyTorch, we can perform various mathematical operations with tensors. PyTorch operations are functions that take tensors as input and produce tensors as output.\n",
    "\n",
    "\n",
    "#### Element-wise operations\n",
    "Element-wise operations are performed on corresponding elements of tensors, similar to TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0147c787-4a42-4e57-a93d-34020b29ce3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      " tensor([[ 6, -4],\n",
      "        [10, 12]])\n",
      "Subtraction:\n",
      " tensor([[-4,  8],\n",
      "        [-4, -4]])\n",
      "Multiplication:\n",
      " tensor([[  5, -12],\n",
      "        [ 21,  32]])\n",
      "Division:\n",
      " tensor([[ 0.2000, -0.3333],\n",
      "        [ 0.4286,  0.5000]])\n",
      "Exponentiation:\n",
      " tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "Absolute value:\n",
      " tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "Cumulative sum:\n",
      " tensor([[1, 2],\n",
      "        [4, 6]])\n",
      "Cumulative product:\n",
      " tensor([[1, 2],\n",
      "        [3, 8]])\n",
      "Square root:\n",
      " tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n",
      "Logarithm:\n",
      " tensor([[0.0000, 0.6931],\n",
      "        [1.0986, 1.3863]])\n",
      "Exponential:\n",
      " tensor([[ 2.7183,  7.3891],\n",
      "        [20.0855, 54.5981]])\n",
      "Reciprocal (1/x):\n",
      " tensor([[1.0000, 0.5000],\n",
      "        [0.3333, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, -6], [7, 8]])\n",
    "\n",
    "# Arithmetic operations\n",
    "print(\"Addition:\\n\", torch.add(a, b))\n",
    "print(\"Subtraction:\\n\", torch.subtract(a, b))\n",
    "print(\"Multiplication:\\n\", torch.multiply(a, b))\n",
    "print(\"Division:\\n\", torch.divide(a, b))\n",
    "print(\"Exponentiation:\\n\", torch.pow(a, 2))\n",
    "print(\"Absolute value:\\n\", torch.abs(b))\n",
    "print(\"Cumulative sum:\\n\", torch.cumsum(a, dim=0))\n",
    "print(\"Cumulative product:\\n\", torch.cumprod(a, dim=0))\n",
    "\n",
    "print(\"Square root:\\n\", torch.sqrt(a.float()))\n",
    "print(\"Logarithm:\\n\", torch.log(a.float()))\n",
    "print(\"Exponential:\\n\", torch.exp(a.float()))\n",
    "print(\"Reciprocal (1/x):\\n\", torch.reciprocal(a.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c6f25-befb-4cfa-8ee1-a3b3508832a2",
   "metadata": {},
   "source": [
    "#### Matrix operations\n",
    "PyTorch provides a range of operations to perform matrix manipulations, similar to TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d44fa33-5171-46b2-9d1c-c6f0f71f7df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication using 'matmul':\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Matrix multiplication using 'tensordot':\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "Transpose:\n",
      " tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "Permuted tensor:\n",
      " tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "Inverse matrix:\n",
      " tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "Determinant:\n",
      " tensor(-2.0000)\n",
      "Trace:\n",
      " tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Matrix multiplication using 'matmul':\\n\", torch.matmul(a, b))\n",
    "print(\"Matrix multiplication using 'tensordot':\\n\", torch.tensordot(a, b, dims=1))\n",
    "print(\"Transpose:\\n\", torch.transpose(a, 0, 1))\n",
    "print(\"Permuted tensor:\\n\", a.permute(1, 0))\n",
    "print(\"Inverse matrix:\\n\", torch.inverse(a.float()))\n",
    "print(\"Determinant:\\n\", torch.det(a.float()))\n",
    "print(\"Trace:\\n\", torch.trace(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af3ecd-fb06-438e-8208-0e9e25294ea1",
   "metadata": {},
   "source": [
    "***Permuting and transposing tensors***\n",
    "- `torch.permute()` can rearrange all dimensions of a tensor in any order. For a 2D tensor, this is functionally similar to `torch.transpose()`, which specifically swaps two dimensions.\n",
    "- `torch.permute()` is more general and can be used with tensors of any number of dimensions. `torch.transpose()` is specifically designed for swapping two dimensions of tensors.\n",
    "\n",
    "#### Tensor manipulation utilities\n",
    "PyTorch provides various tensor manipulation functions to reshape, split, and combine tensors, among other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63debd7f-fa30-4a4e-9ae5-f71073b4e0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "Stacked tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Split tensors:\n",
      " (tensor([1, 2]), tensor([3, 4]), tensor([5]))\n",
      "Chunked tensors:\n",
      " (tensor([1, 2]), tensor([3, 4]), tensor([5]))\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6]])\n",
    "c = torch.cat((a, b), dim=0)  # Concatenate along rows\n",
    "print(\"Concatenated tensor:\\n\", c)\n",
    "\n",
    "\n",
    "# Stacking\n",
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "stacked = torch.stack((a, b), dim=0)\n",
    "print(\"Stacked tensor:\\n\", stacked)\n",
    "\n",
    "\n",
    "# Splitting\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "splits = torch.split(x, 2)  # Split into chunks of size 2\n",
    "print(\"Split tensors:\\n\", splits)\n",
    "\n",
    "\n",
    "# Chuncking\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "chunks = torch.chunk(x, 3)  # Split into 3 chunks\n",
    "print(\"Chunked tensors:\\n\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8967fb-4f58-42ac-a47f-ebd3049bc76c",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **`torch.cat()`**: Concatenates a sequence of tensors along a specified dimension.\n",
    "- **`torch.stack()`**: Stacks a sequence of tensors along a new dimension. Unlike `torch.cat()`, this operation creates a new dimension.\n",
    "- **`torch.split()`**: Splits a tensor into smaller tensors of specified sizes.\n",
    "- **`torch.chunk()`**: Splits a tensor into a specified number of chunks.\n",
    "\n",
    "\n",
    "### Indexing and slicing tensors\n",
    "Indexing and slicing in PyTorch is similar to NumPy and TensorFlow, allowing access to specific elements or sub-tensors within a larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f756c82e-356f-4846-b12c-a13b47334e88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element:\n",
      " tensor(2)\n",
      "Row:\n",
      " tensor([5, 6, 7, 8])\n",
      "Column:\n",
      " tensor([3, 7])\n",
      "Selected elements:\n",
      " tensor([[2, 1],\n",
      "        [6, 5]])\n",
      "Sliced tensor:\n",
      " tensor([2, 3])\n",
      "Sliced tensor with step:\n",
      " tensor([[1, 3],\n",
      "        [5, 7]])\n",
      "Sliced tensor with torch.narrow:\n",
      " tensor([[2, 3],\n",
      "        [6, 7]])\n",
      "Multi-dimensional sliced tensor:\n",
      " tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[7, 8, 9]]])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "\n",
    "# Indexing\n",
    "print(\"Element:\\n\", tensor[0, 1])  # Accessing a specific element - element at row 0, column 1\n",
    "print(\"Row:\\n\", tensor[1])  # Accessing a row - entire row 1\n",
    "print(\"Column:\\n\", tensor[:, 2])  # Accessing a column - entire column 2\n",
    "print(\"Selected elements:\\n\", tensor[:, [1, 0]])  # Indexing with a list\n",
    "\n",
    "# Slicing\n",
    "print(\"Sliced tensor:\\n\", tensor[0, 1:3])  # Slicing a sub-tensor - elements from index 1 to 2 in row 0\n",
    "print(\"Sliced tensor with step:\\n\", tensor[:, 0:4:2])  # Slicing with step size - every second element in each row\n",
    "print(\"Sliced tensor with torch.narrow:\\n\", tensor.narrow(1, 1, 2))  # Slicing with torch.narrow - dim 1, start at index 1, size 2\n",
    "\n",
    "# Multi-dimensional slicing\n",
    "tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(\"Multi-dimensional sliced tensor:\\n\", tensor[0:2, 0:1, :])  # Slicing in 3D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13113ca8-7e31-4b1c-9f87-eb56130f1fcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Indexing\n",
    "- Access individual elements or slices of a tensor using Python indexing.\n",
    "    - **Syntax**: `tensor[index]`\n",
    "      - `index`: The position of the element or slice to access.\n",
    "\n",
    "- Advanced indexing can be achieved using integer arrays or lists.\n",
    "    - **Syntax**: `tensor[:, [indices]]`\n",
    "      - `indices`: The indices of the slices to gather.\n",
    "      \n",
    "##### Slicing\n",
    "- Extract sub-tensors by specifying ranges for each dimension.\n",
    "    - **Syntax**: `tensor[start:stop:step]`\n",
    "      - `start`: The starting index.\n",
    "      - `stop`: The ending index.\n",
    "      - `step`: (Optional) The step size between indices.\n",
    "\n",
    "- The `torch.narrow()` function provides more explicit control over slicing by specifying the dimension, starting index, and size.\n",
    "    - **Syntax**: `tensor.narrow(dim, start, length)`\n",
    "      - `dim`: The dimension along which to slice.\n",
    "      - `start`: The starting index.\n",
    "      - `length`: The size of the slice.\n",
    "      \n",
    "### Broadcasting\n",
    "Broadcasting in PyTorch is similar to TensorFlow, allowing for element-wise operations on tensors of different shapes without explicit reshaping or replication.\n",
    "\n",
    "The broadcasting rules in PyTorch follow the same principles:\n",
    "1. If the tensors have different ranks (number of dimensions), the shape of the tensor with fewer dimensions is padded with ones on the left side until both shapes have the same length.\n",
    "2. Tensors are compatible when:\n",
    "   - The sizes of the dimensions are equal, or\n",
    "   - One of the sizes is 1.\n",
    "3. If the dimensions of the tensors are not compatible according to these rules, broadcasting is not possible, and an error will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970309ec-93e3-4a3a-8da6-9b907d5f6814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting scalar:\n",
      " tensor([[6, 7],\n",
      "        [8, 9]])\n",
      "\n",
      "Broadcasting vector:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "\n",
      "Broadcasting different shapes:\n",
      " tensor([[11, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33]])\n",
      "\n",
      "Broadcasting with explicit reshaping:\n",
      " tensor([[11, 12],\n",
      "        [23, 24]])\n"
     ]
    }
   ],
   "source": [
    "### Example 1: Scalar and tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "scalar = torch.tensor(5)\n",
    "\n",
    "# Broadcasting scalar to add to each element of the tensor\n",
    "result = tensor + scalar\n",
    "print(\"Broadcasting scalar:\\n\", result)\n",
    "\n",
    "\n",
    "### Example 2: Vector and matrix\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "vector = torch.tensor([10, 20, 30])\n",
    "\n",
    "# Broadcasting vector to add to each row of the matrix\n",
    "result = matrix + vector\n",
    "print(\"\\nBroadcasting vector:\\n\", result)\n",
    "\n",
    "\n",
    "### Example 3: Different shapes\n",
    "tensor1 = torch.tensor([[1], [2], [3]])   # Shape (3, 1)\n",
    "tensor2 = torch.tensor([10, 20, 30])      # Shape (3,)\n",
    "\n",
    "# Broadcasting tensor2 to add to each column of tensor1\n",
    "result = tensor1 + tensor2\n",
    "print(\"\\nBroadcasting different shapes:\\n\", result)\n",
    "\n",
    "\n",
    "### Example 4: Broadcasting with explicit reshaping\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])          # Shape (2, 2)\n",
    "tensor2 = torch.tensor([10, 20])                  # Shape (2,)\n",
    "# Reshaping tensor2 to match tensor1 for broadcasting\n",
    "tensor2_reshaped = tensor2.view(2, 1)\n",
    "\n",
    "# Broadcasting reshaped tensor2 to add to tensor1\n",
    "result = tensor1 + tensor2_reshaped\n",
    "print(\"\\nBroadcasting with explicit reshaping:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78301718-8693-44ca-9c96-8766fcb0e743",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Explanations\n",
    "\n",
    "1. Example 1: scalar and tensor\n",
    "   - Rule applied: Scalars are automatically broadcast to any shape. This is because a scalar can be thought of as a tensor with shape `()`, and it can expand to match any tensorâ€™s shape.\n",
    "   - Explanation: The scalar `5` is broadcast to match the shape of the tensor `[[1, 2], [3, 4]]`, which has a shape of `(2, 2)`. The scalar is conceptually expanded to a 2x2 tensor `[[5, 5], [5, 5]]` and then added element-wise, resulting in `[[6, 7], [8, 9]]`.\n",
    "\n",
    "2. Example 2: Vector and matrix\n",
    "\n",
    "   - Rule applied: When the dimensions are different, the smaller tensor (vector) is padded with ones on the left, turning `(3,)` into `(1, 3)`. The vector can then be broadcast across the matrix rows.\n",
    "   - Explanation: The vector `[10, 20, 30]` with shape `(3,)` is effectively reshaped to `(1, 3)` to match the shape of the matrix `(2, 3)`. It is then broadcast to match the matrix dimensions by replicating the vector along the new axis. This results in each row of the matrix having `[10, 20, 30]` added to it, yielding `[[11, 22, 33], [14, 25, 36]]`.\n",
    "\n",
    "3. Example 3: Different shapes\n",
    "\n",
    "   - Rule applied: One of the dimensions is 1. This allows `tensor2` to be broadcast along the second dimension of `tensor1`.\n",
    "   - Explanation: `tensor1` has shape `(3, 1)`, and `tensor2` has shape `(3,)`. The shape `(3,)` is interpreted as `(1, 3)`. The first dimension of `tensor1` matches, and the second dimension is broadcast by expanding `tensor2` across each row. This results in adding `tensor2` to each row of `tensor1`, resulting in a shape of `(3, 3)`, where the output is `[[11, 12, 13], [22, 23, 24], [33, 34, 35]]`.\n",
    "\n",
    "4. Example 4: Broadcasting with explicit reshaping\n",
    "\n",
    "   - Rule applied: Reshaping helps manually adjust dimensions to fit broadcasting rules.\n",
    "   - Explanation: `tensor1` has shape `(2, 2)`, and `tensor2` has shape `(2,)`, which we reshape to `(2, 1)` to align with the first dimension of `tensor1`. Now `tensor2_reshaped` can be broadcast to `(2, 2)`. Each element of `tensor2_reshaped` is broadcasted across the columns of `tensor1`. The operation adds the reshaped `tensor2` to each column, resulting in `[[11, 12], [23, 24]]`.\n",
    "   \n",
    "### Reshaping tensors\n",
    "Reshaping tensors in PyTorch allows us to change the shape of a tensor without altering its data. This is often necessary when preparing data for model input or when manipulating data to perform specific operations. A tensor's shape describes the size of each dimension. Reshaping involves changing the dimensions while keeping the total number of elements constant. PyTorch provides several functions for reshaping tensors, including `torch.reshape()` and `torch.view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d30fce-6322-403d-8282-e427eb9ded40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened tensor:\n",
      " tensor([1, 2, 3, 4, 5, 6])\n",
      "Flattened tensor with view:\n",
      " tensor([1, 2, 3, 4, 5, 6])\n",
      "Reshaped tensor to 2D:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Reshaped tensor to 2D with view:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "3D Tensor with added dimension:\n",
      " tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [6]]])\n",
      "3D Tensor with added dimension using view:\n",
      " tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [6]]])\n",
      "Reshaped tensor with inferred dimension:\n",
      " tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "Reshaped tensor with inferred dimension using view:\n",
      " tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "#### Example 1: Flattening a tensor\n",
    "# Creating a 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Reshaping to a 1D tensor (flattening)\n",
    "flattened_tensor = torch.reshape(tensor_2d, (-1,))\n",
    "print(\"Flattened tensor:\\n\", flattened_tensor)\n",
    "flattened_tensor_view = tensor_2d.view(-1)\n",
    "print(\"Flattened tensor with view:\\n\", flattened_tensor_view)\n",
    "\n",
    "\n",
    "#### Example 2: Changing dimensions\n",
    "# Creating a 1D tensor\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Reshaping to a 2D tensor\n",
    "reshaped_tensor = torch.reshape(tensor_1d, (2, 3))\n",
    "print(\"Reshaped tensor to 2D:\\n\", reshaped_tensor)\n",
    "reshaped_tensor_view = tensor_1d.view(2, 3)\n",
    "print(\"Reshaped tensor to 2D with view:\\n\", reshaped_tensor_view)\n",
    "\n",
    "\n",
    "#### Example 3: Adding a dimension\n",
    "# Creating a 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Adding a new dimension to create a 3D tensor\n",
    "reshaped_tensor = torch.reshape(tensor_2d, (2, 3, 1))\n",
    "print(\"3D Tensor with added dimension:\\n\", reshaped_tensor)\n",
    "reshaped_tensor_view = tensor_2d.view(2, 3, 1)\n",
    "print(\"3D Tensor with added dimension using view:\\n\", reshaped_tensor_view)\n",
    "\n",
    "#### Example 4: Using `-1` for automatic inference\n",
    "# Creating a 3D tensor\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# Reshaping using -1 for automatic inference\n",
    "reshaped_tensor = torch.reshape(tensor_3d, (2, -1))\n",
    "print(\"Reshaped tensor with inferred dimension:\\n\", reshaped_tensor)\n",
    "reshaped_tensor_view = tensor_3d.view(2, -1)\n",
    "print(\"Reshaped tensor with inferred dimension using view:\\n\", reshaped_tensor_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade4254-6c52-4d2e-8663-5c3ad21ef7d5",
   "metadata": {},
   "source": [
    "**Syntax**:\n",
    "- `torch.reshape(tensor, shape)`\n",
    "  - `tensor`: The tensor to reshape.\n",
    "  - `shape`: A tuple specifying the new shape.\n",
    "  \n",
    "- `tensor.view(*shape)`\n",
    "  - `shape`: A tuple specifying the new shape. Like `torch.reshape()`, the total number of elements must remain the same.\n",
    "  - Note: `torch.view()` is only available for contiguous tensors, meaning the tensor must be laid out in memory in a contiguous block.\n",
    "\n",
    "  \n",
    "##### Explanations\n",
    "1. Example 1: Flattening a tensor involves converting a multi-dimensional tensor into a 1D tensor. This is useful when we need to feed data into a neural network layer that requires a vector input. The shape `(-1,)` automatically infers the size needed to flatten the tensor into a 1D array, resulting in `[1, 2, 3, 4, 5, 6]`.\n",
    "2. Example 2: We can change the dimensions of a tensor, provided the total number of elements remains the same. The original tensor with shape `(6,)` is reshaped to `(2, 3)`, resulting in a 2D tensor `[[1, 2, 3], [4, 5, 6]]`.\n",
    "3. Example 3: Adding a new dimension can be useful for adjusting the shape to fit model input requirements. The tensor is reshaped from `(2, 3)` to `(2, 3, 1)`, adding a new dimension without changing the total number of elements.\n",
    "4. Example 4: The `-1` in the shape allows PyTorch to infer the appropriate size for that dimension. The original tensor with shape `(2, 2, 2)` is reshaped to `(2, 4)`, where `-1` infers the second dimension.\n",
    "\n",
    "\n",
    "### Squeezing a tensor\n",
    "Squeezing a tensor in PyTorch involves removing dimensions of size 1 from its shape. This operation is useful when we need to simplify the tensor's shape, often to make it compatible with certain operations or layers. The `torch.squeeze()` function removes all dimensions with size 1 from a tensor's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9c1654-ba8e-4300-901a-2449b0ceb713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed tensor:\n",
      " tensor([1, 2, 3])\n",
      "Tensor squeezed at dim 0:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "#### Example 1: Squeezing all single dimensions\n",
    "# Creating a tensor with single dimensions\n",
    "tensor = torch.tensor([[[1], [2], [3]]])  # Shape (1, 3, 1)\n",
    "\n",
    "# Squeezing all dimensions of size 1\n",
    "squeezed_tensor = torch.squeeze(tensor)\n",
    "print(\"Squeezed tensor:\\n\", squeezed_tensor)\n",
    "\n",
    "\n",
    "#### Example 2: Squeezing specific dimensions\n",
    "# Creating a tensor with single dimensions\n",
    "tensor = torch.tensor([[[1], [2], [3]]])  # Shape (1, 3, 1)\n",
    "\n",
    "# Squeezing a specific dimension (e.g., axis 0)\n",
    "squeezed_tensor_dim = torch.squeeze(tensor, dim=0)\n",
    "print(\"Tensor squeezed at dim 0:\\n\", squeezed_tensor_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb7f00-beec-4a25-bb01-d96a98aefbe2",
   "metadata": {},
   "source": [
    "- **Syntax**: `torch.squeeze(tensor, dim=None)`\n",
    "  - `tensor`: The tensor to squeeze.\n",
    "  - `dim`: (Optional) An integer specifying which specific dimension to squeeze. If not specified, all dimensions with size 1 will be removed.\n",
    "\n",
    "##### Explanations\n",
    "1. Example 1: The tensor with shape `(1, 3, 1)` is squeezed to `(3,)`, removing both dimensions with size 1.\n",
    "2. Example 2: By specifying `dim=0`, only the dimension at position 0 is removed, resulting in a shape of `(3, 1)`.\n",
    "\n",
    "### One-hot encoding\n",
    "One-hot encoding is a technique used to convert categorical data into a format suitable for machine learning models. It transforms each category into a vector where only one element is \"hot\" (set to 1), and all others are \"cold\" (set to 0). The `torch.nn.functional.one_hot()` function creates a one-hot representation of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca71966d-76bf-43ce-9c06-d3148a0ef484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded tensor:\n",
      " tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 1, 0]])\n",
      "One-hot encoded tensor with custom axis:\n",
      " tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 1],\n",
      "        [0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#### Example 1: Basic one-hot encoding\n",
    "# Indices representing categories\n",
    "indices = torch.tensor([0, 1, 2, 1])\n",
    "\n",
    "# One-hot encode with depth of 3\n",
    "one_hot_encoded = F.one_hot(indices, num_classes=3)\n",
    "print(\"One-hot encoded tensor:\\n\", one_hot_encoded)\n",
    "\n",
    "\n",
    "#### Example 2: Custom axis for one-hot encoding\n",
    "# Indices representing categories\n",
    "indices = torch.tensor([0, 1, 2, 1])\n",
    "\n",
    "# One-hot encode with depth of 3, placing vectors along a new first axis\n",
    "one_hot_encoded_axis = F.one_hot(indices, num_classes=3).transpose(0, 1)\n",
    "print(\"One-hot encoded tensor with custom axis:\\n\", one_hot_encoded_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a3347-3656-4c83-9097-6075050c19cc",
   "metadata": {},
   "source": [
    "- **Syntax**: `F.one_hot(indices, num_classes)`\n",
    "  - `indices`: A tensor of indices to be converted into one-hot vectors.\n",
    "  - `num_classes`: The number of categories (i.e., the length of the one-hot vectors).\n",
    "\n",
    "##### Explanations\n",
    "1. Example 1: The `num_classes` is set to `3`, meaning there are three possible categories (0, 1, and 2). This specifies the length of each one-hot vector. The `indices` tensor `[0, 1, 2, 1]` is one-hot encoded with a `num_classes` of 3, resulting in a tensor where each index is represented as a one-hot vector. The resulting tensor has a shape of `(4, 3)` because there are four indices and each one is converted to a vector of length `3`.\n",
    "2. Example 2: By transposing the one-hot encoded tensor, we rearrange the one-hot vectors along a different axis, resulting in a different tensor structure.\n",
    "\n",
    "### Manipulating variable tensors\n",
    "In PyTorch, `torch.nn.Parameter` or `torch.tensor` with `requires_grad=True` allows us to create tensors that require gradients, making them suitable for optimization during training. Unlike TensorFlow's `tf.Variable`, PyTorch allows operations on tensors in place or by creating new tensors. Updating values in place can be done with regular tensor operations, and gradients are automatically tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b527032a-6043-4d64-ad8f-c96166735554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial variable:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "Updated variable:\n",
      " tensor([[5., 6.],\n",
      "        [7., 8.]], requires_grad=True)\n",
      "Updated variable after addition:\n",
      " tensor([[6., 7.],\n",
      "        [8., 9.]], grad_fn=<AddBackward0>)\n",
      "Updated variable after subtraction:\n",
      " tensor([[4., 5.],\n",
      "        [6., 7.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Creating a variable with an initial value\n",
    "variable = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "print(\"Initial variable:\\n\", variable)\n",
    "\n",
    "# Re-assigning a new value to the variable\n",
    "variable = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
    "print(\"Updated variable:\\n\", variable)\n",
    "\n",
    "# Add to the variable\n",
    "variable = variable + torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "print(\"Updated variable after addition:\\n\", variable)\n",
    "\n",
    "# Subtract from the variable\n",
    "variable = variable - torch.tensor([[2.0, 2.0], [2.0, 2.0]])\n",
    "print(\"Updated variable after subtraction:\\n\", variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992014c-46da-407c-a338-149d7e6e8e0f",
   "metadata": {},
   "source": [
    "PyTorch allows direct arithmetic operations on tensors, which can be used to modify their values. These operations can be done in place or by reassigning the tensor, depending on the need.\n",
    " \n",
    "#### Converting variables to tensors\n",
    "In PyTorch, tensors are used directly in operations, and there is no need to explicitly convert between variables and tensors as in TensorFlow. However, if we need to ensure that a tensor doesn't require gradients, we can use `detach()` to get a tensor without the gradient tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "665ed53e-0ee7-4788-ae3f-e0aca221428a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor version without gradients:\n",
      " tensor([[4., 5.],\n",
      "        [6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# Convert variable to a tensor without gradients\n",
    "tensor_version = variable.detach()\n",
    "print(\"Tensor version without gradients:\\n\", tensor_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c489ff-c3dd-4c57-8c0f-ba1136912ee5",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "Automatic differentiation is a technique used to compute gradients (derivatives) of functions with respect to their inputs automatically. This is crucial in training machine learning models, where gradients are needed to adjust model parameters for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51e55fe7-146e-427d-9cc5-17891a0a5e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      " tensor([4., 6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# Compute gradients\n",
    "y.sum().backward()\n",
    "\n",
    "# Access gradients\n",
    "print(\"Gradients:\\n\", x.grad)\n",
    "\n",
    "# No-gradient context\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    \n",
    "# Detaching tensor\n",
    "x_detached = x.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd654b-8059-4a55-acee-81da22c20820",
   "metadata": {},
   "source": [
    "**Syntax**:\n",
    "- **`requires_grad` attribute**: To track operations on a tensor for gradient computation, we must set the `requires_grad` attribute to `True`. By default, tensors have `requires_grad=False`.\n",
    "- **Computing gradients**: Once we perform operations on tensors with `requires_grad=True`, we can compute gradients using the `backward()` method. This method calculates the derivative of the output tensor with respect to the input tensors.\n",
    "    - `y.sum()`: We sum up all elements of `y` to get a single value. This is necessary because `.backward()` needs a scalar (single value) to compute gradients. In this case, it sums the elements of `y` to produce one number (e.g., `4 + 9 + 16 = 29`).\n",
    "    - `.backward()`: Computes the gradient of this summed value with respect to `x`. PyTorch tracks how changes in `x` affect this summed value, so we can understand how to adjust `x` to minimize or maximize this value.\n",
    "- ***Accessing gradients***: After calling `.backward()`, we can access the gradients stored in `x.grad`. This shows how each element in `x` affects the final summed value of `y`. Here, `x.grad` will contain the gradient of the summed `y` with respect to `x`.\n",
    "- **No-gradient context**: For inference or operations where gradients are not needed, use the `torch.no_grad()` context manager to prevent the calculation of gradients, saving memory and computation.\n",
    "- **Detaching tensors**: To stop tracking history and avoid future gradient computations, use the `detach()` method. This creates a new tensor that shares the same data but does not require gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
