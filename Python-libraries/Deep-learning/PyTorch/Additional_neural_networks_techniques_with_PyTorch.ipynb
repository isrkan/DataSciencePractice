{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3193863c-2278-402d-9732-bf55a21fe05e",
   "metadata": {},
   "source": [
    "# Additional techniques for building neural networks with PyTorch\n",
    "\n",
    "This notebook explores additional techniques to enhance neural networks' performance using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2baf4b-a946-4ba0-939b-33f206c17c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6558c0b0-f8a1-4ce3-8bfa-5fcca2b02996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "X_train = np.random.rand(100, 10).astype(np.float32)\n",
    "y_train = np.random.rand(100, 1).astype(np.float32)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5170504-bfd1-41b1-9c28-aef5cc3fc9b4",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "Regularization is essential to prevent overfitting, where a model performs well on training data but poorly on unseen data. In PyTorch, regularization techniques like L1 and L2 regularization can be easily applied by adding penalty terms to the loss function or using regularization parameters in layers.\n",
    "\n",
    "### L1 and L2 regularization\n",
    "\n",
    "- **L1 regularization**: Adds a penalty equal to the absolute value of the model weights, encouraging sparsity (i.e., some weights are driven to zero). In PyTorch, we can add L1 by manually adding the penalty to the loss function.\n",
    "- **L2 regularization**: Adds a penalty equal to the square of the model weights, encouraging smaller weights. In PyTorch, L2 regularization can be implemented using the `weight_decay` parameter in optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c278e3e-8d8e-40b1-a6c0-d10eb6fd4cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.6478\n",
      "Epoch 2, Loss: 2.6148\n",
      "Epoch 3, Loss: 2.5822\n",
      "Epoch 4, Loss: 2.5500\n",
      "Epoch 5, Loss: 2.5184\n",
      "Epoch 6, Loss: 2.4874\n",
      "Epoch 7, Loss: 2.4570\n",
      "Epoch 8, Loss: 2.4270\n",
      "Epoch 9, Loss: 2.3974\n",
      "Epoch 10, Loss: 2.3683\n"
     ]
    }
   ],
   "source": [
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define L1 regularization manually\n",
    "def l1_regularization(model, lambda_l1):\n",
    "    l1_penalty = sum(param.abs().sum() for param in model.parameters())\n",
    "    return lambda_l1 * l1_penalty\n",
    "\n",
    "# Define L2 regularization (already available in optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # L2 regularization via weight_decay\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop with L1 regularization\n",
    "num_epochs = 10\n",
    "lambda_l1 = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Add L1 regularization\n",
    "    loss += l1_regularization(model, lambda_l1)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6778-0c36-4ff4-86c3-f413eb4fdf40",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **L1 regularization**: We manually add the L1 penalty by summing the absolute values of the model's parameters and multiplying by a regularization coefficient (`lambda_l1`) and adding this to the loss.\n",
    "- **L2 regularization**: Can be directly implemented using the `weight_decay` parameter in optimizers like `Adam`. This parameter controls the strength of L2 regularization.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique that randomly sets a fraction of input units to zero during training. This prevents the network from becoming overly dependent on any particular feature, helping it generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ca59db-ce20-4146-b98c-43d384dc9e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2200\n",
      "Epoch 2, Loss: 0.2077\n",
      "Epoch 3, Loss: 0.2065\n",
      "Epoch 4, Loss: 0.1859\n",
      "Epoch 5, Loss: 0.1817\n",
      "Epoch 6, Loss: 0.1665\n",
      "Epoch 7, Loss: 0.1619\n",
      "Epoch 8, Loss: 0.1487\n",
      "Epoch 9, Loss: 0.1325\n",
      "Epoch 10, Loss: 0.1374\n"
     ]
    }
   ],
   "source": [
    "class DropoutNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropoutNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.2)  # 20% dropout\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # Apply first layer and activation function\n",
    "        x = self.dropout1(x) # Apply dropout\n",
    "        x = torch.relu(self.fc2(x)) # Apply second layer and activation function\n",
    "        x = self.dropout2(x) # Apply dropout\n",
    "        x = self.fc3(x) # Apply final output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = DropoutNN()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb5d717-ff25-499b-b13a-07553714fdb8",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `nn.Dropout(p)`: randomly sets a fraction `p` of the input units to zero during training. This encourages the model to learn redundant representations, improving generalization.\n",
    "\n",
    "In PyTorch, dropout is defined in the `__init__` method of the neural network class because this method is where we set up all the layers and components of our model. When we define dropout here, we are telling PyTorch to include a dropout layer as part of our model architecture. This setup ensures that dropout is a part of the model when it is initialized, which allows us to control the dropout rate and apply it consistently during training.\n",
    "\n",
    "In the `forward` method, we apply dropout to the model’s activations (the outputs of layers). Dropout is applied during training to randomly set a portion of the activations to zero, which helps to prevent overfitting. When we call `self.dropout(x)` in the forward method, we are telling PyTorch to randomly drop out some of the activations based on the dropout rate.\n",
    "\n",
    "During evaluation (e.g., when you are testing the model or making predictions), dropout is automatically turned off. After training, we switch the model to evaluation mode using `model.eval()`, which disables dropout, so the entire network is used for making predictions.\n",
    "\n",
    "\n",
    "## Normalization techniques\n",
    "Normalization techniques are methods used to improve the training process of neural networks. They help make training faster and more stable by adjusting the inputs to each layer.\n",
    "\n",
    "### Layer normalization\n",
    "Layer normalization normalizes the inputs across the features for each individual training example. This means that for each data point, the values of its features are adjusted to have a mean of zero and a standard deviation of one, independent of other data points. Since layer normalization is done for each example independently, it is particularly useful in RNNs and other models where the data comes in sequences, as it ensures that each step in the sequence is treated consistently.\n",
    "\n",
    "- How it works:\n",
    "    - It calculates the mean and variance for each individual layer across all its neurons.\n",
    "    - Then, it normalizes the activations within that layer (e.g., the outputs of neurons), so they all have the same range and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885060b0-7774-40a4-800b-b30cb597d354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6791\n",
      "Epoch 2, Loss: 0.3399\n",
      "Epoch 3, Loss: 0.2070\n",
      "Epoch 4, Loss: 0.2194\n",
      "Epoch 5, Loss: 0.2590\n",
      "Epoch 6, Loss: 0.2628\n",
      "Epoch 7, Loss: 0.2324\n",
      "Epoch 8, Loss: 0.1872\n",
      "Epoch 9, Loss: 0.1449\n",
      "Epoch 10, Loss: 0.1163\n"
     ]
    }
   ],
   "source": [
    "class LayerNormNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LayerNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.layernorm1 = nn.LayerNorm(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.layernorm2 = nn.LayerNorm(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.layernorm1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = LayerNormNN()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edba5ab-282c-4192-8822-e0c50ac87d12",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Layer normalization** (`nn.LayerNorm(normalized_shape)`): Normalizes the activations of a layer for each individual data sample. It adjusts the mean and variance of the activations, which helps stabilize and speed up training.\n",
    "    - `normalized_shape`: The shape of the input that needs to be normalized, typically the number of features in the layer.\n",
    "\n",
    "\n",
    "### Batch normalization\n",
    "Batch normalization normalizes the inputs across the batch dimension. This means that it computes the mean and standard deviation of each feature across the entire batch of training examples, and then normalizes each feature using these statistics. By normalizing the data in batches, batch normalization reduces what's known as \"internal covariate shift,\" where the distribution of each layer’s inputs changes during training. This helps the model to converge faster and can allow for higher learning rates, leading to quicker and often more effective training. Batch normalization is more commonly used in feedforward networks, CNNs, and in some cases RNNs.\n",
    "\n",
    "- How it works:\n",
    "    - During training, for each mini-batch, it calculates the mean and variance of the activations (outputs of neurons) across that batch.\n",
    "    - Then, it normalizes these activations to have a mean of 0 and a standard deviation of 1.\n",
    "    - After normalization, it applies a scale and shift, allowing the network to adjust the normalized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c5991-e5b7-4eb7-9946-6e00923287e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8467\n",
      "Epoch 2, Loss: 0.7256\n",
      "Epoch 3, Loss: 0.6294\n",
      "Epoch 4, Loss: 0.5546\n",
      "Epoch 5, Loss: 0.4976\n",
      "Epoch 6, Loss: 0.4552\n",
      "Epoch 7, Loss: 0.4238\n",
      "Epoch 8, Loss: 0.4000\n",
      "Epoch 9, Loss: 0.3809\n",
      "Epoch 10, Loss: 0.3645\n"
     ]
    }
   ],
   "source": [
    "class BatchNormNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = BatchNormNN()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f7333-0d00-4aa0-89da-ef2ba28821a2",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Batch normalization** (`nn.BatchNorm1d(num_features)`): Normalizes the activations (outputs) across the entire batch. This helps stabilize training and can improve convergence by reducing internal covariate shift.\n",
    "    - `num_features`: The number of features in the input to the BatchNorm layer.\n",
    "\n",
    "\n",
    "### Group normalization\n",
    "Group normalization is a technique that splits the channels of the data into smaller groups and then applies normalization within each group. In linear (fully connected) layers, channels typically refer to the number of features or neurons in the layer. In convolutional layers, channels are the depth dimension of the input or output tensor. For example, in an RGB image, there are 3 channels corresponding to red, green, and blue.\n",
    "\n",
    "It is particularly useful in scenarios where batch normalization might not perform well, such as with very small batch sizes. Unlike batch normalization, group normalization does not depend on the batch size, making it more effective in situations with small or varying batch sizes. We can control the granularity of the normalization process, by adjusting the number of groups, allowing for more flexibility in different tasks.\n",
    "- How it works:\n",
    "    - Grouping: The channels of the input are divided into groups (e.g., if you have 32 channels and divide into 4 groups, each group will have 8 channels).\n",
    "    - Normalization: For each group, it computes the mean and variance and then normalizes the activations within that group to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438743ef-c6cb-4f16-a461-7b5a0108e7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7407\n",
      "Epoch 2, Loss: 0.4172\n",
      "Epoch 3, Loss: 0.2973\n",
      "Epoch 4, Loss: 0.2756\n",
      "Epoch 5, Loss: 0.2725\n",
      "Epoch 6, Loss: 0.2597\n",
      "Epoch 7, Loss: 0.2292\n",
      "Epoch 8, Loss: 0.1956\n",
      "Epoch 9, Loss: 0.1638\n",
      "Epoch 10, Loss: 0.1439\n"
     ]
    }
   ],
   "source": [
    "class GroupNormNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GroupNormNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.groupnorm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.groupnorm2 = nn.GroupNorm(num_groups=4, num_channels=32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.groupnorm1(x.unsqueeze(2)).squeeze(2)  # Add dimension, normalize, and remove it\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.groupnorm2(x.unsqueeze(2)).squeeze(2)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = GroupNormNN()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21570421-cf03-4f97-8994-1c0939de6076",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Group normalization** (`nn.GroupNorm(num_groups, num_channels)`): Divides the channels into groups and normalizes within each group.\n",
    "    - `num_groups`: The number of groups to divide the channels into.\n",
    "    - `num_channels`: The number of channels in the input.\n",
    "\n",
    "\n",
    "## Weight initialization strategies\n",
    "\n",
    "Weight initialization is crucial for training neural networks efficiently and effectively. Proper initialization can help avoid problems like vanishing or exploding gradients, particularly in deep networks.\n",
    "\n",
    "### Glorot (Xavier) initialization\n",
    "Glorot initialization, also known as Xavier initialization, is suitable for layers using linear or tanh activation functions. It aims to keep the scale of the gradients similar across all layers. Weights are typically initialized from a distribution with zero mean and a variance of 2/(input units + output units). Typically used in networks with activation functions like `tanh` or `linear` or when we are unsure about the network's behavior and want a general-purpose initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef0ae5d-d5dc-4350-a884-400a412d0f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with Glorot initialization:\n",
      "Epoch 1, Loss: 0.2428\n",
      "Epoch 2, Loss: 0.2132\n",
      "Epoch 3, Loss: 0.1874\n",
      "Epoch 4, Loss: 0.1653\n",
      "Epoch 5, Loss: 0.1472\n",
      "Epoch 6, Loss: 0.1334\n",
      "Epoch 7, Loss: 0.1237\n",
      "Epoch 8, Loss: 0.1182\n",
      "Epoch 9, Loss: 0.1160\n",
      "Epoch 10, Loss: 0.1164\n"
     ]
    }
   ],
   "source": [
    "# Define a simple feedforward neural network with Glorot Initialization\n",
    "class GlorotNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlorotNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Apply Glorot (Xavier) initialization to the weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and train the model with Glorot Initialization\n",
    "model_glorot = GlorotNN()\n",
    "optimizer = optim.Adam(model_glorot.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training model with Glorot initialization:\")\n",
    "for epoch in range(10):\n",
    "    model_glorot.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_glorot(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f5411-3aeb-4c98-b144-4b2c790973c7",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **`__init__` method**: In PyTorch, the `__init__` method is where we define the architecture of our neural network. Normally, when we define layers in PyTorch (like `nn.Linear`), PyTorch automatically initializes the weights and biases of those layers using some default method. We don’t need to explicitly create them as they are generated as part of the `nn.Linear` layer. However, sometimes we might want to customize this initialization process for better training performance. This is where adding custom initialization comes in, such as initialization functions or additional configurations.\n",
    "\n",
    "- **How to customize initialization?**\n",
    "- **Accessing weights** (`self.fc.weight`): In the model class, we can access the weights of a layer directly by referring to them as `self.fc1.weight`, `self.fc2.weight`, etc. The `.weight` is an attribute of the `nn.Linear` object that stores the matrix of weights for that layer. Even though we didn’t manually define `weight`, PyTorch did it for us under the hood when we created `self.fc`.\n",
    "- **Glorot initialization** (`nn.init.xavier_uniform_`): This method initializes the weights using a uniform distribution with bounds calculated to maintain the variance of gradients throughout the network.\n",
    "    - Adding lines like `nn.init.xavier_uniform_(self.fc1.weight)` in the `__init__` method is how we can replace the default initialization with our custom one. Specifically, this line of code applies Glorot (Xavier) initialization to the weights of `self.fc1`. \n",
    "        - The `__init__` method is where we define the structure of our model. Weights Initialization is something we typically only need to do once—right after the layers are created. By putting it in the `__init__` method, we ensure that the custom initialization is applied every time we instantiate the model.\n",
    "\n",
    "\n",
    "#### Kaiming (He) initialization\n",
    "Kaiming initialization, also known as He initialization, is ideal for layers with ReLU or its variants as activation functions. This method is designed to mitigate the vanishing gradient problem by initializing weights from a distribution with zero mean and a variance of 2/(input units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945aad52-734c-4f03-8ada-b3575263f2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with Kaiming initialization:\n",
      "Epoch 1, Loss: 0.1149\n",
      "Epoch 2, Loss: 0.1116\n",
      "Epoch 3, Loss: 0.1085\n",
      "Epoch 4, Loss: 0.1056\n",
      "Epoch 5, Loss: 0.1031\n",
      "Epoch 6, Loss: 0.1009\n",
      "Epoch 7, Loss: 0.0989\n",
      "Epoch 8, Loss: 0.0971\n",
      "Epoch 9, Loss: 0.0955\n",
      "Epoch 10, Loss: 0.0940\n"
     ]
    }
   ],
   "source": [
    "# Define a simple feedforward neural network with Kaiming Initialization\n",
    "class KaimingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KaimingNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Apply Kaiming (He) initialization to the weights\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='linear')  # linear output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and train the model with Kaiming Initialization\n",
    "model_kaiming = KaimingNN()\n",
    "optimizer = optim.Adam(model_kaiming.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training model with Kaiming initialization:\")\n",
    "for epoch in range(10):\n",
    "    model_kaiming.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_kaiming(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b22dd-4db7-46a6-8bb6-5611a478ea8a",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **Kaiming initialization** (`nn.init.kaiming_uniform_`): Weights are initialized from a uniform distribution designed to avoid issues like vanishing or exploding gradients in networks with ReLU activations. Best suited for networks where the ReLU activation function (or its variants) is used, as it helps maintain the variance of gradients during backpropagation, thus preventing issues like vanishing gradients.\n",
    "\n",
    "## Learning rate scheduling\n",
    "Learning rate scheduling adjusts the learning rate during training, which can lead to better convergence. Common schedules include reducing the learning rate on a plateau or using cyclic learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0b9456-37d9-493a-b5d4-7c5eed516644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3112, Learning Rate: 0.010000\n",
      "Epoch 2, Loss: 0.1733, Learning Rate: 0.010000\n",
      "Epoch 3, Loss: 0.1172, Learning Rate: 0.010000\n",
      "Epoch 4, Loss: 0.0900, Learning Rate: 0.010000\n",
      "Epoch 5, Loss: 0.1065, Learning Rate: 0.001000\n",
      "Epoch 6, Loss: 0.1190, Learning Rate: 0.001000\n",
      "Epoch 7, Loss: 0.1176, Learning Rate: 0.001000\n",
      "Epoch 8, Loss: 0.1148, Learning Rate: 0.001000\n",
      "Epoch 9, Loss: 0.1113, Learning Rate: 0.001000\n",
      "Epoch 10, Loss: 0.1076, Learning Rate: 0.000100\n",
      "Epoch 11, Loss: 0.1040, Learning Rate: 0.000100\n",
      "Epoch 12, Loss: 0.1036, Learning Rate: 0.000100\n",
      "Epoch 13, Loss: 0.1033, Learning Rate: 0.000100\n",
      "Epoch 14, Loss: 0.1029, Learning Rate: 0.000100\n",
      "Epoch 15, Loss: 0.1025, Learning Rate: 0.000010\n",
      "Epoch 16, Loss: 0.1021, Learning Rate: 0.000010\n",
      "Epoch 17, Loss: 0.1021, Learning Rate: 0.000010\n",
      "Epoch 18, Loss: 0.1021, Learning Rate: 0.000010\n",
      "Epoch 19, Loss: 0.1020, Learning Rate: 0.000010\n",
      "Epoch 20, Loss: 0.1020, Learning Rate: 0.000001\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop with learning rate scheduling\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764f6aa-6f80-4fdf-95fa-6d2384d33afd",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **optimizer** (`optim.Adam`): An optimizer that implements the Adam algorithm.\n",
    "- **Learning rate scheduler** (`optim.lr_scheduler.StepLR`): A learning rate scheduler adjusts the learning rate during training. This can help the model converge more efficiently by using a higher learning rate in the early stages and reducing it as training progresses. `StepLR` is a specific type of learning rate scheduler that reduces the learning rate by a fixed factor (`gamma`) every few epochs (`step_size`). Here, the learning rate will be multiplied by 0.1 every 5 epochs.\n",
    "- **Updates the model parameters** (`optimizer.step()`): After computing the gradients of the loss with respect to the model parameters, `optimizer.step()` updates the parameters to minimize the loss. This is called in each iteration (or batch) of the training loop after `loss.backward()` has been called to compute the gradients.\n",
    "- **Updates the learning rate** (`scheduler.step()`): Updates the learning rate according to the schedule defined by the learning rate scheduler. It usually decreases the learning rate as training progresses. Typically called at the end of each epoch (or after a set number of iterations), depending on the type of scheduler.\n",
    "- **Last computed learning rate** (`scheduler.get_last_lr()`): Returns the most recent learning rate that was computed by the scheduler. It’s useful for monitoring how the learning rate changes over time during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
