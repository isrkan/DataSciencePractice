{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c064cf-d3d1-4220-8edf-f0b8015a1366",
   "metadata": {},
   "source": [
    "# Learning paradigms with PyTorch\n",
    "\n",
    "This notebook explores various learning paradigms in deep learning, implemented using PyTorch. Deep learning has evolved to include diverse techniques that extend beyond traditional supervised learning. These paradigms enable models to perform better on complex tasks, adapt to new tasks with limited data, and leverage shared knowledge across multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17771066-b12a-4978-ab3f-541816376d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601dc202-098b-4c69-83d0-aba271cde96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transfer learning\n",
    "Transfer learning is a machine learning technique where a model that has already been trained on a large dataset is reused or fine-tuned on a new, often smaller dataset. Instead of starting from scratch, transfer learning allows us to leverage the knowledge captured in a pre-trained model to improve the performance and efficiency of a new model. This approach is particularly valuable because training deep neural networks from scratch typically requires vast amounts of data and computational resources. Transfer learning allows us to start with a pre-trained model, reducing the time and data needed. Key concepts:\n",
    "- **Pre-trained model**: A neural network model that has already been trained on a large dataset.\n",
    "- **Fine-tuning**: Adjusting the weights of the pre-trained model to adapt it to a new dataset or task.\n",
    "\n",
    "We will start by training a model from scratch, saving its weights, and then using that model in various transfer learning scenarios.\n",
    "\n",
    "### Pre-trained model\n",
    "We will define a simple feedforward neural network model and train it on synthetic data. After training, we will save the model's state dictionary (its weights) so that we can use them later in different transfer learning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fb48f3-197a-4609-9346-28848d59f1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.692634105682373\n",
      "Epoch 2, Loss: 0.6919400095939636\n",
      "Epoch 3, Loss: 0.6915565729141235\n",
      "Epoch 4, Loss: 0.691327691078186\n",
      "Epoch 5, Loss: 0.6911556124687195\n",
      "Epoch 6, Loss: 0.6909562945365906\n",
      "Epoch 7, Loss: 0.6907436847686768\n",
      "Epoch 8, Loss: 0.6905004978179932\n",
      "Epoch 9, Loss: 0.6902583241462708\n",
      "Epoch 10, Loss: 0.6900127530097961\n",
      "Test Loss: 0.6926771402359009\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "class FFNNModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = FFNNModel(input_size=X_train.shape[1])\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test)\n",
    "    test_loss = criterion(test_output, y_test)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'pretrained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3974fde-a20b-4604-905c-69145bbe7c68",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "In this section, we created a simple FFNN and trained it on synthetic binary classification data. Here is a breakdown of the steps:\n",
    "\n",
    "- **Step 1**: Load/generate data for a related task - We generate synthetic data with 20 features and a binary target variable. The data is then split into training and testing sets.\n",
    "- **Step 2**: Model definition - We define a model class `FFNNModel` with three hidden layers and an output layer. The hidden layers use ReLU activation, and the output layer uses the sigmoid activation function for binary classification.\n",
    "- **Step 3**: Model training - The model is trained for 10 epochs using the Adam optimizer and binary cross-entropy loss function.\n",
    "- **Step 4**: Saving the model - The trained model's state dictionary is saved to a file, which will be used later for transfer learning.\n",
    "\n",
    "### Types of transfer learning\n",
    "\n",
    "Transfer learning can be applied in several ways, depending on how the pre-trained model is used and the nature of the new task. Let's explore different types of transfer learning techniques using the pre-trained model we just saved.\n",
    "\n",
    "#### Model as a fixed pre-trained model\n",
    "In this approach, we use the pre-trained model directly without any changes. This is typically done when the new task is very similar to the original task for which the model was trained. The pre-trained model's layers are kept unchanged, and the model is used as-is without further training to make predictions on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c2e8be-0beb-4d54-8e30-b889ac33e8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed pre-trained model test loss: 0.6959823369979858\n",
      "Predictions from fixed pre-trained model: tensor([0.5286, 0.5261, 0.5318, 0.5276, 0.5363])\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data\n",
    "X_fixed = np.random.rand(100, 20)\n",
    "y_fixed = np.random.randint(2, size=100)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_fixed = torch.tensor(X_fixed, dtype=torch.float32)\n",
    "y_fixed = torch.tensor(y_fixed, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Define a new model with the same architecture as the trained model\n",
    "class FFNNModelFixed(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNNModelFixed, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "model_fixed = FFNNModelFixed(input_size=X_fixed.shape[1])\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_fixed.load_state_dict(torch.load('pretrained_model.pth'))\n",
    "\n",
    "# Evaluate the model on new data\n",
    "model_fixed.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model_fixed(X_fixed)\n",
    "    test_loss = criterion(test_output, y_fixed)\n",
    "    print(f\"Fixed pre-trained model test loss: {test_loss.item()}\")\n",
    "\n",
    "# Use the pre-trained model directly for prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model_fixed(X_fixed)\n",
    "    print(f\"Predictions from fixed pre-trained model: {predictions[:5].squeeze()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181356f2-8cba-42d6-9484-a95f286118ea",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "Here, we use the previously trained model as-is, without any further training:\n",
    "\n",
    "- **Step 1**: Load the data for a similar task.\n",
    "- **Step 2**: Model definition: We define a new model with the same architecture as the pre-trained model to ensure compatibility with the saved weights.\n",
    "- **Step 3**: Load the pre-trained weights using `load_state_dict`.\n",
    "- **Step 4**: Evaluate the model on the new dataset without further training.\n",
    "- **Step 5**: Prediction - The pre-trained model is used to make predictions on the new data, demonstrating its ability to generalize to unseen data.\n",
    "\n",
    "#### Feature extraction transfer learning\n",
    "In this approach, we use the pre-trained model as a feature extractor. We freeze the lower layers (which capture general features) and add new layers on top to adapt to the new task. The output of the pre-trained model (before the final layer) is fed into a new model designed for the new task, allowing the model to learn task-specific features without retraining the entire network. This method is particularly useful when the new task is related to the original task but requires a different output or representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2225ab7-ae65-4ea3-b2f7-ddb13586a0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0973738431930542\n",
      "Epoch 2, Loss: 1.0973533391952515\n",
      "Epoch 3, Loss: 1.0973336696624756\n",
      "Epoch 4, Loss: 1.0973143577575684\n",
      "Epoch 5, Loss: 1.0972952842712402\n",
      "Feature extractor model test loss: 1.1009647846221924\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data (related but different task)\n",
    "np.random.seed(42)\n",
    "X_feature = np.random.rand(300, 20)\n",
    "y_feature = np.random.randint(3, size=300)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_feature = torch.tensor(X_feature, dtype=torch.float32)\n",
    "y_feature = torch.tensor(y_feature, dtype=torch.long)  # For multiclass classification\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_feature, X_test_feature, y_train_feature, y_test_feature = train_test_split(X_feature, y_feature, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define a new model with the same architecture as the trained model\n",
    "class FFNNModelFeature(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNNModelFeature, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "model_feature_extraction = FFNNModelFeature(input_size=X_feature.shape[1])\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_feature_extraction.load_state_dict(torch.load('pretrained_model.pth'))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for param in model_feature_extraction.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer for the new task\n",
    "model_feature_extraction.fc4 = nn.Linear(32, 3)  # Assuming 3 classes for the new task\n",
    "\n",
    "# Define a new loss function and optimizer\n",
    "criterion_new = nn.CrossEntropyLoss()\n",
    "optimizer_new = optim.Adam(model_feature_extraction.fc4.parameters(), lr=0.001)  # Only optimize the new layer\n",
    "\n",
    "# Train the new model\n",
    "for epoch in range(5):\n",
    "    model_feature_extraction.train()\n",
    "    optimizer_new.zero_grad()\n",
    "    output = model_feature_extraction(X_train_feature)\n",
    "    loss = criterion_new(output, y_train_feature)\n",
    "    loss.backward()\n",
    "    optimizer_new.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model_feature_extraction.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model_feature_extraction(X_test_feature)\n",
    "    test_loss = criterion_new(test_output, y_test_feature)\n",
    "    print(f\"Feature extractor model test loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761843b-7fbc-4f6e-abe4-f35da73eca69",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "In this approach, we use the pre-trained model as a feature extractor for a related but different task:\n",
    "\n",
    "- **Step 1**: Load the data for a related but different task.\n",
    "- **Step 2**: Load the pre-trained model and freeze its layers to retain the pre-trained features.\n",
    "- **Step 3**: Replace the last layer(s) of the pre-trained model with a new layer tailored to the new task (e.g., multiclass classification).\n",
    "- **Step 4**: Train only the new layer(s) on the new dataset.\n",
    "- **Step 5**: Evaluate the model on the new dataset.\n",
    "\n",
    "## Fine-tuning transfer learning\n",
    "\n",
    "Fine-tuning is a more flexible approach to transfer learning, where we start with a pre-trained model but allow some or all layers to be further trained on the new task. This approach allows the model to adapt more closely to the new task while retaining the knowledge learned from the pre-trained model in the original task. Fine-tuning is often used when the new task is sufficiently different from the original task, and the pre-trained model needs to be adjusted to better fit the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec4c4e5-4923-4538-93ce-0d0d6f4b091f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.7765142917633057\n",
      "Epoch 2, Loss: 2.7724404335021973\n",
      "Epoch 3, Loss: 2.7685468196868896\n",
      "Epoch 4, Loss: 2.764758586883545\n",
      "Epoch 5, Loss: 2.7609825134277344\n",
      "Fine-tuned model test loss: 2.7544312477111816\n",
      "Fine-tuned model accuracy: 0.3375000059604645\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data (related task)\n",
    "np.random.seed(42)\n",
    "X_finetune = np.random.rand(400, 20)\n",
    "y_finetune = np.random.randint(3, size=400)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_finetune = torch.tensor(X_finetune, dtype=torch.float32)\n",
    "y_finetune = torch.tensor(y_finetune, dtype=torch.long)  # Use long type for classification\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_finetune, X_test_finetune, y_train_finetune, y_test_finetune = train_test_split(X_finetune, y_finetune, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a new model with the same architecture as the trained model\n",
    "class PretrainedModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PretrainedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "pretrained_model_finetune = PretrainedModel(input_size=X_finetune.shape[1])\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model_finetune.load_state_dict(torch.load('pretrained_model.pth'))\n",
    "\n",
    "# Freeze some layers in the pre-trained model (e.g., first two layers)\n",
    "for param in list(pretrained_model_finetune.parameters())[:2]:  # Assuming the first two layers\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the model: Add a new dense layer and the new output layer for the new task\n",
    "pretrained_model_finetune.fc4 = nn.Linear(32, 16)  # New dense layer with 16 units\n",
    "pretrained_model_finetune.fc5 = nn.Linear(16, 3)   # New output layer for the 3-class classification\n",
    "\n",
    "# Define a new loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, pretrained_model_finetune.parameters()), lr=0.001)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(5):\n",
    "    pretrained_model_finetune.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = pretrained_model_finetune(X_train_finetune)\n",
    "    loss = criterion(output, y_train_finetune)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "pretrained_model_finetune.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = pretrained_model_finetune(X_test_finetune)\n",
    "    test_loss = criterion(test_output, y_test_finetune)\n",
    "    print(f\"Fine-tuned model test loss: {test_loss.item()}\")\n",
    "    _, predicted = torch.max(test_output, 1)\n",
    "    accuracy = (predicted == y_test_finetune).float().mean().item()\n",
    "    print(f\"Fine-tuned model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600f4d4-d0be-411b-92d8-45f302d63f76",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **Step 1**: Generate or load data for a related task and split it into training and testing sets.\n",
    "- **Step 2**: Define and load the pre-trained model.\n",
    "- **Step 3**: Freeze the first layer to retain the pre-trained features.  \n",
    "- **Step 4**: Modify the architecture if needed, especially the output layer to match the new task's requirements. We can also add new layers before the final output layer to allow the model to learn more complex features specific to the new task.\n",
    "- **Step 5**: Fine-tune the model by training only the unfrozen layers on the new dataset.\n",
    "- **Step 6**: Evaluate the fine-tuned model on the test set.\n",
    "\n",
    "## Knowledge distillation (Teacher-student model)\n",
    "\n",
    "Knowledge distillation transfers knowledge from a large, pre-trained model (the teacher) to a smaller and simpler model (the student). The idea is that the student model learns to mimic the teacher's behavior, achieving similar performance with fewer parameters, which makes it more efficient for deployment on devices with limited resources.\n",
    "\n",
    "### Response-based knowledge distillation\n",
    "\n",
    "Response-based knowledge distillation focuses on the output predictions (responses) of the teacher model. The student model is trained to mimic the probability distribution (soft labels) produced by the teacher model rather than the hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1441f534-5676-496d-b5aa-fb9323861766",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model - Epoch 1, BCE Loss: 0.6930196285247803\n",
      "Teacher Model - Epoch 2, BCE Loss: 0.6917576789855957\n",
      "Teacher Model - Epoch 3, BCE Loss: 0.6908623576164246\n",
      "Teacher Model - Epoch 4, BCE Loss: 0.6902269124984741\n",
      "Teacher Model - Epoch 5, BCE Loss: 0.6897659301757812\n",
      "Teacher Model - Epoch 6, BCE Loss: 0.6894090175628662\n",
      "Teacher Model - Epoch 7, BCE Loss: 0.6891055107116699\n",
      "Teacher Model - Epoch 8, BCE Loss: 0.6888608336448669\n",
      "Teacher Model - Epoch 9, BCE Loss: 0.6886587738990784\n",
      "Teacher Model - Epoch 10, BCE Loss: 0.6884830594062805\n",
      "Response-based teacher model test loss: 0.7004870772361755\n",
      "Response-based teacher model accuracy: 0.48500001430511475\n",
      "Student Model - Epoch 1, KLD Loss: -0.6050364375114441\n",
      "Student Model - Epoch 2, KLD Loss: -0.6076161861419678\n",
      "Student Model - Epoch 3, KLD Loss: -0.610180139541626\n",
      "Student Model - Epoch 4, KLD Loss: -0.6127474308013916\n",
      "Student Model - Epoch 5, KLD Loss: -0.6153145432472229\n",
      "Student Model - Epoch 6, KLD Loss: -0.6179109215736389\n",
      "Student Model - Epoch 7, KLD Loss: -0.6205543875694275\n",
      "Student Model - Epoch 8, KLD Loss: -0.6232771277427673\n",
      "Student Model - Epoch 9, KLD Loss: -0.6261118054389954\n",
      "Student Model - Epoch 10, KLD Loss: -0.6291106939315796\n",
      "Response-based student model test loss: 0.7030095458030701\n",
      "Response-based student model accuracy: 0.48500001430511475\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Teacher model\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate and train the teacher model\n",
    "teacher_model = TeacherModel(input_size=X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    teacher_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = teacher_model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Teacher Model - Epoch {epoch+1}, BCE Loss: {loss.item()}\")\n",
    "\n",
    "# Generate soft labels (probability distribution) from the Teacher model\n",
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    teacher_logits = teacher_model(X_train)\n",
    "    # In our case of binary classification problem we don't need to apply softmax. The output of our teacher model already represents a probability distribution\n",
    "    #teacher_soft_labels = F.softmax(teacher_logits, dim=1)  # Using temperature scaling\n",
    "    teacher_soft_labels = teacher_logits\n",
    "    # Evaluate the teacher model\n",
    "    teacher_output = teacher_model(X_test)\n",
    "    teacher_loss = criterion(teacher_output, y_test)\n",
    "    print(f\"Response-based teacher model test loss: {teacher_loss.item()}\")\n",
    "    accuracy = ((teacher_output > 0.5).float() == y_test).float().mean().item()\n",
    "    print(f\"Response-based teacher model accuracy: {accuracy}\")\n",
    "\n",
    "# Define the Student model\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the Student model\n",
    "student_model = StudentModel(input_size=X_train.shape[1])\n",
    "\n",
    "# Compile the Student model using KLD loss\n",
    "criterion_kld = nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the Student model on the soft labels\n",
    "for epoch in range(10):\n",
    "    student_model.train()\n",
    "    optimizer_student.zero_grad()\n",
    "    student_output = student_model(X_train)\n",
    "    loss = criterion_kld(student_output, teacher_soft_labels)\n",
    "    loss.backward()\n",
    "    optimizer_student.step()\n",
    "    print(f\"Student Model - Epoch {epoch+1}, KLD Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the Student model\n",
    "student_model.eval()\n",
    "with torch.no_grad():\n",
    "    student_output = student_model(X_test)\n",
    "    student_loss = criterion(student_output, y_test)\n",
    "    print(f\"Response-based student model test loss: {student_loss.item()}\")\n",
    "    accuracy = ((student_output > 0.5).float() == y_test).float().mean().item()\n",
    "    print(f\"Response-based student model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afcf884-93e3-4e60-9e97-c30fbd467d9b",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **Step 1**: Define and train the teacher model on the original task if not pre-trained.\n",
    "- **Step 2**: Generate soft labels (probability distribution) using the trained teacher model. Since our problem is binary, we don't need to apply softmax. The output of your teacher model already represents a probability distribution\n",
    "- **Step 3**: Define and train the student model, which is a smaller network.\n",
    "- **Step 4**: Train the student model using the soft labels from the teacher model with Kullback-Leibler divergence loss to minimize the difference between the student's output and the teacher's soft labels.\n",
    "- **Step 4**: Evaluate the student model on the test data.\n",
    "\n",
    "## Multi-task learning (MTL)\n",
    "\n",
    "Multi-task learning (MTL) is a technique where a single model is trained to perform multiple tasks simultaneously. Instead of training separate models for each task, MTL uses a unified architecture to learn from related tasks concurrently. The core idea behind MTL is that by learning multiple tasks together, the model can exploit commonalities and shared patterns across tasks, leading to improved generalization and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07267821-3698-47b3-88f8-3b3dbb2d92d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classification Loss: 0.6874178051948547, Regression Loss: 0.09434368461370468\n",
      "Epoch 2, Classification Loss: 0.6925233602523804, Regression Loss: 0.11183096468448639\n",
      "Epoch 3, Classification Loss: 0.6853673458099365, Regression Loss: 0.112230584025383\n",
      "Epoch 4, Classification Loss: 0.6908676624298096, Regression Loss: 0.08216945081949234\n",
      "Epoch 5, Classification Loss: 0.7056833505630493, Regression Loss: 0.06934796273708344\n",
      "Epoch 6, Classification Loss: 0.6926990747451782, Regression Loss: 0.10169712454080582\n",
      "Epoch 7, Classification Loss: 0.6919596791267395, Regression Loss: 0.08397369086742401\n",
      "Epoch 8, Classification Loss: 0.6817010045051575, Regression Loss: 0.09836194664239883\n",
      "Epoch 9, Classification Loss: 0.6826958656311035, Regression Loss: 0.08766327798366547\n",
      "Epoch 10, Classification Loss: 0.7005208730697632, Regression Loss: 0.07402367144823074\n",
      "Test Classification Accuracy: 0.5149999856948853\n",
      "Test Regression MSE: 0.08783645927906036\n",
      "Classification Predictions (first 5): tensor([[0.5321],\n",
      "        [0.5110],\n",
      "        [0.5382],\n",
      "        [0.5435],\n",
      "        [0.5443]])\n",
      "Regression Predictions (first 5): tensor([[0.4854],\n",
      "        [0.5761],\n",
      "        [0.5752],\n",
      "        [0.4885],\n",
      "        [0.5186]])\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_mtl = np.random.rand(1000, 20)\n",
    "\n",
    "# Task 1: Binary classification labels\n",
    "y_classification = np.random.randint(2, size=1000)\n",
    "\n",
    "# Task 2: Regression labels\n",
    "y_regression = np.random.rand(1000)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_mtl = torch.tensor(X_mtl, dtype=torch.float32)\n",
    "y_classification = torch.tensor(y_classification, dtype=torch.float32).unsqueeze(1)\n",
    "y_regression = torch.tensor(y_regression, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_mtl, X_test_mtl, y_train_class, y_test_class = train_test_split(X_mtl, y_classification, test_size=0.2, random_state=42)\n",
    "_, _, y_train_reg, y_test_reg = train_test_split(X_mtl, y_regression, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_mtl, y_train_class, y_train_reg)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the MTL model\n",
    "class MTLModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MTLModel, self).__init__()\n",
    "        # Shared layers\n",
    "        self.shared_fc1 = nn.Linear(input_size, 64)\n",
    "        self.shared_fc2 = nn.Linear(64, 32)\n",
    "        \n",
    "        # Task 1: Classification\n",
    "        self.classification_fc = nn.Linear(32, 1)\n",
    "        \n",
    "        # Task 2: Regression\n",
    "        self.regression_fc = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared = self.relu(self.shared_fc1(x))\n",
    "        shared = self.relu(self.shared_fc2(shared))\n",
    "        \n",
    "        # Task 1: Classification\n",
    "        classification_output = self.sigmoid(self.classification_fc(shared))\n",
    "        \n",
    "        # Task 2: Regression\n",
    "        regression_output = self.regression_fc(shared)\n",
    "        \n",
    "        return classification_output, regression_output\n",
    "\n",
    "# Initialize model, loss functions, and optimizer\n",
    "mtl_model = MTLModel(input_size=X_train_mtl.shape[1])\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_reg = nn.MSELoss()\n",
    "optimizer_mtl = optim.Adam(mtl_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    mtl_model.train()\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_class_batch, y_reg_batch = batch\n",
    "        \n",
    "        optimizer_mtl.zero_grad()\n",
    "        class_pred, reg_pred = mtl_model(X_batch)\n",
    "        \n",
    "        loss_class = criterion_class(class_pred, y_class_batch)\n",
    "        loss_reg = criterion_reg(reg_pred, y_reg_batch)\n",
    "        \n",
    "        loss = loss_class + loss_reg\n",
    "        loss.backward()\n",
    "        optimizer_mtl.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Classification Loss: {loss_class.item()}, Regression Loss: {loss_reg.item()}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mtl_model.eval()\n",
    "with torch.no_grad():\n",
    "    class_pred, reg_pred = mtl_model(X_test_mtl)\n",
    "    \n",
    "    test_class_loss = criterion_class(class_pred, y_test_class)\n",
    "    test_reg_loss = criterion_reg(reg_pred, y_test_reg)\n",
    "    \n",
    "    test_class_accuracy = (class_pred.round() == y_test_class).float().mean()\n",
    "    \n",
    "    print(f\"Test Classification Accuracy: {test_class_accuracy.item()}\")\n",
    "    print(f\"Test Regression MSE: {test_reg_loss.item()}\")\n",
    "\n",
    "# Generate predictions for both tasks on the test data\n",
    "classification_predictions = class_pred[:5]  # Predictions for the classification task\n",
    "regression_predictions = reg_pred[:5]  # Predictions for the regression task\n",
    "print(\"Classification Predictions (first 5):\", classification_predictions)\n",
    "print(\"Regression Predictions (first 5):\", regression_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c5a96-dde5-43ec-bec9-ff3e3559287c",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- **Step 1**: Generate data for multiple tasks (binary classification and regression) and split the data into training and testing sets.\n",
    "- **Step 2**: Define the MTL model with shared layers and task-specific outputs.\n",
    "- **Step 3**: Compile the model with separate loss functions for each task (binary cross-entropy for classification and MSE for regression).\n",
    "- **Step 4**: Train the model using combined losses.\n",
    "- **Step 5**: Evaluate the model on the test set for both tasks and generate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
