{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9d0d47-8a72-4981-9ff6-8c0377df25ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Additional techniques for building neural networks with TensorFlow\n",
    "\n",
    "This notebook explores additional techniques to enhance neural networks' performance using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f5ea2f-d352-484e-8a7e-56a4e8c6e729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LayerNormalization, GaussianNoise\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeUniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, TensorBoard\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabea669-0d1f-41aa-88a5-8dbd87570d0f",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "Regularization is a technique to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on new, unseen data. By introducing a penalty on the complexity of the model, regularization encourages simpler models that generalize better to new data. Regularization adds a penalty to the loss function of a neural network. This penalty discourages the network from assigning too much importance to any single weight, which can help in generalizing the model to unseen data.\n",
    "\n",
    "During training, the loss function of the model is modified to include the regularization penalty. The goal is to minimize both the error on the training data and the regularization penalty. This helps the model to not only fit the training data but also to maintain a simpler form that is less likely to overfit.\n",
    "\n",
    "#### L1 and L2 regularization\n",
    "- **L1 regularization** adds a penalty equal to the absolute value of the magnitude of coefficients. It can drive some weights to zero, effectively performing feature selection. This means L1 regularization can make the model sparse and reduce the complexity of the model. Mathematically: for a weight $w$, the penalty is proportional to $|w|$.\n",
    "- **L2 regularization** adds a penalty equal to the square of the magnitude of coefficients. It tends to shrink the weights evenly without driving them to zero, which helps in stabilizing the model. Mathematically: for a weight $w$, the penalty is proportional to $w^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8e4196-71f5-489e-855e-52e68e30dd99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with L1 regularization:\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 85ms/step - loss: 4.0696 - val_loss: 3.8612\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.7931 - val_loss: 3.6279\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.6042 - val_loss: 3.4720\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.4747 - val_loss: 3.3818\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 3.4045 - val_loss: 3.3324\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 3.3580 - val_loss: 3.2904\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.3111 - val_loss: 3.2359\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.2521 - val_loss: 3.1714\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.1858 - val_loss: 3.1059\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 3.1194 - val_loss: 3.0454\n",
      "\n",
      "Training model with L2 regularization:\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.8171 - val_loss: 0.7003\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7519 - val_loss: 0.6576\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7256 - val_loss: 0.6388\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7040 - val_loss: 0.6238\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6804 - val_loss: 0.6112\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6610 - val_loss: 0.6030\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6438 - val_loss: 0.5949\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6276 - val_loss: 0.5820\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6105 - val_loss: 0.5692\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5955 - val_loss: 0.5560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355784a160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.rand(100, 1)\n",
    "\n",
    "# Build a simple FFNN with L1 and L2 regularization\n",
    "\n",
    "# Model with L1 regularization\n",
    "model_l1 = Sequential()\n",
    "model_l1.add(Dense(64, activation='relu', kernel_regularizer=l1(0.01), input_shape=(10,)))\n",
    "model_l1.add(Dense(32, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "model_l1.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_l1.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "print(\"Training model with L1 regularization:\")\n",
    "model_l1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Model with L2 regularization\n",
    "model_l2 = Sequential()\n",
    "model_l2.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_shape=(10,)))\n",
    "model_l2.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model_l2.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_l2.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "print(\"\\nTraining model with L2 regularization:\")\n",
    "model_l2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6698f-41ad-4d76-9769-323c7c0893af",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- L1 regularization: (`kernel_regularizer=l1(0.01)`) - Adds L1 regularization with a penalty of `0.01`.\n",
    "- L2 regularization: (`kernel_regularizer=l2(0.01)`) - Adds L2 regularization with a penalty of `0.01`.\n",
    "\n",
    "#### Dropout\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting by randomly setting a fraction of input units to zero during training. By doing this, dropout forces the network to be more robust and prevents it from becoming overly dependent on any particular feature. This technique helps in making the model generalize better to unseen data.\n",
    "\n",
    "During each training iteration, dropout randomly \"drops out\" (i.e., sets to zero) a specified percentage of neurons in a layer. This means that at each update, only a subset of neurons is used to compute the forward pass and backpropagation. By dropping out neurons, the network is forced to learn redundant representations, making it more resilient and less likely to overfit. The stochastic behavior of dropout ensures that different neurons are dropped each time, which effectively creates an ensemble of networks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9aef81-ffec-40a9-ba66-6019ee1e0edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 106ms/step - loss: 0.5354 - val_loss: 0.2850\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3727 - val_loss: 0.1877\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2707 - val_loss: 0.1251\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2084 - val_loss: 0.0927\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1581 - val_loss: 0.0778\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2216 - val_loss: 0.0723\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1456 - val_loss: 0.0705\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1541 - val_loss: 0.0693\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1694 - val_loss: 0.0681\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1632 - val_loss: 0.0669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13559c768b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN with dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1799-19f5-446d-9d60-700c4f50f6db",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `Dropout(0.5)`: Sets 50% of the input units to 0 at each update during training. Similarly with `Dropout(0.2)`\n",
    "\n",
    "#### Layer normalization\n",
    "Layer normalization normalizes the inputs to a layer, which helps in stabilizing and accelerating the training process. Unlike batch normalization, which normalizes across the batch, layer normalization normalizes across the features for each training example independently. Layer normalization computes the mean and variance for each feature vector, rather than across the batch. It then uses these statistics to normalize the feature vector for each individual sample. It applies a linear transformation to the normalized values to restore the representational capacity of the network.\n",
    "\n",
    "In simple words, let's say we have a layer in a neural network with many neurons. Each neuron processes a different feature of the input data. As the data passes through this layer, the values can vary a lot from one feature to another. This variation can make training unstable and slow. Layer normalization helps to fix this problem by ensuring that the activations (outputs) of a layer are standardized. Specifically, it adjusts the activations so that they have a mean of zero and a variance of one for each individual sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b33ef6b-22c9-4eb0-a1c2-72241e75616c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 142ms/step - loss: 0.6163 - val_loss: 0.3549\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4313 - val_loss: 0.1675\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2130 - val_loss: 0.2405\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2238 - val_loss: 0.2253\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1689 - val_loss: 0.1395\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1283 - val_loss: 0.1400\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1458 - val_loss: 0.1346\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1180 - val_loss: 0.1385\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1004 - val_loss: 0.1538\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1017 - val_loss: 0.1501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355ae0f880>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN with layer normalization\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f59e7-6eff-4d86-93af-8e44b9a5cfec",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `LayerNormalization()`: Applies layer normalization to the inputs of the layer.\n",
    "\n",
    "#### Gaussian noise\n",
    "Gaussian noise adds random noise to the inputs or weights during training, which can help the model become more robust by preventing it from learning spurious patterns specific to the training data. It is applied by adding a random Gaussian distribution (mean = 0, variance = specified) to the inputs of a layer. The noise helps in regularizing the network by making it less sensitive to the exact values of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187229d4-5191-4c6a-bc67-2cf0a540abbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 159ms/step - loss: 0.2355 - val_loss: 0.0778\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1664 - val_loss: 0.0777\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1780 - val_loss: 0.0815\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1948 - val_loss: 0.0828\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2147 - val_loss: 0.0770\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1850 - val_loss: 0.0712\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1390 - val_loss: 0.0713\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1772 - val_loss: 0.0719\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1404 - val_loss: 0.0722\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1632 - val_loss: 0.0718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355c0413d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN with gaussian noise\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(GaussianNoise(stddev=0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(GaussianNoise(stddev=0.2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed600a-d236-4343-a118-9a1468f93e9d",
   "metadata": {},
   "source": [
    "***Explanation:***\n",
    "- `GaussianNoise(stddev=0.1)`: Adds Gaussian noise with a standard deviation of 0.1 to the inputs of the layer. The added noise forces the network to become more resilient to minor variations in the input data.\n",
    "\n",
    "## Weight initialization strategies\n",
    "Weight initialization sets the initial values of the weights of the network's layers before training begins. Proper initialization can significantly affect the speed of convergence and the success of the training process. Proper weight initialization helps in avoiding problems like vanishing or exploding gradients, which can make training inefficient or even impossible, especially in deep networks.\n",
    "\n",
    "***Glorot (Xavier) initialization***\n",
    "Glorot initialization is suitable for layers with a linear or tanh activation function. It designed to keep the scale of the gradients roughly the same in all layers. Weights are initialized from a distribution with zero mean and a variance of 2/(input units + output units).\n",
    "\n",
    "***He initialization***\n",
    "He initialization is suitable for layers with ReLU or its variants as activation functions. It helps mitigate the vanishing gradient problem in networks using ReLU activations. Weights are initialized from a distribution with zero mean and a variance of 2/(input units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96fab9b-4c52-4de8-ab08-e6a6de0c3de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with Glorot initialization:\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 111ms/step - loss: 0.3158 - val_loss: 0.2242\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2120 - val_loss: 0.1380\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1457 - val_loss: 0.0859\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1055 - val_loss: 0.0641\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0957 - val_loss: 0.0615\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0975 - val_loss: 0.0657\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1015 - val_loss: 0.0681\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0996 - val_loss: 0.0662\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0932 - val_loss: 0.0638\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0869 - val_loss: 0.0630\n",
      "\n",
      "Training model with He initialization:\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 107ms/step - loss: 0.6643 - val_loss: 0.5379\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3810 - val_loss: 0.3668\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2489 - val_loss: 0.3077\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2219 - val_loss: 0.3101\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2261 - val_loss: 0.3192\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2219 - val_loss: 0.3077\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2012 - val_loss: 0.2787\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1768 - val_loss: 0.2476\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1522 - val_loss: 0.2270\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1396 - val_loss: 0.2161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355ae0d520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN with Glorot Initialization\n",
    "model_glorot = Sequential()\n",
    "model_glorot.add(Dense(64, activation='relu', kernel_initializer=GlorotUniform(), input_shape=(10,)))\n",
    "model_glorot.add(Dense(32, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "model_glorot.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_glorot.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "print(\"Training model with Glorot initialization:\")\n",
    "model_glorot.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Build a simple FFNN with He Initialization\n",
    "model_he = Sequential()\n",
    "model_he.add(Dense(64, activation='relu', kernel_initializer=HeUniform(), input_shape=(10,)))\n",
    "model_he.add(Dense(32, activation='relu', kernel_initializer=HeUniform()))\n",
    "model_he.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_he.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "print(\"\\nTraining model with He initialization:\")\n",
    "model_he.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ceb40-46d9-4d03-a08e-174d31ae3ca5",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "- Glorot initialization (`kernel_initializer=GlorotUniform()`): Initializes the weights with a Glorot (Xavier) uniform distribution.\n",
    "- He initialization (`kernel_initializer=HeUniform()`): Initializes the weights with a He uniform distribution.\n",
    "\n",
    "## Batch normalization\n",
    "Batch normalization is a technique used to improve the training of deep neural networks. It helps to stabilize the learning process and significantly reduces the number of training epochs required to train deep networks. By normalizing the input of each layer, batch normalization helps in reducing the sensitivity to the scale of parameters or initialization, thereby stabilizing the learning process. It reduces the internal covariate shift by normalizing the layer inputs, making training faster and more stable. Batch normalization has a slight regularizing effect, which can sometimes eliminate the need for Dropout. With batch normalization, we can often use a higher learning rate, which can speed up the training process.\n",
    "\n",
    "Batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. It then applies a learned linear transformation. This transformation allows the network to undo any normalization if it wants to, thus preserving the representational capacity of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a98695-f9de-4ebf-9e0c-da3497d8dbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 118ms/step - loss: 1.8738 - val_loss: 0.3665\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.0702 - val_loss: 0.2766\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6736 - val_loss: 0.2060\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5187 - val_loss: 0.1557\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3984 - val_loss: 0.1220\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.3682 - val_loss: 0.1023\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3770 - val_loss: 0.0916\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2878 - val_loss: 0.0850\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2896 - val_loss: 0.0822\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1784 - val_loss: 0.0804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355e3ed4f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN with Batch Normalization\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e359f8-2bfc-4cb5-9add-20ad9691a17c",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "- Batch normalization layer (`BatchNormalization()`) - is added after the Dense layer to normalize the outputs of the previous layer. This layer normalizes each batch's activations and rescales them using two learnable parameters: a scale factor and a shift value.\n",
    "\n",
    "## Callbacks\n",
    "Callbacks in TensorFlow allow you to intervene during the training process of a neural network. They provide a way to monitor the training and validation metrics, alter the learning rate, save the model at specific intervals, and even stop the training process based on certain conditions. These automated processes help improve model performance, make training more efficient, and prevent overfitting. Callbacks are functions or objects that are called at specific points during the training process. They allow us to perform actions at the beginning or end of each epoch, batch, or even when the training process starts or ends.\n",
    "\n",
    "Let's explore some of the built-in callbacks provided by TensorFlow:\n",
    "\n",
    "#### Model checkpoint\n",
    "The `ModelCheckpoint` callback is used to save the model or model weights at specific intervals during training. This is useful for long training sessions where you want to ensure that you don't lose progress if training is interrupted. It saves the model at the end of an epoch if the monitored metric (e.g., validation loss) has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe02cec5-d4e4-4d7e-a348-9d4e3e82d6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 141ms/step - loss: 0.8116 - val_loss: 0.7894\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.6437 - val_loss: 0.6073\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.5016 - val_loss: 0.4520\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.3816 - val_loss: 0.3239\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2822 - val_loss: 0.2261\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.2083 - val_loss: 0.1550\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1643 - val_loss: 0.1080\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.1361 - val_loss: 0.0825\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1260 - val_loss: 0.0729\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.1253 - val_loss: 0.0717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1355e605ee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Path to save the model file\n",
    "    save_best_only=True,       # Save only the best model based on monitored metric\n",
    "    monitor='val_loss',        # Metric to monitor\n",
    "    mode='min'                 # Save the model when val_loss is minimized\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270443dd-263b-4a2e-a8c7-d2e934f4f25c",
   "metadata": {
    "tags": []
   },
   "source": [
    "When using the ModelCheckpoint callback in TensorFlow, you have several options for saving the model file:\n",
    "- HDF5 format - `filepath='best_model.h5'`\n",
    "- SavedModel format -  `filepath='saved_model/'`\n",
    "\n",
    "#### Early stopping\n",
    "`EarlyStopping` stops the training when the monitored metric has stopped improving. This helps prevent overfitting by stopping training once performance plateaus. It monitors a metric and stops training if no improvement is seen for a specified number of epochs (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f57b9f-700c-4722-9c50-b51ab48c7e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 99ms/step - loss: 0.2522 - val_loss: 0.2364\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1904 - val_loss: 0.1738\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1460 - val_loss: 0.1280\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1147 - val_loss: 0.1007\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0967 - val_loss: 0.0907\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0934 - val_loss: 0.0917\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0941 - val_loss: 0.0948\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0966 - val_loss: 0.0960\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0958 - val_loss: 0.0940\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0925 - val_loss: 0.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13560796490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min'           # Stop when the monitored metric is minimized (can be min or max)\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb606-fcde-4b4f-a7a9-196d11fd4cd0",
   "metadata": {},
   "source": [
    "#### Learning rate scheduler\n",
    "`LearningRateScheduler` dynamically adjusts the learning rate according to a specified schedule or function. It applies a user-defined function to update the learning rate at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b369254c-57ec-47bf-98ea-592e8471f3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 104ms/step - loss: 0.1719 - val_loss: 0.1287 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1294 - val_loss: 0.1028 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1138 - val_loss: 0.1001 - lr: 5.0000e-04\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1126 - val_loss: 0.0994 - lr: 5.0000e-04\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1109 - val_loss: 0.0989 - lr: 2.5000e-04\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1098 - val_loss: 0.0980 - lr: 2.5000e-04\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1088 - val_loss: 0.0975 - lr: 1.2500e-04\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1081 - val_loss: 0.0970 - lr: 1.2500e-04\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1073 - val_loss: 0.0968 - lr: 6.2500e-05\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1070 - val_loss: 0.0966 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135608b1c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define a function to update the learning rate\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 2 == 0 and epoch:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "# Define the LearningRateScheduler callback\n",
    "lr_scheduler_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[lr_scheduler_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379f3e3-e727-453e-b659-820e6b221bc2",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- `scheduler`: A function that takes the epoch index and the current learning rate and returns the updated learning rate.\n",
    "- Learning rate is halved every 2 epochs in this example.\n",
    "\n",
    "#### Reduce learning rate on plateau\n",
    "`ReduceLROnPlateau` reduces the learning rate when a metric has stopped improving. This can help in fine-tuning the model when progress slows. It monitors a metric and reduces the learning rate by a factor if no improvement is seen for a set number of epochs. `ReduceLROnPlateau` adjusts the learning rate to improve training, while `EarlyStopping` stops training to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146381e2-1347-48e7-af7a-2c40d699791d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 94ms/step - loss: 0.2572 - val_loss: 0.1874 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1818 - val_loss: 0.1239 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1358 - val_loss: 0.0946 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1110 - val_loss: 0.0910 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1075 - val_loss: 0.0966 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1127 - val_loss: 0.1003 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1141 - val_loss: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1136 - val_loss: 0.0992 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1129 - val_loss: 0.0991 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1128 - val_loss: 0.0990 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13561a5a340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define the ReduceLROnPlateau callback\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.1,          # Factor by which the learning rate will be reduced\n",
    "    patience=2,          # Number of epochs with no improvement before reducing the learning rate\n",
    "    mode='min'           # Reduce when the monitored metric is minimized\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18fcdf-5f65-40e9-8b6f-e7f205b33e91",
   "metadata": {},
   "source": [
    "#### Tensor board\n",
    "Tensor board is a tool for visualizing metrics during training. It provides a suite of visualizations to help understand the model's performance. It logs data during training, which can then be visualized in the TensorBoard UI and can assist in debugging and optimizing our models.\n",
    "\n",
    "TensorBoard is a web-based dashboard that allows us to visualize and analyze different aspects of our model's training process. It provides tools to:\n",
    "- Visualize metrics like loss and accuracy over epochs.\n",
    "- Track and visualize training and validation metrics.\n",
    "- Display model graphs and architecture.\n",
    "- Inspect and compare histograms and distributions of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7706986d-8df5-4e03-b5bb-669641a393d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 217ms/step - loss: 0.2616 - val_loss: 0.1452\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1431 - val_loss: 0.0721\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.0997 - val_loss: 0.0629\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1026 - val_loss: 0.0705\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1131 - val_loss: 0.0709\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.1076 - val_loss: 0.0629\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.0951 - val_loss: 0.0584\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.0866 - val_loss: 0.0600\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0854 - val_loss: 0.0642\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0845 - val_loss: 0.0648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13561bc9f70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple FFNN\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define the TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir='logs',          # Directory where the logs will be saved\n",
    "    histogram_freq=1,        # Compute histograms every epoch\n",
    "    write_graph=True,        # Visualize the model graph\n",
    "    write_images=True,       # Write activation images\n",
    "    update_freq='epoch'      # Update metrics at the end of each epoch\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad359420-07e2-4928-b520-ef4b2539008c",
   "metadata": {},
   "source": [
    "***Parameters of `TensorBoard` callback***\n",
    "1. `log_dir` - The directory where TensorBoard will save the logs. It specifies a directory path where log files will be written. TensorBoard will read from this directory to display the metrics.\n",
    "2. `histogram_freq` - Frequency (in epochs) at which histograms of weights and biases will be computed. Set this to `1` to compute histograms at every epoch or `0` to disable histogram computation. This is useful for analyzing how weights change during training.\n",
    "3. `write_graph` - Whether to visualize the model graph in TensorBoard. Set to `True` to write the model graph to the log directory, allowing us to visualize the model architecture.\n",
    "4. `write_images` - Whether to write model activation images to the log directory. Set to `True` to log images of weights and activations. This can help in understanding how our model's features are evolving.\n",
    "5. `update_freq` - Frequency (in steps) at which the metrics will be updated. Set to `'batch'` to update after each batch or `'epoch'` to update after each epoch. This controls the granularity of logging.\n",
    "\n",
    "##### Accessing TensorBoard UI\n",
    "To access TensorBoard and visualize the logged data, follow these steps:\n",
    "\n",
    "1. **Start TensorBoard:**\n",
    "   - Open a terminal or command prompt.\n",
    "   - Run the following command:\n",
    "     ```bash\n",
    "     tensorboard --logdir=logs\n",
    "     ```\n",
    "   - This command starts TensorBoard and points it to the `logs` directory where our training logs are saved.\n",
    "2. **Open TensorBoard in a browser:**\n",
    "   - Open a web browser and go to the URL to access the TensorBoard dashboard. We will see a message indicating the URL where TensorBoard is running (usually `http://localhost:6006`).\n",
    "3. **Explore TensorBoard UI:**\n",
    "   - The TensorBoard interface provides various tabs such as Scalars, Graphs, Distributions, and Histograms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
