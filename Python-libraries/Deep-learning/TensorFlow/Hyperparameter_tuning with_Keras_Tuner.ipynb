{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9105160-9640-405a-9c6a-075ae5741249",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with TensorFlow Keras Tuner\n",
    "\n",
    "In neural networks, hyperparameters are the configuration settings used to control the training process. Unlike model parameters (such as weights and biases), hyperparameters are set before training begins. They include settings such as learning rate, number of layers and neurons, batch size, epochs and more.\n",
    "\n",
    "Keras Tuner is a library that helps to find the optimal set of hyperparameters for our model. It automates the process of searching through hyperparameter space and can find the best hyperparameters efficiently.\n",
    "\n",
    "**Here is how it works:**\n",
    "1. Define a hypermodel and the search space: Create a model-building function that specifies which hyperparameters to tune and their possible values. This function should take hyperparameters as input and return a compiled model.\n",
    "2. Choose a tuning strategy and initialize the tuner: Keras Tuner provides several search algorithms to explore the hyperparameter space efficiently.\n",
    "3. Perform the search: The tuner evaluates multiple models with different hyperparameter settings and finds the best combination.\n",
    "4. Retrieve the best model: Once the tuning is complete, we can retrieve the model with the optimal hyperparameters.\n",
    "\n",
    "**Types of tuners:**\n",
    "1. Random search: This tuner randomly samples the hyperparameter space and builds the model for each sample. It is useful for exploring a wide hyperparameter space quickly.\n",
    "2. Bayesian optimization: This tuner uses a probabilistic model to choose hyperparameters, aiming to find the best values in fewer iterations compared to a random search.\n",
    "3. Hyperband: This tuner uses a bandit-based approach to tune hyperparameters. It explores a large hyperparameter space and can be more efficient by stopping poorly performing trials early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2029c67c-4857-4343-bec9-8ba1b28d160c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras_tuner import RandomSearch, GridSearch, BayesianOptimization, Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250c21b1-1077-49e4-ab51-4af39c1c8500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa2810-99b1-459b-8fd2-cf670c85c9d1",
   "metadata": {},
   "source": [
    "### Step 1: Define the model-building function and the search space\n",
    "The model-building function in Keras Tuner is a key component of the hyperparameter tuning process. It allows us to define the architecture of our neural network and specify which hyperparameters should be tuned. The function-based approach allows Keras Tuner to build multiple models with different hyperparameter configurations. By defining the model architecture in a function, we provide a flexible template that Keras Tuner can use to explore various combinations of hyperparameters.\n",
    "\n",
    "We will define tunable hyperparameters using the `hp` object, specifying ranges or options for each. This includes integers (e.g., number of units), floats (e.g., learning rate), and choices (e.g., activation functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d517c99-3048-45d2-93b3-1e8db7f7811d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Adding the first dense layer with variable units\n",
    "    model.add(Dense(units=hp.Int('units_layer1', min_value=32, max_value=512, step=32), \n",
    "                    activation='relu', \n",
    "                    input_shape=(10,)))\n",
    "    \n",
    "    # Adding dropout layer and tuning the dropout rate to control overfitting\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Adding a second dense layer with the same variable units as above\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), \n",
    "                    activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])))\n",
    "    \n",
    "    # Conditional hyperparameters to add or remove layers\n",
    "    for i in range(hp.Int('num_additional_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int('units_' + str(i), min_value=2, max_value=10, step=2), \n",
    "                        activation='relu'))\n",
    "    \n",
    "    # Adding the output layer with a single unit for regression\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compiling the model with a tunable optimizer and learning rate\n",
    "    model.compile(optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')), \n",
    "                  loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b877b0e-a56b-4df4-89db-02f91a869bbf",
   "metadata": {},
   "source": [
    "**Explanation of general syntax and concepts**\n",
    "- **Model-building function**:\n",
    "    ```python\n",
    "   def build_model(hp):\n",
    "       # Create and configure the model using 'hp' for hyperparameters\n",
    "       return compiled_model\n",
    "   ```\n",
    "   \n",
    "    This function is called by the tuner during the search process. It receives a `HyperParameters` object (`hp`) that provides methods to define and manage which hyperparameters should be tuned. This function returns a compiled model that can be trained.\n",
    "- **Defining integer hyperparameters** (`hp.Int('name', min_value, max_value, step)`) - Allows the tuner to explore different integer values for a hyperparameter.\n",
    "    - **Parameters:** \n",
    "        - `'name'`: Unique identifier for the hyperparameter.\n",
    "        - `min_value`: Minimum value for the hyperparameter.\n",
    "        - `max_value`: Maximum value.\n",
    "        - `step`: Incremental step between values.\n",
    "    - For example, `hp.Int('units_layer1', min_value=32, max_value=512, step=32)` defines an integer hyperparameter named `'units_layer1'`. The range for this hyperparameter is from 32 to 512, in steps of 32. This enables the tuner to test different layer sizes (numbers of neurons in the layer).\n",
    "- **Defining floating-point hyperparameters** (`hp.Float('name', min_value, max_value, step)`) - Allows exploration of continuous values within a specified range.\n",
    "    - **Parameters:** \n",
    "        - `'name'`: Identifier for the hyperparameter.\n",
    "        - `min_value`: Minimum floating-point value.\n",
    "        - `max_value`: Maximum value.\n",
    "        - `step`: Incremental step for values.\n",
    "    - For example, `hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)` defines a floating-point hyperparameter for the dropout rate. This range and step size allow the tuner to explore various levels of dropout.\n",
    "- **Choice hyperparameters** (`hp.Choice('name', ['option1', 'option2', ...])`) - Enables selection from a predefined set of options.\n",
    "    - **Parameters:** \n",
    "        - `'name'`: Name of the hyperparameter.\n",
    "        - A list of possible values (e.g., activation functions).\n",
    "    - For example, `hp.Choice('activation_layer1', ['relu', 'tanh', 'sigmoid'])` defines a choice hyperparameter, allowing the tuner to select the activation function for the layer from a list of options.\n",
    "- **Conditional hyperparameters**:\n",
    "    ```python\n",
    "   if hp.Boolean('condition'):\n",
    "       # Conditional code based on boolean hyperparameter\n",
    "   ```\n",
    "   \n",
    "   It allows dynamic changes in the model architecture based on the value of other hyperparameters. For example, adding or removing layers based on a boolean condition.\n",
    "   - For example, the loop with `hp.Int('num_additional_layers', 1, 3)` defines a conditional integer hyperparameter that determines how many additional layers are added. This allows for dynamic changes in model architecture based on other hyperparameter values.\n",
    "- **Sampling on a logarithmic scale:** - `sampling='LOG'` - Specifies that the learning rate values should be sampled on a logarithmic scale. This allows for more efficient exploration of a wide range of values, especially useful for learning rates that can span several orders of magnitude.\n",
    "\n",
    "### Step 2: Initialize the tuner\n",
    "Choose a tuning strategy and initialize the tuner. Each tuner has different strengths and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6511eae3-b0ee-4d77-91b3-3e435cbb54ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random search tuner\n",
    "tuner_random = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,  # Maximum number of hyperparameter combinations to try\n",
    "    executions_per_trial=3,  # Number of times to train each model for reliability\n",
    "    directory='tuning_dir',\n",
    "    project_name='random_search')\n",
    "\n",
    "# Bayesian optimization tuner\n",
    "tuner_bayesian = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuning_dir',\n",
    "    project_name='bayesian_optimization')\n",
    "\n",
    "# Hyperband tuner\n",
    "tuner_hyperband = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=5,  # Maximum number of epochs for training\n",
    "    factor=3,  # Factor to reduce the number of trials\n",
    "    directory='tuning_dir',\n",
    "    project_name='hyperband')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81961f77-68a9-474e-956e-5c2e1f98b75f",
   "metadata": {},
   "source": [
    "**Explanation of general syntax and concepts**\n",
    "\n",
    "1. **Tuner initialization:**\n",
    "   ```python\n",
    "   tuner = TunerClass(\n",
    "       build_model, \n",
    "       objective='objective_metric',\n",
    "       max_trials=number_of_trials,\n",
    "       executions_per_trial=number_of_executions,\n",
    "       directory='directory_name',\n",
    "       project_name='project_name'\n",
    "   )\n",
    "   ```\n",
    "   - **Purpose:** Initializes a tuner with a specific strategy to find the best hyperparameters.\n",
    "   - **Parameters:**\n",
    "     - `TunerClass`: The specific tuner class to use (e.g., `RandomSearch`, `BayesianOptimization`, `Hyperband`).\n",
    "     - `build_model`: The model-building function defined in Step 1.\n",
    "     - `objective`: The metric to optimize, typically a validation loss or accuracy.\n",
    "     - `max_trials`: The maximum number of hyperparameter combinations to try.\n",
    "     - `executions_per_trial`: The number of times each model configuration is trained to ensure reliability.\n",
    "     - `directory`: The directory where tuning results are saved.\n",
    "     - `project_name`: A name for the current tuning project, used to distinguish results.\n",
    "\n",
    "2. **Tuning strategies:**\n",
    "   - **Random search:** `RandomSearch(build_model, ...)`\n",
    "   - **Bayesian optimization:** `BayesianOptimization(build_model, ...)`\n",
    "   - **Hyperband:** `Hyperband(build_model, max_epochs=number_of_epochs, factor=reduction_factor, ...)`\n",
    "     - **Parameters:**\n",
    "       - `max_epochs`: The maximum number of epochs for training.\n",
    "       - `factor`: Reduction factor for the number of trials per round, keeping only the top-performing configurations.\n",
    "       \n",
    "### Step 3: Search for the best hyperparameters\n",
    "Initiate the search process with the tuner, specifying the training data and any other necessary parameters. The `search` method is similar to `model.fit` in that it trains a model on the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b45c23-5079-44d8-b8b0-a96597972067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 07s]\n",
      "val_loss: 0.09554338206847508\n",
      "\n",
      "Best val_loss So Far: 0.09554338206847508\n",
      "Total elapsed time: 00h 00m 43s\n"
     ]
    }
   ],
   "source": [
    "print(\"Search process for random search tuner:\")\n",
    "tuner_random.search(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3205d3f3-14eb-41a5-a196-de6c13d565c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 07s]\n",
      "val_loss: 0.3189740628004074\n",
      "\n",
      "Best val_loss So Far: 0.08553072065114975\n",
      "Total elapsed time: 00h 00m 31s\n"
     ]
    }
   ],
   "source": [
    "print(\"Search process for Bayesian optimization tuner:\")\n",
    "tuner_bayesian.search(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feab2c8f-20cc-4a06-b6f1-aff444595efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "val_loss: 0.11265049874782562\n",
      "\n",
      "Best val_loss So Far: 0.08507603406906128\n",
      "Total elapsed time: 00h 00m 24s\n"
     ]
    }
   ],
   "source": [
    "print(\"Search process for hyperband tuner:\")\n",
    "tuner_hyperband.search(X_train, y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb43b4-b6e8-413c-b64f-47b081eafa12",
   "metadata": {},
   "source": [
    "**Explanation of general syntax and concepts**\n",
    "\n",
    "- **Search method** (`tuner.search(X, y, epochs=number_of_epochs, validation_split=split_fraction)`) - Starts the hyperparameter search using the defined model and search space.\n",
    "   - **Parameters:**\n",
    "     - `X`: The input features for training the model.\n",
    "     - `y`: The target values for the model to learn.\n",
    "     - `epochs`: The number of epochs each model will be trained for during the search.\n",
    "     - `validation_split`: Fraction of the training data to be used as validation data. It helps evaluate model performance on unseen data.\n",
    "\n",
    "### Step 4: Get the best model\n",
    "Now, We can extract the best-performing models and their corresponding hyperparameters. After retrieving the best model, we can further train it on more data, evaluate its performance on a test set, or deploy it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de223c6d-9e0f-4cc5-bedc-9a5f3ad490e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters from random search: {'units_layer1': 256, 'dropout_rate': 0.4, 'units': 128, 'activation': 'relu', 'num_additional_layers': 1, 'units_0': 4, 'learning_rate': 0.0012052065865684864, 'units_1': 4, 'units_2': 10}\n",
      "\n",
      "Best hyperparameters from Bayesian optimization: {'units_layer1': 448, 'dropout_rate': 0.1, 'units': 320, 'activation': 'tanh', 'num_additional_layers': 2, 'units_0': 6, 'learning_rate': 0.0005437534080408972, 'units_1': 8}\n",
      "\n",
      "Best hyperparameters from hyperband search: {'units_layer1': 96, 'dropout_rate': 0.2, 'units': 224, 'activation': 'relu', 'num_additional_layers': 3, 'units_0': 6, 'learning_rate': 0.007815055091465812, 'units_1': 4, 'units_2': 10, 'tuner/epochs': 5, 'tuner/initial_epoch': 2, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0002'}\n"
     ]
    }
   ],
   "source": [
    "# Random search best model\n",
    "best_model_random = tuner_random.get_best_models(num_models=1)[0]\n",
    "best_hp_random = tuner_random.get_best_hyperparameters()[0]\n",
    "print(f\"Best hyperparameters from random search: {best_hp_random.values}\")\n",
    "\n",
    "# Bayesian optimization best model\n",
    "best_model_bayesian = tuner_bayesian.get_best_models(num_models=1)[0]\n",
    "best_hp_bayesian = tuner_bayesian.get_best_hyperparameters()[0]\n",
    "print(f\"\\nBest hyperparameters from Bayesian optimization: {best_hp_bayesian.values}\")\n",
    "\n",
    "# Hyperband search best model\n",
    "best_model_hyperband = tuner_hyperband.get_best_models(num_models=1)[0]\n",
    "best_hp_hyperband = tuner_hyperband.get_best_hyperparameters()[0]\n",
    "print(f\"\\nBest hyperparameters from hyperband search: {best_hp_hyperband.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64269d18-96f2-4824-87fa-45ee886c9334",
   "metadata": {},
   "source": [
    "**Explanation of general syntax and concepts**\n",
    "\n",
    "1. **Get best model:** (`best_models = tuner.get_best_models(num_models=n)`) - Retrieves the top `n` models with the best performance based on the objective metric.\n",
    "    - `num_models`: The number of top models to retrieve.\n",
    "2. **Get best hyperparameters:**\n",
    "   `best_hyperparameters = tuner.get_best_hyperparameters(num_trials=n)` - Obtains the hyperparameters corresponding to the top-performing models.\n",
    "    - `num_trials`: The number of top hyperparameter sets to retrieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
