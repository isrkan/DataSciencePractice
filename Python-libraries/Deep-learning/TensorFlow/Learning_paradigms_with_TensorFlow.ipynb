{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaedac6-3be2-49d0-a638-24012eae23c9",
   "metadata": {},
   "source": [
    "# Learning paradigms with TensorFlow\n",
    "\n",
    "This notebook explores various learning paradigms in deep learning, implemented using TensorFlow. Deep learning has evolved to include diverse techniques that extend beyond traditional supervised learning. These paradigms enable models to perform better on complex tasks, adapt to new tasks with limited data, and leverage shared knowledge across multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687a2048-a236-4f83-8b62-b5bec6b8393b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb7f8e-2533-4d2a-a32e-36c2da02842a",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "Transfer learning is a machine learning technique where a model that has already been trained on a large dataset is reused or fine-tuned on a new, often smaller dataset. Instead of starting from scratch, transfer learning allows us to leverage the knowledge captured in a pre-trained model to improve the performance and efficiency of a new model. This approach is particularly valuable because training deep neural networks from scratch typically requires vast amounts of data and computational resources. Transfer learning allows us to start with a pre-trained model, reducing the time and data needed. Key concepts:\n",
    "- Pre-trained model: A neural network model that has already been trained on a large dataset.\n",
    "- Fine-tuning: Adjusting the weights of the pre-trained model to adapt it to a new dataset or task.\n",
    "\n",
    "We will start by training a model from scratch, saving its weights, and then using that model in various transfer learning scenarios.\n",
    "\n",
    "### Pre-trained model\n",
    "We will define a simple FFNN model and train it on the synthetic data. After training, we will save the model's weights so that we can use them later in different transfer learning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d043942b-fa48-47e4-b417-da6322fe3f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 20ms/step - loss: 0.6961 - accuracy: 0.5181 - val_loss: 0.6927 - val_accuracy: 0.4875\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6911 - accuracy: 0.5264 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6887 - accuracy: 0.5403 - val_loss: 0.6927 - val_accuracy: 0.5125\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6877 - accuracy: 0.5250 - val_loss: 0.6914 - val_accuracy: 0.5375\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6846 - accuracy: 0.5417 - val_loss: 0.6929 - val_accuracy: 0.5125\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6813 - accuracy: 0.5833 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6764 - accuracy: 0.5639 - val_loss: 0.6940 - val_accuracy: 0.5125\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6728 - accuracy: 0.5847 - val_loss: 0.6916 - val_accuracy: 0.5625\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.5833 - val_loss: 0.6944 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6645 - accuracy: 0.6278 - val_loss: 0.6902 - val_accuracy: 0.5500\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6978 - accuracy: 0.5200\n",
      "Model accuracy: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model weights\n",
    "model.save_weights('pretrained_model_weights.h5')\n",
    "model.save('pretrained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0b809-27cd-4764-a85a-2985fc1ad37f",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "In this section, we created a simple feedforward neural network (FFNN) and train it on synthetic binary classification data. Hereâ€™s a breakdown of the steps:\n",
    "- Step 1: Load/generate data for a related task - Here, we generate synthetic data with 20 features and a binary target variable. The data is then split into training and testing sets.\n",
    "- Step 2: Model definition - Here, We defined a sequential model with three hidden layers and an output layer. The hidden layers use ReLU activation, and the output layer uses the sigmoid activation function for binary classification.\n",
    "- Step 3: Model training - Here, the model is trained for 10 epochs, using a validation split of 10%.\n",
    "- Step 4: Saving the model - The trained model's weights and the entire model are saved to files, which will be used later for transfer learning.\n",
    "\n",
    "### Types of transfer learning\n",
    "Transfer learning can be applied in several ways, depending on how the pre-trained model is used and the nature of the new task. Let's explore different types of transfer learning techniques using the pre-trained model we just saved.\n",
    "\n",
    "#### Model as a fixed pre-trained model\n",
    "In this approach, we use the pre-trained model directly without any changes. This is typically done when the new task is very similar to the original task for which the model was trained. The pre-trained model's layers are kept unchanged, and the model is used as-is without further training to make predictions on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073cf592-db8b-4500-b15c-9e3688eb420e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6961 - accuracy: 0.4800\n",
      "Fixed pre-trained model accuracy: 0.47999998927116394\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Predictions from fixed pre-trained model: [[0.6053627 ]\n",
      " [0.56828934]\n",
      " [0.58841944]\n",
      " [0.54554385]\n",
      " [0.6093628 ]]\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data\n",
    "X_fixed = np.random.rand(100, 20)\n",
    "y_fixed = np.random.randint(2, size=100)\n",
    "\n",
    "# Define a new model with the same architecture as the trained model\n",
    "model_fixed = Sequential()\n",
    "model_fixed.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model_fixed.add(Dense(64, activation='relu'))\n",
    "model_fixed.add(Dense(32, activation='relu'))\n",
    "model_fixed.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model_fixed.load_weights('pretrained_model_weights.h5')\n",
    "\n",
    "# Compile the loaded model\n",
    "model_fixed.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model on new data\n",
    "loss, accuracy = model_fixed.evaluate(X_fixed, y_fixed)\n",
    "print(f\"Fixed pre-trained model accuracy: {accuracy}\")\n",
    "\n",
    "# Use the pre-trained model directly for prediction\n",
    "predictions = model_fixed.predict(X_fixed)\n",
    "print(f\"Predictions from fixed pre-trained model: {predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e568909-4895-40e7-abad-c16e53888924",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "Here, we use the previously trained model as-is, without any further training:\n",
    "- Step 1: Load the data for a similar task.\n",
    "- Step 2: Model definition: We defined a new model with the same architecture as the pre-trained model to ensure compatibility with the saved weights.\n",
    "- Step 3: Load the pre-trained weights.\n",
    "- Step 4: Evaluation - Evaluate the model on the new dataset without further training.\n",
    "- Step 5: Prediction - The pre-trained model is used to make predictions on the new data, demonstrating its ability to generalize to unseen data.\n",
    "\n",
    "#### Feature extraction transfer learning\n",
    "In this approach, we use the pre-trained model as a feature extractor. We freeze the lower layers (which capture general features) and add new layers on top to adapt to the new task. The output of the pre-trained model (before the final layer) is fed into a new model designed for the new task, allowing the model to learn task-specific features without retraining the entire network. This method is particularly useful when the new task is related to the original task but requires a different output or representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a19a893-c61b-4e50-aaf1-71805d094980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 60ms/step - loss: 0.7036 - accuracy: 0.3843 - val_loss: 0.6952 - val_accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6924 - accuracy: 0.3796 - val_loss: 0.6855 - val_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6829 - accuracy: 0.3611 - val_loss: 0.6773 - val_accuracy: 0.3333\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6740 - accuracy: 0.3704 - val_loss: 0.6707 - val_accuracy: 0.3750\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.6667 - accuracy: 0.3704 - val_loss: 0.6650 - val_accuracy: 0.4167\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6810 - accuracy: 0.2167\n",
      "Feature extractor model accuracy: 0.21666666865348816\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data (related but different task)\n",
    "np.random.seed(42)\n",
    "X_feature = np.random.rand(300, 20)\n",
    "y_feature = np.random.randint(3, size=300)\n",
    "y_feature_categorical = to_categorical(y_feature, num_classes=3)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_feature, X_test_feature, y_train_feature, y_test_feature = train_test_split(X_feature, y_feature_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "pretrained_model_feature_extraction = load_model(\"pretrained_model.h5\")\n",
    "\n",
    "# Define a new model with the pre-trained layers as a feature extractor\n",
    "feature_extractor = Sequential(pretrained_model_feature_extraction.layers[:-1])  # Exclude the last layer\n",
    "\n",
    "# Freeze the layers in the feature extractor\n",
    "for layer in feature_extractor.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Use the pre-trained model as a feature extractor and add new layers for the new task\n",
    "model_feature_extraction = Sequential()\n",
    "model_feature_extraction.add(feature_extractor)  # The pre-trained feature extractor\n",
    "model_feature_extraction.add(Dense(16, activation='relu'))  # New dense layer\n",
    "model_feature_extraction.add(Dense(3, activation='softmax'))  # Output layer for the new task\n",
    "    \n",
    "# Compile the new model\n",
    "model_feature_extraction.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model\n",
    "model_feature_extraction.fit(X_train_feature, y_train_feature, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model_feature_extraction.evaluate(X_test_feature, y_test_feature)\n",
    "print(f\"Feature extractor model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a3c82-b9d6-46cf-86dc-4ac99965658f",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "In this approach, we use the pre-trained model as a feature extractor for a related but different task:\n",
    "- Step 1: Load the data for a related but different task.\n",
    "- Step 2: Load the pre-trained layers as a feature extractor by excluding the final layer(s). The last layer of the pre-trained model is typically a classification layer tailored for the original task. Since we are focusing on feature extraction, we exclude this layer to use the rest of the model as a fixed feature extractor.\n",
    "- Step 3: Freeze the layers of the feature extractor to ensure the pre-trained weights are not updated during the training process of the new task.\n",
    "- Step 4: Add new layers on top of the feature extractor to adapt the model to the new task. This typically includes a dense layer (or layers) and an output layer tailored to the number of classes in the new task.\n",
    "- Step 5: Train the model on the new dataset for a few epochs.\n",
    "- Step 6: Evaluate the model on the new dataset.\n",
    "\n",
    "#### Fine-tuning transfer learning\n",
    "Fine-tuning is a more flexible approach where we start with the pre-trained model but allow some or all layers to be further trained on the new task. This approach allows the model to adapt more closely to the new task while retaining the knowledge learned from the pre-trained modelin the original task. Fine-tuning is often used when the new task is sufficiently different from the original task, and the pre-trained model needs to be adjusted to better fit the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0729a1c6-0818-4cb1-b3ec-d5139e80805a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 2s 49ms/step - loss: 1.1203 - accuracy: 0.2917 - val_loss: 1.1301 - val_accuracy: 0.1562\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.1034 - accuracy: 0.2917 - val_loss: 1.1155 - val_accuracy: 0.0938\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 1.0986 - accuracy: 0.3229 - val_loss: 1.1088 - val_accuracy: 0.1875\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 1.0968 - accuracy: 0.3438 - val_loss: 1.0984 - val_accuracy: 0.4062\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 1.0961 - accuracy: 0.3715 - val_loss: 1.0993 - val_accuracy: 0.4062\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0987 - accuracy: 0.3500\n",
      "Fine-tuned model accuracy: 0.3499999940395355\n"
     ]
    }
   ],
   "source": [
    "# Generate new synthetic data (related task)\n",
    "np.random.seed(42)\n",
    "X_finetune = np.random.rand(400, 20)\n",
    "y_finetune = np.random.randint(3, size=400)\n",
    "y_finetune_categorical = to_categorical(y_finetune, num_classes=3)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_finetune, X_test_finetune, y_train_finetune, y_test_finetune = train_test_split(X_finetune, y_finetune_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model_finetune = load_model(\"pretrained_model.h5\")\n",
    "\n",
    "# Make some layers trainable\n",
    "# Optionally, freeze some layers early in the network\n",
    "for layer in pretrained_model_finetune.layers[:2]:  # Assuming the first 3 layers should be frozen\n",
    "    layer.trainable = False\n",
    "# The remaining layers will be fine-tuned\n",
    "for layer in pretrained_model_finetune.layers[2:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Add new layers for the new task (if necessary). If the pre-trained model's output layer is not suitable, replace it\n",
    "pretrained_model_finetune.pop()  # Remove the last layer\n",
    "pretrained_model_finetune.add(Dense(64, activation='relu'))  # Add a new dense layer\n",
    "pretrained_model_finetune.add(Dense(3, activation='softmax'))  # New output layer for the new task\n",
    "\n",
    "# Compile the model\n",
    "pretrained_model_finetune.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "pretrained_model_finetune.fit(X_train_finetune, y_train_finetune, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = pretrained_model_finetune.evaluate(X_test_finetune, y_test_finetune)\n",
    "print(f\"Fine-tuned model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8038d4-ee37-4cb7-ad2d-2a7031f0d229",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "This approach involves fine-tuning a pre-trained model to better fit a related task:\n",
    "- Step 1: Load data for a related task\n",
    "- Step 2: Load the pre-trained model\n",
    "- Step 3: Decide which layers to fine-tune (if needed) - Typically, the earlier layers are kept frozen because they capture more generic features, while the later layers are fine-tuned since they capture task-specific features.\n",
    "- Step 4: Add new layers for the new task (if necessary) - If the output layer of the pre-trained model doesn't match the number of classes in the new dataset, we should replace it or add new layers on top of the pre-trained model.\n",
    "- Step 5: Train the model on the new dataset.\n",
    "- Step 6: Evaluate the model on the new dataset.\n",
    "\n",
    "#### Knowledge distillation (teacher-student model)\n",
    "\n",
    "In knowledge distillation, the knowledge from a large, pre-trained model (the teacher model) is transferred to a smaller and simpler model (the student model). The idea is that the student model learns to mimic the teacher model's behavior, achieving a similar performance with fewer parameters, which makes it more efficient for deployment on devices with limited resources.\n",
    "\n",
    "##### Response-based knowledge distillation\n",
    "Response-based knowledge distillation focuses on the output predictions (responses) of the teacher model. The student model is trained to mimic the probability distribution (soft labels) produced by the teacher model, rather than the hard labels. \n",
    "\n",
    "We will use a pre-trained model as the teacher model. The teacher model generates a probability distribution over classes (soft labels) for each input. Then, we will define a new, smaller student model. The student model will be trained to reproduce a similar probability distribution to match the output of the teacher model rather than directly training on the target labels. \n",
    "\n",
    "The typical loss function used is Kullback-Leibler Divergence (KLD), which measures the difference between the teacher's and student's probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bfe347e-36d6-4b80-bf3e-61a33d73a8e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 18ms/step - loss: 0.6958 - accuracy: 0.4778 - val_loss: 0.6933 - val_accuracy: 0.5125\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6886 - accuracy: 0.5639 - val_loss: 0.6933 - val_accuracy: 0.4875\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.5750 - val_loss: 0.6901 - val_accuracy: 0.4750\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6835 - accuracy: 0.5431 - val_loss: 0.6921 - val_accuracy: 0.4750\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6776 - accuracy: 0.5625 - val_loss: 0.6903 - val_accuracy: 0.4625\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6738 - accuracy: 0.5583 - val_loss: 0.6964 - val_accuracy: 0.5125\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.6749 - accuracy: 0.5736 - val_loss: 0.6886 - val_accuracy: 0.5125\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.6690 - accuracy: 0.5819 - val_loss: 0.7015 - val_accuracy: 0.4875\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6560 - accuracy: 0.6139 - val_loss: 0.6935 - val_accuracy: 0.5125\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.6028 - val_loss: 0.7134 - val_accuracy: 0.5125\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 19ms/step - loss: 0.3496 - accuracy: 0.9944 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.9289e-04 - accuracy: 1.0000 - val_loss: 8.2199e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.3321e-04 - accuracy: 1.0000 - val_loss: 5.5223e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.3446e-04 - accuracy: 1.0000 - val_loss: 3.9582e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3.1710e-04 - accuracy: 1.0000 - val_loss: 2.9366e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.3914e-04 - accuracy: 1.0000 - val_loss: 2.2723e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8026e-04 - accuracy: 1.0000 - val_loss: 1.5287e-04 - val_accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 6.7197e-05 - accuracy: 0.5150\n",
      "Response-based Student model accuracy: 0.5149999856948853\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "y = np.random.randint(2, size=1000)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Teacher model\n",
    "teacher_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "teacher_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "teacher_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Generate soft labels (probability distribution) from the Teacher model\n",
    "teacher_logits = teacher_model.predict(X_train)\n",
    "teacher_soft_labels = tf.nn.softmax(teacher_logits / 5.0)  # Using temperature scaling\n",
    "\n",
    "# Define the Student model\n",
    "student_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the Student model using KLD loss\n",
    "student_model.compile(optimizer=Adam(), loss=KLDivergence(), metrics=['accuracy'])\n",
    "\n",
    "# Train the Student model on the soft labels\n",
    "student_model.fit(X_train, teacher_soft_labels, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the Student model\n",
    "student_loss, student_accuracy = student_model.evaluate(X_test, y_test)\n",
    "print(f\"Response-based Student model accuracy: {student_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d76e1c-4d97-4fa8-a894-6e893b01ba09",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- Step 1: Teacher model definition and training - Here, a FFNN with three hidden layers is defined. It uses ReLU activation functions in the hidden layers and a sigmoid activation function in the output layer for binary classification. The teacher model is compiled with the Adam optimizer and binary cross-entropy loss. It is trained for 10 epochs using the training data.\n",
    "- Step 2: Generate soft labels - After training, the teacher model's predictions are used to create soft labels. These labels are the probability distributions over classes, scaled by a temperature parameter (5.0 in this case). The temperature scaling helps smooth the probability distributions, making it easier for the student model to learn from them.\n",
    "- Step 3: Student Model definition andt training - Here, a smaller FFNN with fewer layers and units is defined for the student model. This model has fewer parameters and is simpler than the teacher model. The student model is compiled with the Kullback-Leibler Divergence loss function, which measures the difference between the soft labels provided by the teacher and the student model's predictions. The student model is trained on the soft labels generated by the teacher model for 10 epochs.\n",
    "- Step 4: Evaluation - Finally, the student model is evaluated on the test set to measure its accuracy. This step demonstrates how effectively the student model has learned to replicate the teacher's behavior by using the response-based distillation approach.\n",
    "\n",
    "## Multi-task learning\n",
    "\n",
    "Multi-task learning (MTL) is an technique where a single model is trained to perform multiple tasks simultaneously. Instead of training separate models for each task, MTL uses a unified architecture to learn from related tasks concurrently. The core idea behind MTL is that by learning multiple tasks together, the model can exploit commonalities and shared patterns across tasks, leading to improved generalization and performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b8bc08-a4bb-4655-8f3e-fd0d87fd060f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 28ms/step - loss: 0.8151 - classification_loss: 0.6994 - regression_loss: 0.1156 - classification_accuracy: 0.5312 - regression_mse: 0.1156 - val_loss: 0.8195 - val_classification_loss: 0.7059 - val_regression_loss: 0.1135 - val_classification_accuracy: 0.5063 - val_regression_mse: 0.1135\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7846 - classification_loss: 0.6932 - regression_loss: 0.0914 - classification_accuracy: 0.5312 - regression_mse: 0.0914 - val_loss: 0.8117 - val_classification_loss: 0.7008 - val_regression_loss: 0.1109 - val_classification_accuracy: 0.5063 - val_regression_mse: 0.1109\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7779 - classification_loss: 0.6910 - regression_loss: 0.0869 - classification_accuracy: 0.5328 - regression_mse: 0.0869 - val_loss: 0.8057 - val_classification_loss: 0.6979 - val_regression_loss: 0.1078 - val_classification_accuracy: 0.5188 - val_regression_mse: 0.1078\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7733 - classification_loss: 0.6896 - regression_loss: 0.0837 - classification_accuracy: 0.5328 - regression_mse: 0.0837 - val_loss: 0.8031 - val_classification_loss: 0.6969 - val_regression_loss: 0.1063 - val_classification_accuracy: 0.5188 - val_regression_mse: 0.1063\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7703 - classification_loss: 0.6882 - regression_loss: 0.0820 - classification_accuracy: 0.5359 - regression_mse: 0.0820 - val_loss: 0.8062 - val_classification_loss: 0.6968 - val_regression_loss: 0.1094 - val_classification_accuracy: 0.5188 - val_regression_mse: 0.1094\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7674 - classification_loss: 0.6873 - regression_loss: 0.0801 - classification_accuracy: 0.5344 - regression_mse: 0.0801 - val_loss: 0.8021 - val_classification_loss: 0.6960 - val_regression_loss: 0.1061 - val_classification_accuracy: 0.5188 - val_regression_mse: 0.1061\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7652 - classification_loss: 0.6862 - regression_loss: 0.0789 - classification_accuracy: 0.5453 - regression_mse: 0.0789 - val_loss: 0.8042 - val_classification_loss: 0.6964 - val_regression_loss: 0.1078 - val_classification_accuracy: 0.5188 - val_regression_mse: 0.1078\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7632 - classification_loss: 0.6855 - regression_loss: 0.0777 - classification_accuracy: 0.5406 - regression_mse: 0.0777 - val_loss: 0.8014 - val_classification_loss: 0.6947 - val_regression_loss: 0.1068 - val_classification_accuracy: 0.5375 - val_regression_mse: 0.1068\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7612 - classification_loss: 0.6843 - regression_loss: 0.0768 - classification_accuracy: 0.5516 - regression_mse: 0.0768 - val_loss: 0.8015 - val_classification_loss: 0.6955 - val_regression_loss: 0.1060 - val_classification_accuracy: 0.5312 - val_regression_mse: 0.1060\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.7585 - classification_loss: 0.6836 - regression_loss: 0.0749 - classification_accuracy: 0.5578 - regression_mse: 0.0749 - val_loss: 0.8015 - val_classification_loss: 0.6952 - val_regression_loss: 0.1063 - val_classification_accuracy: 0.5437 - val_regression_mse: 0.1063\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7901 - classification_loss: 0.6930 - regression_loss: 0.0970 - classification_accuracy: 0.5400 - regression_mse: 0.0970\n",
      "Classification accuracy: 0.5400000214576721\n",
      "Regression MSE: 0.09702430665493011\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Classification Predictions (first 5): [[0.5105735 ]\n",
      " [0.53726524]\n",
      " [0.5058471 ]\n",
      " [0.5249949 ]\n",
      " [0.5506573 ]]\n",
      "Regression Predictions (first 5): [[0.5503157 ]\n",
      " [0.57404166]\n",
      " [0.48006114]\n",
      " [0.5600674 ]\n",
      " [0.4826324 ]]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 20)\n",
    "\n",
    "# Task 1: Binary classification labels\n",
    "y_classification = np.random.randint(2, size=1000)\n",
    "\n",
    "# Task 2: Regression labels\n",
    "y_regression = np.random.rand(1000)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "_, _, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Shared layers\n",
    "shared = Dense(64, activation='relu')(input_layer)\n",
    "shared = Dense(32, activation='relu')(shared)\n",
    "\n",
    "# Task 1: Classification\n",
    "classification_output = Dense(1, activation='sigmoid', name='classification')(shared)\n",
    "\n",
    "# Task 2: Regression\n",
    "regression_output = Dense(1, name='regression')(shared)\n",
    "\n",
    "# Define the model\n",
    "mtl_model = Model(inputs=input_layer, outputs=[classification_output, regression_output])\n",
    "\n",
    "# Compile the model with two loss functions: binary_crossentropy for classification and mse for regression\n",
    "mtl_model.compile(optimizer=Adam(), \n",
    "                  loss={'classification': 'binary_crossentropy', 'regression': 'mse'},\n",
    "                  metrics={'classification': 'accuracy', 'regression': 'mse'})\n",
    "\n",
    "# Train the model\n",
    "history = mtl_model.fit(X_train, \n",
    "                        {'classification': y_train_class, 'regression': y_train_reg}, \n",
    "                        epochs=10, \n",
    "                        batch_size=32, \n",
    "                        validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation = mtl_model.evaluate(X_test, {'classification': y_test_class, 'regression': y_test_reg})\n",
    "print(f\"Classification accuracy: {evaluation[3]}\")  # Accuracy for classification task\n",
    "print(f\"Regression MSE: {evaluation[4]}\")  # MSE for regression task\n",
    "\n",
    "# Generate predictions for both tasks on the test data\n",
    "predictions = mtl_model.predict(X_test)\n",
    "# Extract predictions for each task\n",
    "classification_predictions = predictions[0]  # Predictions for the classification task\n",
    "regression_predictions = predictions[1]  # Predictions for the regression task\n",
    "print(\"Classification Predictions (first 5):\", classification_predictions[:5])\n",
    "print(\"Regression Predictions (first 5):\", regression_predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3b4db-b4f2-4826-b393-84344902037a",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "- Step 1: Load/generate data for a related task - Here, we generated synthetic data with 20 features. We created two types of labels, binary classification labels (for the classification task, we generate binary labels) and regression labels (for the regression task, we generate continuous values). Then, the data is splitted into training and testing sets separately for each task.\n",
    "- Step 2: Model definition:\n",
    "  - Input layer: An input layer is defined to accept data with 20 features.\n",
    "  - Shared layers: Two hidden layers are defined, which are shared between both tasks. These layers learn common features from the input data.\n",
    "  - Task-specific outputs**:\n",
    "    - Classification output: A dense layer with a sigmoid activation function for binary classification.\n",
    "    - Regression output: A dense layer for regression with no activation function, suitable for predicting continuous values.\n",
    "- Step 2: Model compilation - The model is compiled with the Adam optimizer and the specified loss functions and metrics.\n",
    "  - Loss functions:\n",
    "    - Binary cross-entropy: Used for the classification task to measure the difference between predicted and true binary labels.\n",
    "    - Mean squared error: Used for the regression task to measure the difference between predicted and true continuous values.\n",
    "  - Metrics:\n",
    "    - Accuracy: For the classification task.\n",
    "    - MSE: For the regression task.\n",
    "- Step 3: Training - The model is trained on the training data for both tasks. The training process involves minimizing the combined loss from both tasks.\n",
    "- Step 4: Evaluation - After training, the model is evaluated on the test set for both tasks. Here, the evaluation metrics include classification accuracy and regression MSE.\n",
    "- Step 5: Creating predictions - After training, predictions for both tasks can be made using the model on new or test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
