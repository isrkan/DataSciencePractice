{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42d1772-9a6e-4ebc-bf2a-a84a09b2eb16",
   "metadata": {},
   "source": [
    "# Regression models with scikit-learn\n",
    "\n",
    "Scikit-learn offers a wide variety of regression models, each suited to different types of data and problem domains. Whether we are dealing with linear relationships (like linear regression), complex interactions (like decision trees and neural networks), or need robustness to outliers (like Theil-Sen), Scikit-learn provides easy-to-use implementations. With its consistent API, Scikit-learn simplifies the process of building, evaluating, and fine-tuning regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9494e33-a123-4e4a-89d9-a8f7f59fd97f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, TheilSenRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfaf35-af6f-4fc4-8447-4711f5888dd8",
   "metadata": {},
   "source": [
    "We will use the California housing dataset available in Scikit-Learn. This dataset contains information about house prices in California and includes features such as median income, house age, and the number of rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94db0718-f712-4ece-be8a-399a354ee3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California housing dataset (First 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the California housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "\n",
    "# Display the dataset\n",
    "print(\"California housing dataset (First 5 rows):\")\n",
    "display(X.head())\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d2c5-25b4-44b6-98e1-7e44c4e2aca9",
   "metadata": {},
   "source": [
    "## Linear regression models\n",
    "Linear Regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the dependent variable and the independent variables.\n",
    "\n",
    "### Ordinary least squares (OLS)\n",
    "OLS is the most commonly used method for estimating the parameters in a linear regression model. It works by minimizing the sum of the squared differences between the observed values (actual data points) and the values predicted by the linear model. The OLS method provides the best linear unbiased estimates of the coefficients in the linear regression model under the assumptions of linearity, independence, homoscedasticity (constant variance of the errors), and normality of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b904d8-ece7-47cc-881d-8cc00eb92b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.5559\n",
      "Mean absolute error (MAE): 0.5332\n",
      "R-squared score: 0.5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('linear_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('linear_regression_model.pkl', 'rb') as f:\n",
    "    loaded_lr_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_lr)\n",
    "mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2 = r2_score(y_test, y_pred_lr)\n",
    "print(f\"Linear regression performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc98a8-b48e-4153-979f-74a8cc2ba226",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "**Syntax**: The `LinearRegression` class in Scikit-learn is used to implement the OLS regression model. \n",
    "```python\n",
    "LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
    "```\n",
    "- `*`: Indicates that all parameters following `*` must be passed as keyword arguments, not positional arguments.\n",
    "- **`fit_intercept`** (`bool`, default=`True`): Determines whether to calculate the intercept (`b0`) for this model. If `True`, the model will include the intercept term in the regression equation. If `False`, the model will not include an intercept term, forcing the line to pass through the origin (0,0). Most regression models require an intercept term to account for the baseline level of the dependent variable when all independent variables are zero. We might set this to `False` where you know for certain that the dependent variable should be zero when all independent variables are zero.\n",
    "- **`copy_X`** (`bool`, default=`True`): If `True`, the input data `X` will be copied. Otherwise, it may be overwritten. Use `True` to ensure the original dataset remains unchanged, which is important if we are using the same dataset for multiple operations or if data integrity is critical. Use `False` to save memory and computation time if we are working with very large datasets.\n",
    "- **`n_jobs`** (`int`, default=`None`): The number of CPU cores to use for the computation. `None` means using a single core, `-1` means using all available cores. Use `-1` or a specific integer for large datasets or computationally intensive tasks where parallel processing can significantly speed up the fitting process.\n",
    "- **`positive`** (`bool`, default=`False`): When set to `True`, forces the coefficients to be positive. This option is useful in some types of problems to avoid negative weights. Use `True` in scenarios where the relationship between the dependent and independent variables is known to be strictly positive.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging the model from fitting too closely to the training data and thereby improving its generalization performance. Scikit-learn provides three popular regularization methods: ridge, lasso, and elastic net.\n",
    "\n",
    "### Ridge regression\n",
    "Ridge regression, also known as L2 regularization, adds a penalty term proportional to the square of the magnitude of the coefficients to the loss function. This penalization discourages large coefficients, effectively shrinking them, and helps reduce the model’s complexity, which can prevent overfitting. Ridge regression does not force coefficients to zero, allowing all features to contribute to the model, albeit with smaller weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e700820-65fb-47eb-ac2c-1e7f050cc1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.5558\n",
      "Mean absolute error (MAE): 0.5332\n",
      "R-squared score: 0.5759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('ridge_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(ridge_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('ridge_regression_model.pkl', 'rb') as f:\n",
    "    loaded_ridge_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ridge = loaded_ridge_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2 = r2_score(y_test, y_pred_ridge)\n",
    "print(f\"Ridge regression performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26f3af-070a-4a15-baef-4b132a5e8cea",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "**Syntax**: \n",
    "```python\n",
    "Ridge(*, alpha=1.0, fit_intercept=True, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
    "```\n",
    "- `*`: Indicates that all parameters following `*` must be passed as keyword arguments, not positional arguments.\n",
    "- **`alpha`** (`float`, default=`1.0`): Regularization strength; must be a positive float. Larger values specify stronger regularization. Adjust `alpha` to control the trade-off between bias and variance. Use a larger `alpha` if the model is overfitting, and reduce it if the model underfits or is too simplistic.\n",
    "- **`fit_intercept`** (`bool`, default=`True`): If `True`, the model calculates the intercept for this model. If set to `False`, no intercept will be used in calculations.\n",
    "- **`copy_X`** (`bool`, default=`True`): If `True`, the input data `X` will be copied. Otherwise, it may be overwritten. Use `True` to avoid altering the original data. Set to `False` for memory efficiency when dealing with large datasets.\n",
    "- **`max_iter`** (`int`, default=`None`): Maximum number of iterations for the solver. Increase this if the solver doesn’t converge, especially for large or complex datasets.\n",
    "- **`tol`** (`float`, default=`0.001`): Tolerance for the stopping criterion. Lower `tol` for higher precision in the solution, or increase it for faster convergence.\n",
    "- **`solver`** (`{'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'}`): Solver to use in the computational routines. ‘auto’ chooses the solver based on the type of data.\n",
    "    - `auto`: Automatically selects the solver based on data.\n",
    "    - `svd`: Suitable for small problems.\n",
    "    - `cholesky`: Faster but less stable.\n",
    "    - `lsqr`, `sparse_cg`: Useful for sparse data.\n",
    "    - `sag`, `saga`: Good for large datasets or when memory is limited.\n",
    "    - `lbfgs`: Limited-memory Broyden–Fletcher–Goldfarb–Shanno, useful for optimization.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): Controls the randomness for data shuffling. Used when the `solver` is set to ‘sag’ or ‘saga’ to shuffle the data.\n",
    "\n",
    "\n",
    "#### Lasso regression\n",
    "Lasso regression, or L1 regularization, adds a penalty proportional to the absolute value of the coefficients to the loss function. This can lead to sparse solutions where some coefficients are exactly zero, effectively performing feature selection. Lasso is useful when we have a large number of features and expect that only a small subset is actually relevant to predicting the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b27f82-b06a-4006-8ea4-2c19d378f251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.6135\n",
      "Mean absolute error (MAE): 0.5816\n",
      "R-squared score: 0.5318\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the lasso regression model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('lasso_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lasso_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('lasso_regression_model.pkl', 'rb') as f:\n",
    "    loaded_lasso_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lasso = loaded_lasso_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_lasso)\n",
    "mae = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2 = r2_score(y_test, y_pred_lasso)\n",
    "print(f\"Lasso regression performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0443d-bc82-48df-9dd4-ce04915b5c4d",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "**Syntax**: \n",
    "```python\n",
    "Lasso(*, alpha=1.0, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "```\n",
    "- `*`: Indicates that all parameters following `*` must be passed as keyword arguments, not positional arguments.\n",
    "- **`alpha`** (`float`, default=`1.0`): Regularization strength; must be a positive float. Larger values specify stronger regularization and more coefficients being zeroed out.\n",
    "- **`fit_intercept`** (`bool`, default=`True`): If `True`, the model calculates the intercept for this model. If set to `False`, no intercept will be used in calculations.\n",
    "- **`precompute`** (`bool` or `array-like`, default=`False`): Determines whether to use a precomputed Gram matrix to speed up calculations. Set to `True` for large datasets to speed up computations.\n",
    "- **`copy_X`** (`bool`, default=`True`): If `True`, the input data `X` will be copied. Otherwise, it may be overwritten. Typically `True` unless working with very large datasets where memory is a concern.\n",
    "- **`max_iter`** (`int`, default=`1000`): Maximum number of iterations for the solver. Increase if the solver does not converge, especially with larger datasets or stricter `tol`.\n",
    "- **`tol`** (`float`, default=`0.0001`): Tolerance for the optimization. Lower it for more precise solutions or increase for faster convergence.\n",
    "- **`warm_start`** (`bool`, default=`False`): If set to `True`, reuses the solution of the previous call to `fit` as initialization. Useful when running Lasso in a loop where we iteratively adjust `alpha` or other parameters.\n",
    "- **`positive`** (`bool`, default=`False`): When set to `True`, forces the coefficients to be positive. Use when the application logically requires non-negative coefficients, such as in economic modeling.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): The seed of the pseudo-random number generator to use when shuffling the data. Set this for reproducibility in experiments.\n",
    "- **`selection`** (`{'cyclic', 'random'}`): Determines the order of coefficient updates.\n",
    "    - `cyclic`: Standard choice, where coefficients are updated in order.\n",
    "    - `random`: A random coefficient is updated every iteration rather than looping over features sequentially. For larger problems or if we suspect the order of features matters; helps in faster convergence sometimes.\n",
    "\n",
    "\n",
    "### Elastic net regression\n",
    "Elastic net regression is a hybrid method that combines the penalties of ridge (L2) and lasso (L1) regression. It adds both the absolute value and square of the magnitude of coefficients as penalties. This allows elastic net to both perform variable selection (like lasso) and shrinkage (like ridge), making it suitable for datasets with highly correlated features or when feature selection is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3091c8f7-9cd7-4083-911c-47a20fc4fbda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net regression performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.5731\n",
      "Mean absolute error (MAE): 0.5565\n",
      "R-squared score: 0.5627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the elastic net regression model\n",
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('elastic_net_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(elastic_net_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('elastic_net_regression_model.pkl', 'rb') as f:\n",
    "    loaded_elastic_net_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_elastic_net = loaded_elastic_net_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_elastic_net)\n",
    "mae = mean_absolute_error(y_test, y_pred_elastic_net)\n",
    "r2 = r2_score(y_test, y_pred_elastic_net)\n",
    "print(f\"Elastic net regression performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7ecf6-cdd3-436c-9fd9-2553fd8af898",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "**Syntax**: \n",
    "```python\n",
    "ElasticNet(*, alpha=1.0, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "```\n",
    "- `*`: Indicates that all parameters following `*` must be passed as keyword arguments, not positional arguments.\n",
    "- **`alpha`** (`float`, default=`1.0`): Constant that multiplies the penalty terms. It can be seen as the regularization strength; must be a positive float. Similar to Ridge and Lasso, adjust `alpha` to control regularization strength. Higher values increase regularization.\n",
    "- **`l1_ratio`** (`float`, default=`0.5`): The ElasticNet mixing parameter, with `0 <= l1_ratio <= 1`. Controls the balance between L1 and L2 regularization. For `l1_ratio = 0`, the penalty is an L2 penalty (pure ridge regression). For `l1_ratio = 1`, it is an L1 penalty (pure lasso regression).\n",
    "- **`fit_intercept`** (`bool`, default=`True`): If `True`, the model calculates the intercept for this model. If set to `False`, no intercept will be used in calculations.\n",
    "- **`precompute`** (`bool` or `array-like`, default=`False`): Whether to use a precomputed Gram matrix. Use `True` to speed up calculations, especially in iterative processes or for large datasets.\n",
    "- **`max_iter`** (`int`, default=`1000`): Maximum number of iterations for the solver. Increase if the algorithm does not converge within the default iterations.\n",
    "- **`copy_X`** (`bool`, default=`True`): If `True`, the input data `X` will be copied. Otherwise, it may be overwritten.\n",
    "- **`tol`** (`float`, default=`0.0001`): Precision of the solution. Adjust for more precise solutions or faster convergence depending on the needs of the model.\n",
    "- **`warm_start`** (`bool`, default=`False`): If set to `True`, reuses the solution of the previous call to `fit` as initialization. Useful in scenarios requiring iterative refinement of the model.\n",
    "- **`positive`** (`bool`, default=`False`): When set to `True`, forces the coefficients to be positive. Apply when the problem domain requires non-negative predictions or coefficients.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): The seed of the pseudo-random number generator to use when shuffling the data. Set for consistent results during multiple runs.\n",
    "- **`selection`** (`{'cyclic', 'random'}`): The order in which coefficients are updated. If set to `‘random’`, a random coefficient is updated every iteration rather than looping over features sequentially. Use `cyclic` for standard, deterministic order.\n",
    "\n",
    "---\n",
    "\n",
    "## Non-parametric regression:\n",
    "Unlike parametric models, which assume a fixed form for the relationship between input features and the target variable, non-parametric models do not make strong assumptions about the data’s underlying structure. This flexibility allows them to adapt to more complex and varied patterns in the data.\n",
    "\n",
    "### Decision tree regressor\n",
    "The decision tree regressor is a non-parametric model that uses a tree structure to predict continuous outcomes. It works by recursively splitting the data into subsets based on feature values, creating decision nodes and leaf nodes, where leaf nodes represent the predicted value. Decision trees are easy to interpret and can capture non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad98d46c-f9fa-4fb6-bf2c-9bb6bedf45bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree regressor performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.4952\n",
      "Mean absolute error (MAE): 0.4547\n",
      "R-squared score: 0.6221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the decision tree regressor model\n",
    "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('decision_tree_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(decision_tree_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('decision_tree_regression_model.pkl', 'rb') as f:\n",
    "    loaded_decision_tree_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_decision_tree = loaded_decision_tree_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_decision_tree)\n",
    "mae = mean_absolute_error(y_test, y_pred_decision_tree)\n",
    "r2 = r2_score(y_test, y_pred_decision_tree)\n",
    "print(f\"Decision tree regressor performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dd7dc-68c8-4ec0-aa56-a2bff9c2aba4",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0)\n",
    "```\n",
    "\n",
    "- **`criterion`** (`{'squared_error', 'friedman_mse', 'absolute_error', 'poisson'}`, default=`'squared_error'`): The function to measure the quality of a split. `'squared_error'` is the default for regression and measures variance.\n",
    "- **`splitter`** (`{'best', 'random'}`, default=`'best'`): The strategy used to choose the split at each node. `'best'` selects the best split, while `'random'` selects a random split.\n",
    "- **`max_depth`** (`int`, default=`None`): The maximum depth of the tree. Limiting depth prevents overfitting.\n",
    "- **`min_samples_split`** (`int` or `float`, default=`2`): The minimum number of samples required to split an internal node. Higher values prevent overfitting.\n",
    "- **`min_samples_leaf`** (`int` or `float`, default=`1`): The minimum number of samples required to be at a leaf node.\n",
    "- **`max_features`** (`int`, `float`, `{'auto', 'sqrt', 'log2'}`, default=`None`): The number of features to consider when looking for the best split.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): Controls the randomness of the estimator. Setting this ensures reproducibility.\n",
    "- **`ccp_alpha`** (`non-negative float`, default=`0.0`): Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "\n",
    "\n",
    "### Support vector regressor (SVR)\n",
    "SVR is an extension SVM for regression tasks. SVR aims to find a function that deviates from the true output by a value no greater than a specified margin (epsilon) while also being as flat as possible. SVR is effective in high-dimensional spaces and is robust against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbb933e-3ef4-4d05-9c44-1e14b9cfe7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector regressor performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 1.3320\n",
      "Mean absolute error (MAE): 0.8600\n",
      "R-squared score: -0.0165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the support vector regressor model\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('svr_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svr_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('svr_regression_model.pkl', 'rb') as f:\n",
    "    loaded_svr_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svr = loaded_svr_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_svr)\n",
    "mae = mean_absolute_error(y_test, y_pred_svr)\n",
    "r2 = r2_score(y_test, y_pred_svr)\n",
    "print(f\"Support vector regressor performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818962c0-99ae-4e59-ac04-42c4acace823",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "```\n",
    "\n",
    "- **`kernel`** (`{'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}`, default=`'rbf'`): The kernel function defines the transformation of data. `'rbf'` (Radial Basis Function) is the default and works well in most cases.\n",
    "- **`degree`** (`int`, default=`3`): Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n",
    "- **`gamma`** (`{'scale', 'auto'}`, default=`'scale'`): Kernel coefficient for `'rbf'`, `'poly'`, and `'sigmoid'`. `'scale'` is based on the number of features.\n",
    "- **`C`** (`float`, default=`1.0`): Regularization parameter. The strength of the regularization is inversely proportional to `C`. A smaller `C` increases regularization.\n",
    "- **`epsilon`** (`float`, default=`0.1`): Defines a margin of tolerance where no penalty is given to errors.\n",
    "- **`shrinking`** (`bool`, default=`True`): Whether to use the shrinking heuristic, which can speed up computation.\n",
    "- **`tol`** (`float`, default=`0.001`): Tolerance for stopping criterion.\n",
    "- **`max_iter`** (`int`, default=`-1`): The maximum number of iterations. `-1` means no limit.\n",
    "\n",
    "\n",
    "### K-nearest neighbors regressor (KNN)\n",
    "KNN regressor is a simple, non-parametric model that predicts the target value based on the average of the k-nearest neighbors in the feature space. KNN is intuitive and easy to implement, though its performance can be affected by the choice of `k` and the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0307f85b-a971-4abf-81b6-a2792af22411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest neighbors regressor performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 1.1187\n",
      "Mean absolute error (MAE): 0.8128\n",
      "R-squared score: 0.1463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the K-nearestnNeighbors regressor model\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('knn_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('knn_regression_model.pkl', 'rb') as f:\n",
    "    loaded_knn_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = loaded_knn_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_knn)\n",
    "mae = mean_absolute_error(y_test, y_pred_knn)\n",
    "r2 = r2_score(y_test, y_pred_knn)\n",
    "print(f\"K-nearest neighbors regressor performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627e852-5128-45b0-9237-47ba8c019d60",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "KNeighborsRegressor(*, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "```\n",
    "\n",
    "- **`n_neighbors`** (`int`, default=`5`): Number of neighbors to use for prediction. Larger values smooth the predictions but may reduce precision.\n",
    "- **`weights`** (`{'uniform', 'distance'}`, default=`'uniform'`): Weight function used in prediction. `'uniform'` assigns equal weight to all neighbors, while `'distance'` gives more weight to closer neighbors.\n",
    "- **`algorithm`** (`{'auto', 'ball_tree', 'kd_tree', 'brute'}`, default=`'auto'`): Algorithm used to compute the nearest neighbors. `'auto'` automatically selects the best option based on the input data.\n",
    "- **`leaf_size`** (`int`, default=`30`): Affects the speed of the tree construction and query. Smaller leaf sizes can lead to faster queries but slower tree construction.\n",
    "- **`p`** (`int`, default=`2`): Power parameter for the Minkowski metric. `p=2` is equivalent to the Euclidean distance.\n",
    "- **`metric`** (`string` or callable, default=`'minkowski'`): The distance metric to use. `'minkowski'` is a generalization of both the Euclidean and Manhattan distances.\n",
    "- **`n_jobs`** (`int`, default=`None`): The number of parallel jobs to run for neighbors search. `None` means 1, and `-1` means using all processors.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Robust regression\n",
    "In robust regression, the goal is to create a model that can perform well even when the data contains outliers or noise. \n",
    "\n",
    "### Theil-Sen regressor\n",
    "Theil-Sen Regressor is a robust linear model that estimates the slope as the median of slopes among all pairs of points. It is less sensitive to outliers compared to ordinary least squares regression, making it a good choice when data contains significant outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c4b2e4-35d3-411f-9e4d-00f7a56ca80a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theil-Sen regressor performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 1.0773\n",
      "Mean absolute error (MAE): 0.5292\n",
      "R-squared score: 0.1779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the Theil-Sen regressor model\n",
    "theil_sen_model = TheilSenRegressor(random_state=42)\n",
    "theil_sen_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('theil_sen_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(theil_sen_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('theil_sen_regression_model.pkl', 'rb') as f:\n",
    "    loaded_theil_sen_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_theil_sen = loaded_theil_sen_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_theil_sen)\n",
    "mae = mean_absolute_error(y_test, y_pred_theil_sen)\n",
    "r2 = r2_score(y_test, y_pred_theil_sen)\n",
    "print(f\"Theil-Sen regressor performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ab363-e488-49c7-ad22-73365b67afab",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "TheilSenRegressor(*, fit_intercept=True, copy_X=True, max_subpopulation=10000, n_subsamples=None, max_iter=300, tol=1e-3, random_state=None, n_jobs=None, verbose=False)\n",
    "```\n",
    "\n",
    "- **`fit_intercept`** (`bool`, default=`True`): Whether to calculate the intercept for this model. If `False`, no intercept will be used.\n",
    "- **`copy_X`** (`bool`, default=`True`): If `True`, the data `X` will be copied; otherwise, it may be overwritten.\n",
    "- **`max_subpopulation`** (`int`, default=`10000`): Maximum subpopulation size for the method to remain efficient.\n",
    "- **`n_subsamples`** (`int`, default=`None`): The number of subsamples to draw for the computation.\n",
    "- **`max_iter`** (`int`, default=`300`): The maximum number of iterations for the optimization algorithm.\n",
    "- **`tol`** (`float`, default=`1e-3`): Tolerance for the optimization.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): Seed for reproducibility.\n",
    "- **`n_jobs`** (`int`, default=`None`): The number of parallel jobs to run. `None` means 1, and `-1` means using all processors.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Multi-layer perceptron regressor\n",
    "The MLP regressor is a type of feedforward artificial neural network model designed for regression tasks. It consists of multiple layers of interconnected nodes (neurons), where each node in a layer is connected to every node in the previous and subsequent layers. The MLP regressor can model complex relationships between inputs and outputs by learning non-linear functions from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb71279-e81a-41c6-82ec-8e69e43c7bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP regression performance metrics:\n",
      "----------------------------------------\n",
      "Mean squared error (MSE): 0.7129\n",
      "Mean absolute error (MAE): 0.6506\n",
      "R-squared score: 0.4560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build and fit the MLP regressor model\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('mlp_regressor_model.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_model, f)\n",
    "\n",
    "# Load the model\n",
    "with open('mlp_regressor_model.pkl', 'rb') as f:\n",
    "    loaded_mlp_model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlp = loaded_mlp_model.predict(X_test)\n",
    "\n",
    "# Display the results\n",
    "mse = mean_squared_error(y_test, y_pred_mlp)\n",
    "mae = mean_absolute_error(y_test, y_pred_mlp)\n",
    "r2 = r2_score(y_test, y_pred_mlp)\n",
    "print(f\"MLP regression performance metrics:\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"Mean squared error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared score: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e846c8a-6358-4f85-b4c9-cb48ae1340de",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "**Syntax**: \n",
    "```python\n",
    "MLPRegressor(*, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
    "```\n",
    "- `*`: Indicates that all parameters following `*` must be passed as keyword arguments, not positional arguments.\n",
    "- **`hidden_layer_sizes`** (`tuple`, default=`(100,)`): Defines the number of neurons in each hidden layer. The default is a single hidden layer with 100 neurons. Adjust based on the complexity of the problem. More layers and neurons allow the model to capture more complex patterns, but may require more data and computational power.\n",
    "- **`activation`** (`{'identity', 'logistic', 'tanh', 'relu'}`, default=`'relu'`): Activation function for the hidden layer.\n",
    "    - `identity`: No-op activation, useful in linear regression models.\n",
    "    - `logistic`: Sigmoid function, maps input to [0, 1], useful for probabilistic interpretation.\n",
    "    - `tanh`: Hyperbolic tangent function, maps input to [-1, 1], useful for data centered around zero.\n",
    "    - `relu`: Rectified Linear Unit, outputs zero for negative inputs and linear for positive. Commonly used due to its simplicity. \n",
    "- **`solver`** (`{'lbfgs', 'sgd', 'adam'}`, default=`'adam'`): The algorithm used for optimizing the weights.\n",
    "    - `lbfgs`: An optimizer in the family of quasi-Newton methods, suitable for smaller datasets or when convergence is a priority.\n",
    "    - `sgd`: Stochastic gradient descent, good for large datasets  where speed is critical.\n",
    "    - `adam`: An adaptive learning rate optimizer, efficient and widely used.\n",
    "- **`alpha`** (`float`, default=`0.0001`): L2 penalty (regularization term) parameter to prevent overfitting by discouraging large weights. Increase `alpha` if the model is overfitting. Adjust based on the model's performance on validation data.\n",
    "- **`batch_size`** (`int`, default=`'auto'`): Size of minibatches for stochastic optimizers (Number of samples per gradient update). `auto` uses `min(200, n_samples)` as the batch size. Specify a value for manual control, especially in large datasets where smaller batches might speed up learning.\n",
    "- **`learning_rate`** (`{'constant', 'invscaling', 'adaptive'}`, default=`'constant'`): Learning rate schedule for weight updates.\n",
    "    - `constant`: Learning rate remains fixed.\n",
    "    - `adaptive`: Learning rate decreases when the improvement rate drops below `tol`.\n",
    "    - `invscaling`: Learning rate decreases at each step inversely proportional to the number of iterations.\n",
    "- **`learning_rate_init`** (`float`, default=`0.001`): The initial learning rate used for weight updates. Adjust this parameter if the model is learning too slowly (increase) or too quickly (decrease).\n",
    "- **`power_t`** (`float`, default=`0.5`): The exponent for inverse scaling learning rate. Fine-tune if using `invscaling` learning rate.\n",
    "- **`max_iter`** (`int`, default=`200`): Maximum number of iterations (epochs) for training. Increase this if the model hasn’t converged by the default number of iterations.\n",
    "- **`shuffle`** (`bool`, default=`True`): Whether to shuffle samples in each iteration. If `True`, shuffles the training data before each iteration. Shuffling generally helps prevent cycles during training.\n",
    "- **`random_state`** (`int`, `RandomState` instance, default=`None`): The seed of the pseudo-random number generator to use when shuffling the data. Set this for reproducible results, especially important when comparing different models or running experiments.\n",
    "- **`tol`** (`float`, default=`0.0001`): Tolerance for the stopping criterion. Decrease `tol` for higher precision or increase for faster training.\n",
    "- **`verbose`** (`bool`, default=`False`): If `True`, prints progress messages during training. Set `True` if we want to monitor the training process, particularly useful during debugging or long training sessions.\n",
    "- **`warm_start`** (`bool`, default=`False`): Reuse the solution of the previous call to `fit` as initialization, otherwise, just erase the previous solution. Set `True` when performing iterative updates to the model without starting from scratch.\n",
    "- **`momentum`** (`float`, default=`0.9`): Momentum for gradient descent update. Increase for faster convergence, especially with `sgd` solver, but may require tuning to prevent overshooting.\n",
    "- **`nesterovs_momentum`** (`bool`, default=`True`): Whether to use Nesterov’s momentum. Typically set to `True` for better convergence, especially in non-convex problems.\n",
    "- **`early_stopping`** (`bool`, default=`False`): Whether to use early stopping to terminate training when validation score is not improving. Useful for avoiding overfitting and reducing training time. Best combined with `validation_fraction`.\n",
    "- **`validation_fraction`** (`float`, default=`0.1`): The proportion of training data to set aside as validation set for early stopping. Set this when using early stopping to determine the right amount of validation data.\n",
    "- **`beta_1`** (`float`, default=`0.9`): Exponential decay rate for estimates of first-moment vector in `adam`, should be in [0, 1). Typically left at default, but can be tuned if `adam` convergence is an issue.\n",
    "- **`beta_2`** (`float`, default=`0.999`): Exponential decay rate for estimates of second-moment vector in `adam`, should be in [0, 1). Like `beta_1`, usually left at default unless specific issues arise with `adam`.\n",
    "- **`epsilon`** (`float`, default=`1e-08`): Value for numerical stability in `adam`. Adjust if `adam` has convergence issues, but generally leave it at default.\n",
    "- **`n_iter_no_change`** (`int`, default=`10`): Maximum number of epochs to not meet `tol` improvement.\n",
    "- **`max_fun`** (`int`, default=`15000`): Only used when `solver='lbfgs'`. Maximum number of function calls. Increase if `lbfgs` does not converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
